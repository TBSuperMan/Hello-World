# 消息队列

## 消息队列的作用

消息队列的核心作用：
- **解耦**：服务的生产者与服务的消费者不必耦合在一起，只需要与消息队列交互即可
- **异步**：假如服务A需要与耗时较长的服务B关联，用户请求服务A后，如果是同步的方案，需要等待服务B也完成，而通过消息队列，可以让服务A先返回结果，再异步地执行服务B
- **削峰**：消息队列可以缓存请求，让大量请求不直接打到系统上

## 各种消息队列的区别，如何根据场景选择

|  | RabbitMQ | ActiveMQ | RocketMQ | Kafka |
|--|--|--|--|--|
| 可用性 | 高（主从架构） | 一般 | 高（分布式架构） | 高（分布式架构） |
| 单机吞吐量 | 万级 | 万级 | 十万级 | 百万级 |
| 消息延迟 | 微秒 | 毫秒 | 毫秒 | 毫秒 |
| 消息可靠性 | 高 | 一般 | 高 | 一般 |
| 消息丢失 | 可能性低 | 可能性低 | 理论上不丢失 | 理论上不丢失 |

# Kafka

## Kafka的消息模型

早期，使用队列模型作为消息通信载体，满足生产者与消费者模式，一条消息只能被一个消费者使用，未被消费的消息在队列中保留直到被消费或超时。
但如果我们需要将生产者产生的消息分发给多个消费者，并且每个消费者都能接收到完整的消息内容。
这种情况就难以通过队列模型实现。

因此Kafka采用的是**发布-订阅**的消息模型，

![](./../images/消息队列/发布订阅模型.png)

发布订阅模型（Pub-Sub） 使用主题（Topic） 作为消息通信载体，类似于广播模式；
发布者发布一条消息，该消息通过主题传递给所有的订阅者，在一条消息广播之后才订阅的用户则是收不到该条消息的。

> RocketMQ 的消息模型和 Kafka 基本是完全一样的。唯一的区别是 Kafka 中没有队列这个概念，与之对应的是 Partition（分区）


## Kafka的数据结构

![](./../images/消息队列/Kafka数据结构.png)

在Kafka中，主要有以下几种角色：
- Producer：消息的生产者
- Consumer：消息的消费者
- Broker（代理）：可以看作是一个独立的 Kafka 实例。多个 Kafka Broker 组成一个 Kafka Cluster。
  - 每**个Broker中又包含Topic 以及 Partition**
- Topic（主题）： Producer 将消息发送到特定的主题，Consumer 通过订阅特定的 Topic(主题) 来消费消息。
- Partition（分区）：Partition 属于 Topic 的一部分。一个 Topic 可以有多个 Partition ，并且同一 Topic 下的 Partition 可以分布在不同的 Broker 上，这也就表明一个 Topic 可以横跨多个 Broker
  - Kafka 中的 Partition（分区） 实际上可以对应成为消息队列中的队列

> Broker包含多个topic，topic也可以分布在多个Broker上

## kafka 架构如何实现高可用的

Kafka是通过多副本机制实现高可用的。

==每个partition都会有多个数据副本，每个副本分别存在于不同的broker==

![](./../images/消息队列/kafka副本.jpeg)

Kafka 为分区（Partition）引入了多副本（Replica）机制，即**主备集群**
分区（Partition）中的多个副本之间会有一个叫做 `leader` 的家伙，其他副本称为 `follower`。
我们发送的消息会被发送到 `leader` 副本，然后 `follower` 副本才能从 `leader` 副本中拉取消息进行同步。

生产者和消费者只与 leader 副本交互，**其他副本只是 leader 副本的拷贝，它们的存在只是为了保证消息存储的安全性**。
- 当 leader 副本发生故障时会从 follower 中选举出一个 leader，但是 follower 中如果有和 leader 同步程度达不到要求，就参加不了 leader 的竞选。

通过多分区与多副本机制，有以下好处：
- 可以**给topic指定多个分区Partition**，而**Partition分布在不同的Broker上**，可以提供较好的并发能力，也可以实现负载均衡
- 通过多副本机制，可以提高Kafka的可用性，极大地提高了消息存储的安全性, 提高了容灾能力

## Zookeeper在Kafka中有什么作用？

ZK主要为 Kafka 提供元数据的管理的功能。

ZK的任务：
1. **Broker 注册** ：
   - 在 ZK 上会有一个专门用来进行 **Broker 服务器列表记录**的节点。
    每个 Broker 在启动时，都会到 ZK 上进行注册，即到 `/brokers/ids` 下创建属于自己的节点。
    每个 Broker 就会**将自己的 IP 地址和端口等信息记录到该节点**中去
2. **Topic 注册** ： 
   - 在 Kafka 中，同一个Topic 的消息会**被分成多个分区，并将其分布在多个 Broker 上**，**这些分区信息及与 Broker 的对应关系也都是由 Zookeeper 在维护**。
     > 比如我创建了一个名字为 `my-topic` 的主题topic并且它有两个分区partition，对应到 zookeeper 中会创建这些文件夹：`/brokers/topics/my-topic/Partitions/0`、`/brokers/topics/my-topic/Partitions/1`
3. **负载均衡** ：
   - 上面也说过了 Kafka 通过给特定 Topic 指定多个 Partition, 而各个 Partition 可以分布在不同的 Broker 上, 这样便能提供比较好的并发能力。 
    对于同一个 Topic 的不同 Partition，**Kafka 会尽力将这些 Partition 分布到不同的 Broker 服务器上**。
    - 当生产者产生消息后也会尽量投递到**不同 Broker 的 Partition**里面。
    - 当 Consumer 消费的时候，Zookeeper 可以根据当前的 Partition 数量以及 Consumer 数量来实现动态负载均衡。

## 生产者的分区选择策略

生产者是如何决定一条消息该投递到哪个Partition的呢？

- 如果在发消息的时候**指定了分区**，则消息投递到指定的分区

- 如果没有指定分区，但是**消息的key不为空**，则基于key的哈希值来选择一个分区

- 如果既没有指定分区，且**消息的key也是空**，则用**轮询**的方式选择一个分区



## Kafka怎么保证消息的消费顺序

Kafka 中 Partition(分区)是真正保存消息的地方，我们发送的消息都被放在了这里。
而 Partition(分区) 又存在于 Topic(主题) 这个概念中，并且我们可以给特定 Topic 指定多个 Partition。

![](./../images/消息队列/Kafka有序性.png)

每次添加消息到 Partition(分区) 的时候都会采用**尾加法**，如上图所示。 ==Kafka 只能为我们保证 Partition(分区) 中的消息有序==。
> 消息在被追加到 Partition(分区)的时候都会分配一个特定的偏移量（offset）。Kafka 通过偏移量（offset）来保证消息在分区内的顺序性。

producer发消息到队列时，通过==加锁==保证有序

这样一来，就引入了两个问题：
- `broker leader`在给`producer`发送`ack`时，因网络原因超时，那么`producer` 将重试，造成**消息重复加入到topic中**。
- 假设有两条消息先后发送。t1时刻`msg1`发送失败，`msg2`发送成功，t2时刻msg1重试后发送成功。造成**乱序**。


那么，为什么**Kafka不保证多个分区的有序性**，即**整个Topic的有序性**呢？
> 如果Kafka要保证多个partition有序，则在Broker保存数据时、消费者消费数据时都需要保证有序，如果一个Topic的Partition1阻塞了，为了有序，那partition2以及后续的分区也不能被消费，这种情况下，Kafka 就退化成了单一队列，毫无并发性可言，极大降低系统性能
> 因此，Kafka使用多partition的概念，并且只保证单partition有序。这样不同partiiton之间不会干扰对方

**如果一定要保证消息的顺序，可以在发送消息时就指定Partition**

> 除此之外，对于同一个Topic，其中的一个分区只允许被同一消费者组中的一个消费者负责，这也是为了实现消息的顺序性

## Kafka怎么保证消息不被重复生产

消息重复生产的场景：生产者将消息发送给Broker，但Broker返回ack时，网络超时，导致Producer重新发送消息，就产生了两条相同的消息

消息重复的原因是：**Producer的操作不具有幂等性**，两条重复的操作造成了两次不同的结果。

解决方案：Kafka引入了`Producer ID`（即PID）和`Sequence Number`：
- 对于每个`Producer`，其发送的每条消息`<Topic, Partition>`都对应一个单调递增的`Sequence Number`。
- 同样，`Broker`端也会为每个`<PID, Topic, Partition>`维护一个序号，并且**每Commit一条消息时将其对应序号递增**。
- 对于接收到的每条消息的`Sequence Number`与`Broker`维护的序号，有以下不同的情况：
  1. 消息的序号与`Broker`维护的序号的差值为`1`，则`Broker`会接受它，否则将其丢弃：
  2. 差值大于1，说明中间有数据没有写入，Broker拒收该消息
  3. 消息的序号小于等于`Broker`，说明该消息已经被保存，直接丢弃

> Producer发送失败后会重试，保证每个消息都被发送到broker


## Kafka怎么保证消息不重复消费

重复消费的原因：
- 消费者侧**已经消费的数据没有成功提交 offset**（根本原因），导致Broker将该消息又发给其他消费者
  - 消费者提交offset的默认时间是5分钟，超过这个时间，若有别的消费者请求消息，就会形成重复消费。
- Kafka侧 由于**消费者处理业务时间长或者网络链接**等原因，让 Kafka 认为服务假死，触发了**分区 rebalance**

[幂等校验](http://www.mydlq.club/article/94/)

解决方案：
- 消费消息服务做==幂等校验==，比如 Redis 的set、MySQL 的主键等天然的幂等功能。这种方法最有效。
  - 可以建一个数据库，**专门存储消息消费记录**
    - 生产者，**发送消息前判断库中是否有记录（有记录说明已发送）**，没有记录，先入库，状态为待消费，然后发送消息并把主键id带上。
    - 消费者，接收消息，**通过主键ID查询记录表，判断消息状态是否已消费**。若没消费过，则处理消息，处理完后，更新消息记录的状态为已消费。
- 将 `enable.auto.commit` 参数设置为 false，==关闭自动提交==，开发者在代码中手动提交 offset。
  - 那么这里会有个问题：什么时候提交offset合适？
    - **处理完消息再提交**：依旧有消息重复消费的风险，和自动提交一样（如果处理时间长）
    - **拉取到消息即提交**：会有消息丢失的风险。允许消息延时的场景，一般会采用这种方式，然后，通过定时任务在业务不繁忙（比如凌晨）的时候做数据兜底。

**幂等校验的实现方式**
- 如果是对数据库的insert，就给这个消息的操作涉及的列加上一个**唯一性约束**，如果重复消费，则冲突报错
- 如果是对redis的set，则redis的set可以保证天然的幂等性
- 其他情况，可以通过一个第三方的存储介质，来存储**消费记录**，例如放到一个分布式缓存里，通过给消息分配全局唯一的ID，消费者消费时，只要去查查看有没有这个记录就行

## Kafka怎么保证消息不丢失

基于Kafka的消息队列主要分为三个部分：生产者、消费者、Broker，因此，要保证消息不丢失，需要对它们进行针对性研究

### 生产者丢失消息的情况：

生产者(Producer) 调用send方法发送消息之后，消息可能**因为网络问题并没有发送过去**

因此，不能默认在调用`send`后，消息就发送成功了，需要**对消息发送的结果进行判断**
但是，Kafka的生产者使用`send`方法发送消息实际上是一个==异步操作==，可以通过`get()`方法获取其结果，但这样也就会令其**退化为同步操作**
因此，可以通过**回调**的方式获取结果：
```java
ListenableFuture<SendResult<String, Object>> future = kafkaTemplate.send(topic, o);
future.addCallback(
  result -> logger.info("生产者成功发送消息到topic:{} partition:{}的消息",
                result.getRecordMetadata().topic(),
                result.getRecordMetadata().partition()),
  ex -> logger.error("生产者发送消失败，原因：{}", ex.getMessage())
);
```
如果检测到消息发送失败，直接重新发送即可

除此之外，需要为`Producer`的**重试次数**与**重试间隔**设置一个比较合理的值。

> 除此之外，在生产者中，为了提升效率，减少IO，producer在发送数据时可以**将多个请求进行合并后发送**。被合并的请求在发送前先缓存在本地buffer中。
> 缓存的方式和前文提到的刷盘类似，producer可以将请求打包成“块”或者按照时间间隔，将buffer中的数据发出。通过buffer我们可以将生产者改造为异步的方式，而这可以提升我们的发送效率。
> 但是，buffer中的数据就是危险的：
> 1. 在正常情况下，客户端的异步调用可以通过callback来处理消息发送失败或者超时的情况，但是，一旦producer被非法的停止了，那么**buffer中的数据将丢失**，broker将无法收到该部分数据。
> 2. 又或者，当Producer客户端内存不够时，如果采取的策略是**丢弃消息**（另一种策略是block阻塞），消息也会被丢失。
> 3. 抑或，消息产生（异步产生）过快，导致挂起线程过多，内存不足，导致程序崩溃，消息丢失。

![异步发送过快的示意图](./../images/消息队列/异步发送过快.jpg)

解决方案：
1. service产生消息时，使用阻塞的线程池，并且线程数有一定上限。整体思路是控制消息产生速度。
2. 扩大Buffer的容量配置。这种方式可以缓解该情况的出现，但不能杜绝、
3. service不**直接将消息发送到buffer（内存），而是将消息写到本地的磁盘中（数据库或者文件），由另一个（或少量）生产线程进行消息发送**。相当于是在buffer和service之间又加了一层空间**更加富裕的缓冲层**。

### 消费者丢失消息的情况

> Kakfa通过偏移量（offset)表示 **Consumer 当前消费到的 Partition(分区)的所在的位置**
> Kafka 通过偏移量（offset）可以保证消息在分区内的顺序性。
> Kafka默认的提交机制是：**根据一定的时间间隔，将收到的消息进行commit**

![](./../images/消息队列/Kafka丢失.jpg)

当消费者拉取到了分区的某个消息之后，消费者自动提交 `offset`，使对应的`offset`向后移动。
但是，如果当消费者刚拿到这个消息准备进行真正消费的时候，突然挂掉了，**消息实际上并没有被消费，但是 offset 却被自动提交了**，就导致这个消息不会被处理。

解决方案：**手动关闭自动提交 offset，每次在真正消费完消息之后再自己手动提交 offset** 
但是，如果忘了手动提交`offset`，反而会造成更多的问题。

### Broker丢失消息的情况

> Kafka为分区Partition引入了多副本（Replication）机制。
> 分区的多个副本内，又被划分为Leader与Follower。
> 生产者和消费者只与 leader 副本交互，Follower从Leader拉取消息进行同步，其只负责保证存储的安全性（和Redis主从类似）

假如 leader 副本所在的 broker 突然挂掉，那么就要从 follower 副本重新选出一个 leader ，但是 **leader 的数据还有一部分没有被 follower 副本的同步**，就会造成消息丢失。

解决步骤：（这几个步骤是相辅相成的，其中的一个并不能解决问题）
1. 设置 `acks = all`（**确认成功发送需要的acks数**）
  - `acks`是**生产者**的重要参数，其默认值为1，代表**我们的消息被leader副本接收之后，就算被成功发送**
  - 如果配置为`acks = all`，表示**在所有副本都要接收到该消息之后，该消息才算真正成功被发送**


2. 设置 `replication.factor >= 3`（**每个Partition的副本数**）
  - `replication.factor`代表每个分区Partition中最少的副本数，增加最少副本数保证**有follower作为leader 副本的备份**，虽然造成了数据冗余，但是带来了数据的安全性。
  - 但是，有一说一，只是增加副本数就能保证消息不丢失吗？不见得吧，如果消息一到leader，它发送完ack就挂掉了，那怎么办？（如果和第一步配合，那就可行了）


3. 设置 `min.insync.replicas > 1`（**确认成功发送需要写入的副本数**）
  - `min.insync.replicas > 1`代表**消息至少要被写入到 2 个副本才算是被成功发送（包括1个Leader和至少1个Follower）**
   > 为了保证Kafka集群的高可用，需要确保 `replication.factor` > `min.insync.replicas`，即`副本数`一定要大于等于`返回成功时需要写入的副本数`，否则一旦有一个副本挂掉了，集群就无法正常工作，因为永远都无法返回ack。
  -  `min.insync.replicas`通常设置为`replication.factor - 1`。
   

4. 设置 `unclean.leader.election.enable = false`
  - 这样配置之后，当 leader 副本发生故障时，Leader选举的范围就不包含**和 leader 同步程度达不到要求的副本**，这样降低了消息丢失的可能性。

> 除此之外，kafka采用了批量异步刷盘的持久化方式：按照一定的消息量，和时间间隔进行刷盘。
> 将数据存储到linux操作系统时，会先存储到页缓存（Page cache）中，按照时间或者其他条件进行刷盘（从page cache到file），或者通过`fsync`命令强制刷盘。**数据在page cache中时，如果系统挂掉，数据会丢失**。
> 

## 消费者组是啥

首先，需要引出消费者组这一概念。
消费者组`Consumer Group` 是 Kafka 提供的**可扩展且具有容错性的消费者机制**，一个组中的多个消费者实例共享一个唯一的`GroupID`。
组内的所有消费者协调在一起来消费订阅主题(subscribed topics)的所有分区(partition)。当然，**每个分区只能由同一个消费组内的一个consumer来消费**（仅针对一个消费者组）


- consumer group下订阅的topic下的**每个分区只能分配给某个group下的一个consumer**
  - 当然该分区还可以被分配给其他group
    所以Kafka可以实现消息队列（一个消费者属于单个Group），也可以实现发布/订阅模型（一个消费者属于多个Group）。

> Kafka中，每个消费者组管理自己的位移`offset`信息，这是由于如果让Broker管理，会引入几个问题：
> 1. Broker从此变成有状态的，会影响伸缩性
> 2. 需要引入应答机制(acknowledgement)来确认消费成功
> 3. 由于要保存很多consumer的offset信息，必然引入复杂的数据结构，造成资源浪费

在消费者组中，通过map存储在各个topic上的offset的信息

## 分区rebalance是啥

Rebalance 本质上是一种协议，规定了一个 Consumer Group 下的所有 consumer 如何达成一致，来分配订阅 Topic 的每个分区。
> 例如：某 Group 下有 20 个 consumer 实例，它订阅了一个具有 100 个 partition 的 Topic 。正常情况下，kafka 会为每个 Consumer 平均的分配 5 个分区。这个分配的过程就是 Rebalance。

**rebalance的触发时机：**
- **Consumer组成员个数**发生变化。例如有新的 consumer 实例加入该消费组或者离开组。
- **订阅的 Topic 个数**发生变化。（如果用正则表达式订阅Topic，是有可能导致topic个数改变的）
- **订阅 Topic 的分区数**发生变化。

> ==在Rebalance时kafka会停止所有的服务==，因为当前版本的Kafka触发Rebalance时候会**重新分配所有的Consumer对应的分区**，并不是像一致性哈希一样，只需要对部分数据进行重定位。所以要尽量避免发生Rebalance的发生。

在Consumer Group内部，默认提供两种**topic分区的分配策略**：
- range
- round-robin

上文说到，rebalance实际上就是group协调工作的协议，且由coordinator执行rebalance，并进行group的管理。

Kafka提供了5个**处理消费组协调问题的协议**：
- Heartbeat请求：consumer需要定期给coordinator发送心跳来表明自己还活着
- LeaveGroup请求：主动告诉coordinator我要离开consumer group
- SyncGroup请求：group leader把分配方案告诉组内所有成员
- JoinGroup请求：成员请求加入组
- DescribeGroup请求：显示组的所有信息，包括成员信息，协议名称，分配方案，订阅信息等。通常该请求是给管理员使用

**rebalance的具体流程：**

## Kafka怎么处理错误消息



## topic partition replication是一个什么样的比例关系，replication数量和consumer数量一般怎么去设置

**partition的数量：**

越多的分区可以提供更高的吞吐量。在kafka中，单个patition是kafka并行操作的最小单元。
- 在`producer`和`broker`端，**向每一个分区写入数据是可以完全并行化的**，此时，可以通过加大硬件资源的利用率来提升系统的吞吐量。
- 在`consumer`段，kafka**只允许单个partition的数据被一个consumer线程消费**，**但一个Consumer可以读取多个partition**。因此，在consumer端，每一个Consumer Group内部的consumer并行度完全依赖于被消费的分区数量。

因此，可以通过吞吐量来粗略的计算Partition的数量：假设对于单个partition，producer端的可达吞吐量为p，Consumer端的可达吞吐量为c，期望的目标吞吐量为t，那么集群所需要的partition数量至少为max(t/p,t/c)

除此之外，由于增加分区会导致rebalance问题，严重影响kafka的可用性，所以应该减少rebalance，把分区数量设置多一点。

但**Partition数量并不是越多越好的**
1. 越多的分区需要打开更多地文件句柄
   - 在kafka的Broker中，每一个Partition都对应文件系统的一个目录。而kafka的数据日志文件目录中，每个日志段会分配两个文件：index文件与数据文件，就需要两个句柄。 
2. 更多地分区会导致更高的不可用性
   - Kafka通过多主备集群的方式，实现kafka集群的高可用和稳定性。
   > 当一个**broker有计划地停止服务**时，controller会在服务停止之前，**将该broker上的所有leader一个个地移走**。由于单个leader的移动时间大约只需要花费几毫秒，因此从客户层面看，有计划的服务停机只会导致系统在很小时间窗口中不可用（Leader选举需要STW）
   > 但如果**Broker意外终止**，**系统的不可用时间窗口将会与受影响的partition数量有关，直到所有partition都完成Leader选举**，才能继续提供服务
   > 更进一步地，如果**宕机的Broker是Controller节点**，需要等到Controller节点自动恢复完成，才会进行Leader选举，而新的controller节点需要从zookeeper中读取每一个partition的元数据信息用于初始化数据，耗时极长
3. 越多的分区可能增加端对端的延迟
   - Kafka在消息提交后，才会将其暴露给消费者，而消息的提交前需要将Leader数据同步到Follower中，而副本是分布在不同的Broker中的，因此需要进行Broker之间的数据交互
   - 但在默认情况下，每个broker从其他broker节点进行数据副本复制时，该broker节点只会为此工作分配**一个线程**，如果一个Broker中的partition过多，复制的时间就会很长
4. 越多的partition意味着需要客户端需要更多的内存

## Kafka的零拷贝原理

Kafka可以高效读写数据，主要有三个原因：
1. kafka本身是分布式集群，同时又采用了分区技术，具有较高的并发度。
2. 数据可以顺序写入磁盘，Kafka 的 producer 生产数据，要写入到 log 文件中，写的过程是一直追加到文件末端，为顺序写。
3. 使用了**零拷贝技术**。

### 零拷贝原理

> **错误说法一：零拷贝就是零复制，即没有进行复制的操作，所以快**
>   - 零拷贝是指计算机操作的过程中，**CPU不需要为数据在内存之间的拷贝消耗资源，这个时候cpu可以干别的事情，至于数据的复制次数只能降低，而不会减少到0**
> - 绝对不是指0次复制，而是0次调用CPU消耗资源
> **错误说法二：DMA技术是为了零拷贝而生的**
> - 即使是普通的read、write，也会用到DMA技术

![](./../images/消息队列/read-write.jpg)

![](./../images/消息队列/DMA.png)

DMA做的事情很清晰，它主要就是帮忙CPU转发一下IO请求，以及拷贝数据。

上图为最传统的read和send文件处理操作伪代码：
```
File.read(file, buf, len);
Socket.send(socket, buf, len);
```

一共进行了**4次上下文切换，4次数据复制，2次CPU调用**。
其中，在内核态，将file读入ReadBuffer与将SocketBuffer读出到NIC Buffer都利用了DMA技术，不需要CPU参与，但将内核空间的数据拷贝到用户空间时，还是需要CPU调度的。

DMA全程`Direct Memory Access`，即直接存储器访问。它允许外设设备和内存存储器之间直接进行IO数据传输，而不需要依赖于 CPU 的的参与，减少了CPU的中断。通俗点理解，就是让**硬件可以跳过CPU的调度，直接访问主内存**。

### **mmpa+write**实现的零拷贝


mmap的原型函数如下：`void *mmap(void *addr, size_t length, int prot, int flags, int fd, off_t offset);`

mmap基于虚拟内存，将内核空间与用户空间的虚拟地址映射到同一个物理地址，也就是==将内核中的读缓冲区与用户空间的缓冲区进行映射==，让所有IO都在内核中完成，不需要将内核缓冲区的数据拷贝回用户缓冲区，**直接通过虚拟地址映射，让用户在映射为内核缓冲区的用户缓冲区上操作**，从而减少了一次有CPU参与的复制（**剩下2次DMA复制，1次CPU复制，以及4次用户态与内核态的切换**）

![](./../images/消息队列/mmap.png)

> 实际应用：Java NIO中，`FileChannel`的`transferTo()` 方法就可以实现这个过程，该方法将数据从文件通道传输到给定的可写字节通道。
> 对应的可以把`file.read()`和`socket.send()`替换为 `transferTo()`

### **sendfile实现的零拷贝**

sendfile是Linux2.1内核版本后引入的一个系统调用函数，表示在两个文件描述符之间传输数据，它是在操作系统内核中操作的，避免了数据从内核缓冲区和用户缓冲区之间的拷贝操作，因此可以使用它来实现零拷贝

![](./../images/消息队列/sendfile.png)

其流程如下：
1. 用户进程发起sendfile系统调用，**上下文（切换1）从用户态转向内核态**
2. DMA控制器将数据从硬盘拷贝到内核缓冲区
3. CPU将内核缓冲区中的数据拷贝到socket缓冲区
4. DMA控制器将socket缓冲区的数据拷贝到网卡
5. **上下文（切换2）从内核态切换回用户态**，sendfile调用返回

即sendfile实现了**将内核缓冲区的数据直接拷贝到socket缓冲区**，最终剩下**2次用户态与内核态的切换、2次DMA拷贝、1次CPU拷贝**

### sendfile + DMA 收集/拷贝 实现的零拷贝

在mmap与sendfile实现的IO方法中，仍然需要1次CPU复制，没有达到零拷贝的要求，如果需要更进一步的话，就需要**底层网络接口支持收集操作**。
即对DMA拷贝加入了scatter/gather操作，它可以**直接从内核空间缓冲区中将数据读取到网卡**，从而省去一次CPU拷贝

成型的零拷贝模型如下图所示：

![](./../images/消息队列/零拷贝.png)

其中，数据的传输只需要两步：
1. 用户进程调用`sendfile`系统调用，触发上下文切换，并切换为内核态
2. DMA控制器将磁盘中的数据拷贝到内核缓冲区
3. CPU将内核缓冲区的**文件描述符信息（包括内核缓冲区的内存地址与偏移量）1**发送到socket缓冲区
4. DMA根据文件描述符信息，直接将数据从内核缓冲区拷贝到网卡中
5. 上下文切换，从内核态切换回用户态，sendfile调用返回
    全程都是DMA参与，从而消除CPU参与的数据复制消耗

最终，发生了**2次用户态与内核态的切换，2次DMA拷贝，没有发生CPU拷贝**，从而实现了真正的零拷贝技术

> JavaNIO包中，FileChannel的`transferTo()/transferFrom()`，底层就是`sendfile()`系统调用函数

以kafka为例，如果有100个消费者消费一份数据:
- 在普通的数据传输方式下，复制次数一共是$100*4 = 400次$，cpu调用次数一共是$100*2 = 200次$；
- 而使用了零拷贝技术之后，复制次数一共是$100+1 = 101次$（1次数据从磁盘到内核读取缓冲区，100次发送给100个消费者消费），cpu调用次数一共是$0$次。极大的提高了数据读写效率。


## Kafka管理平台

kafka-manager

## Kafke怎么把消息同时发给订阅同一个Topic的多个消费者

其实就是多播，上面我们讨论的都是单播的情况，即一条消息只能被一条消费者消费。
多播即一条消息能够被多个消费者消费的模式称为多播。（每个消费者应该属于不同的消费者组）

```
./kafka-console-consumer.sh --bootstrap-server bigdata1:9092 --topic test --from-beginning --consumer-property group.id=testgroup1

./kafka-console-consumer.sh --bootstrap-server bigdata1:9092 --topic test --from-beginning --consumer-property group.id=testgroup2

```



## 为什么kafka的吞吐量比RabbitMQ的吞吐量高

Kafka本身就是分布式的系统，并发量比rabbitmq高得多

除此之外，kafka采用了分区partition的机制，进一步提高了吞吐量



# RocketMQ

## RocketMQ 的数据结构


![](./../images/消息队列/rocketmq模型.jpg)

对于Topic的实现，Kafka采用的是分区Partition，而RocketMQ采用的是队列，RabbitMQ采用的是Exchange

在RocketMQ中，在一个Topic中，对于一个消费者集群，一个队列只会被一个消费者消费，因此最好控制**消费者组中的消费者个数和主题中队列个数相同**

> 每个消费者对应一个队列，是为了通过offset维护消费者当前需要消费的位置

在RocketMQ中，一共包含四类角色：
- Broker：负责消息的存储、投递和查询以及服务高可用保证。其实就是消息队列服务器，
- NameServer：注册中心，负责**Broker的管理和路由信息的管理**，存放Broker的路由表，以供生产者与消费者查询

## RocketMQ怎么保证高可用

1. Broker实现了==主从集群==，与Kafka类似。 salve 定时从 master 同步数据(同步刷盘或者异步刷盘)，如果 master 宕机，则 ==slave 提供消费服务，但是不能写入消息==
2. NameServer也做了集群部署，但采用了==去中心化==的策略，也就意味着 ==单个Broker和所有NameServer保持长连接==，且每隔一定时间，Broker向所有NameServer发送心跳消息，包含自身的Topic配置信息。
3. 生产者发送信息时，首先向NameServer请求**Broker的路由信息**，然后以**轮询**的方式向每个队列生产数据
4. 消费者通过NameServer获得Broker路由信息，向Broker发送Pull请求以获得数据。Consumer可以以**广播或集群**的方式启动，
   1. 广播：一条消息会发送给同一消费者组的所有消费者
   2. 集群：就是单播

## RocketMQ怎么消息的有序性

- 普通顺序：消费者**通过同一个消费队列收到的消息是有顺序的** ，不同消息队列收到的消息则可能是无顺序的。且同一组中，不同消费者收到的消息在整体上是无序的
  - 普通顺序消息在 Broker 重启情况下不会保证消息顺序性 (短暂时间) 

- 严格顺序：消费者收到的 **所有消息** 均是有顺序的。
  - 严格顺序消息 即使在异常情况下也会保证消息的顺序性 。

严格顺序模式的代价较大，`Broker`集群中只要有一台机器不可用，就会导致整个集群不可用。

## RocketMQ如何保证数据不重复消费？

即保证消费操作的幂等性。
这需要根据具体业务决定采用哪种方案：
- 写入Redis，Redis的key-value天然支持幂等
- 数据库插入法，通过数据库的UNIQUE键来保证幂等

## 怎么保证消息的可靠性

## 分布式事务

常见的分布式解决方案有2PC、3PC、TCC、MQ事务

在MQ事务实现方案中，是通过RocketMQ的半消息实现的。
半消息：在事务提交之前，对于消费者来说，这个消息是不可见的

> 半消息的实现方式：如果消息是half消息，将备份原消息的主题与消息消费队列，然后改变TOPIC 为RMQ_SYS_TRANS_HALF_TOPIC。
> 由于消费组未订阅该主题，故消费端无法消费half类型的消息。
> 然后RocketMQ会开启一个定时任务，从Topic为RMQ_SYS_TRANS_HALF_TOPIC中拉取消息进行消费，根据生产者组获取一个服务提供者发送回查事务状态请求（也就是查询生产者的Commit请求），根据事务状态来决定是提交或回滚消息。

![](./../images/消息队列/分布式事务.png)

其中，比较重要的是第5步的**事务反查机制**。
如果第4步因网络波动而没有发送成功，MQ就不知道是否把消息给消费者。

## 消息堆积问题

消息队列的一个重要功能就是**削峰**，但如果生产者生产太快或消费者消费太慢，导致消息在消息队列中堆积怎么办？

- 生产者生产太快
  - 可以采用**限流**、**降级**的方法，减慢生产速度
  - 
- 消费者消费太慢
  - 水平扩展：增加消费者实例数，提高消费能力，同时也需要增加每个Topic的队列数（在一个消费者组中，一个队列只会被一个消费者消费）
  - 检查是否出现大量消费错误，或者检查是否有哪个线程卡死，出现了锁资源不释放的问题

## 回溯消费是什么

回溯消费是指 **Consumer 已经消费成功的消息**，由于业务上需求需要重新消费。
在RocketMQ 中， Broker 在向Consumer 投递成功消息后，**消息仍然需要保留**。
重新消费一般是按照时间回溯，例如由于 Consumer 系统故障，恢复后需要重新消费1小时前的数据，那么 Broker 要提供一种机制，可以按照时间维度来回退消费进度。
RocketMQ 支持按照时间回溯消费，时间维度精确到毫秒。

## RocketMQ刷盘机制

在Broker的Topic中，需要将消息持久化，就需要用到刷盘机制

- 同步刷盘：调用刷盘的操作后，需要等待刷盘结果返回，性能差，但可靠性强
- 异步刷盘：采用后台线程的方式进行刷盘，降低了读写延迟，提高了MQ吞吐量
  - 但异步刷盘影响了消息的可靠性

### 同步复制与异步复制

同步刷盘与异步刷盘是针对单个节点层面的，但同步复制与异步复制是指：Borker 主从模式下，**主节点返回消息给客户端的时候是否需要同步从节点**。

- 同步复制： 也叫 “同步双写”，也就是说，只有消息同步双写到**主从节点**上时才返回写入成功 。
- 异步复制： 消息**写入主节点之后就直接返回写入成功**。

注意：异步复制并不会影响消息的可靠性，只是影响了该Broker集群的**可用性**。
因为RocketMQ**不支持自动主从切换**，当Topic的主节点挂掉之后，生产者就不能再给这个主节点生产消息了，需要切换到从节点进行读操作

如果采用的是异步复制，在主节点挂掉之后，会有一部分数据没有同步到从节点，会导致这一段时间产生主从不一致的问题，也降低了可用性。但会在主节点重启后继续复制这部分数据，而不会因自动主从切换而导致丢失。

> master挂掉以后，slave提供读服务，但是不能写入信息

## RocketMQ存储机制

RocketMQ的存储架构中，主要包含三大角色：
- CommitLog：消息主体以及元数据的存储主体，存储Producer写入的消息的主题内容，CommitLog是定长的。
  - 单个文件大小默认1G ，文件名长度为20位，左边补零，剩余为起始偏移量。消息主要是顺序写入日志文件，当文件满了，写入下一个文件。
- ConsumeQueue：消费消息队列，引入的目的主要是**提高消息消费的性能**。
  - 消息消费是针对Topic进行的，如果要通过遍历 `commitlog` 文件的方式实现根据 Topic 检索消息，这是非常低效的。
  - 因此，Consumer 即可根据 `ConsumeQueue` 来**查找待消费的消息**。
  - ConsumeQueue作为**队列中消息的索引**，保存了指定队列中，消息在 CommitLog 中的起始物理偏移量 offset ，消息大小 size 和消息 Tag 的 HashCode 值。
- IndexFile：提供了一种可以通过key或时间区间来查询消息的方法

RocketMQ 采用的是 **混合型**的存储结构 ，即 ==Broker单个实例下**所有的队列共用一个日志数据文件**来存储消息==，而Kafka会为每个topic分配一个存储文件。

这是为了提高写入效率：不按Topic划分文件，则更有可能进行成批数据的写入，但遍历时会比较麻烦。
因此RocketMQ采用`ComsumeQueue`作为每个队列的索引文件，从而提高读取消息的效率。

## 为什么CommitLog要设计成定长的？



## 实践中的常见问题

### 消息丢失的问题

当系统需要保证百分百消息不丢失，可以使用生产者每发送一个消息，**Broker 同步返回一个消息发送成功的反馈消息**
即每发送一个消息，**同步落盘后才返回生产者消息发送成功**，这样只要生产者得到了消息发送生成的返回，事后除了硬盘损坏，都可以保证不会消息丢失

### 同步落盘怎么才能快

1. 使用 `FileChannel` + `DirectBuffer` 池，使用堆外内存，加快内存拷贝
2. 使用数据和索引分离，当消息需要写入时，使用 `commitlog` 文件顺序写，当需要定位某个消息时，查询`index` 文件来定位，从而减少文件IO随机读写的性能损耗

### 消息堆积的问题

1. 后台定时任务每隔72小时，删除旧的没有使用过的消息信息
2. 根据不同的业务实现不同的丢弃策略，具体参考线程池的 `AbortPolicy`，例如FIFO/LRU等（RocketMQ没有此策略）
3. 消息定时转移，或者对某些重要的 TAG 型（支付型）消息真正落库




### RocketMQ 不使用 ZooKeeper 作为注册中心的原因，以及自制的 NameServer 优缺点？

1. ZooKeeper 作为支持顺序一致性的中间件，在某些情况下，它为了满足一致性，**会丢失一定时间内的可用性**，RocketMQ 需要注册中心**只是为了发现组件地址**。在某些情况下，**RocketMQ 的注册中心可以出现数据不一致性**，这同时也是 NameServer 的缺点，因为 NameServer 集群间互不通信，它们之间的注册信息可能会不一致
2. 另外，当有新的服务器加入时，NameServer 并不会立马通知到 `Producer`，而是由 Producer 定时去请求 NameServer 获取最新的 Broker/Consumer 信息（这种情况是通过 Producer 发送消息时，负载均衡解决）

# RabbitMQ

## RabbitMQ的数据结构

![](./../images/消息队列/rabbitMQ.jpg)

RabbitMQ的Broker中，主要包含两个角色：
- Exchange：消息首先经过Exchange，再被分配到对应的Queue中
- Queue：消息队列


## Exchange有哪些模式

RabbitMQ的发布订阅模式与之前模型的区别：通过加入交换机（exchange），实现将统一消息发送给多个消费者

注意：交换机不能缓存消息，如果路由失败，则消息丢失

- RoutingKey：生产者将消息发给交换器的时候，一般会指定一个 `RoutingKey`，用来**指定这个消息的路由规则**，根据路由器类型与`BindingKey`联合，从而将消息路由到该Exchange的队列中。

### Direct

将接收到的消息**根据路由规则路由到指定的Queue**，称为路由模式（routes）

Direct模式下，Exchange将消息路由到BindingKey与RoutingKey完全匹配的队列中

- 每一个`Queue`与`Exchange`设置一个BindingKey
- 发布者发送消息时，指定消息的RoutingKey

### Fanout交换机

将接收到的消息路由到**每一个绑定的Queue**，即**广播类型**。

由于不需要任何判断操作，速度非常快

### Topic

与Direct类似，区别在于RoutingKey与BindingKey匹配的规则：
- **routingKey必须是多个单词的列表，并且以 . 分割**
- BindingKey和RoutingKey一样，也是`.`分割的字符串
- BindingKey中可以进行模糊匹配


例如：`china.news` \ `china.weather` \ `japan.news`
可以通过`*.news`/`#.news`来进行模糊匹配
> 

### Headers

不推荐。
不依赖于路由键的匹配规则来路由消息，而是根据发送的消息内容中的 headers 属性进行匹配。

## RabbitMQ怎么消息的有序性

如果生产者很快地发送了两条消息，这两条消息可能由于网络情况，导致Broker收到的顺序与发送的顺序不同，导致消息乱序

rabbitMQ实现消息的有序性的方式和kafka类似，kafka是规定一个partition只能被一个消费者组中的一个消费者节点订阅，并在收到新的消息时，按顺序插入Partition内的消息队列的尾部，并通过offset来维护已完成消费的消息的偏移量，从而实现单个Partition的消息有序

因此，rabbitmq要实现消息的有序，需要进行如下改进：
1. 将需要保持先后顺序的消息放到同一个消息队列中
2. 只用一个消费者去消费该队列，同一个queue里的消息一定是顺序消息的
3. 在收到消费者的ack后，才将下一条消息推给消费者

## RabbitMQ如何保证数据不重复消费？

消费者在消费一条消息后，会返回ACK给Broker，如果这个消息由于网络状况丢失了，或者消费者准备返回ACK时挂掉了，就会导致Broker收不到ACK，并再次发送这条消息给消费者，导致重复消费

关键是保证**消息消费的幂等性**

- 如果是对数据库的insert，就给这个消息的操作涉及的列加上一个唯一性约束，如果重复消费，则冲突报错
- 如果是对redis的set，则redis的set可以保证天然的幂等性
- 其他情况，可以通过一个第三方的存储介质，来进行消费记录，例如放到一个分布式缓存里，通过给消息分配全局唯一的ID，消费者消费时，只要去查查看有没有这个记录就行

## 怎么保证消息的可靠性，即消息不丢失

消息从生产者到`exchange`再到`queue`，最后到`消费者`，只要有消息传递的地方，就有可能丢失消息。也可能因为mq宕机导致消息丢失

- 发送时丢失
- MQ宕机导致的丢失
- consumer接收消息后未消费就宕机

### 生产者消息确认

RabbitMQ通过`publicher confirm`机制避免消息发送到MQ过程中丢失。

**消息发送到MQ后，会返回一个结果给发送者，表示消息是否处理成功**：
- publisher-confirm，发送者确认
  - 消息成功投递到exchange，返回ack
  - 未投递到exchange，返回nack
  - 发送过程出现异常，没有收到回执
- publisher-return，发送者回执
  - 消息投递到交换机，但没有路由到队列，调用ReturnCallback，返回ACK与路由失败原因

### 消息持久化

MQ默认是**内存存储消息**，开启持久化功能可以确保缓存在MQ中的消息不丢失

1. 交换机持久化（默认持久）

2. 队列持久化（默认持久）

3. 消息持久化，SpringAMQP中的消息默认是持久的，可以通过MessageProperties中的DeliveryMode指定；

### 消费者消息确认

消费者确认：**消费者处理消息后向MQ发送ack回执，MQ收到ack回执后才删除消息**

SpringAMQP允许配置三种确认模式：
- manual：**手动ack，需要在业务代码结束后，调用api返回ack**
- auto：自动ack，由spring检测listener代码是否出现异常，没有则返回ack，异常则nack（通过AOP实现）
- none：关闭ack，**MQ假定消费者获取消息后会成功处理，因此消息投递后立即删除**

### 消费失败重试机制

**消费者处理消息失败后，会返回nack给MQ，而MQ会继续发送给消费者**，导致大量不必要的循环

因此，可以利用Spring的retry机制，在消费者出现异常时进行本地重试

```yaml
spring:
  rabbitmq:
    listener:
      simple:
        prefetch:   1
          retry:
            enabled: true #开启消费者失败重试
            initial-interval: 1000 #初始的失败等待时长为1s
            multiplier: 2 #下次失败的等待时长倍数
            max-attempts: 3 #最大重试次数，耗尽后丢弃，返回reject
            stateless: true #true：无状态，false：有状态（包含事务则应为有状态）
```

如果重试次数耗尽后依然失败，则需要有MessageRecoverer接口来处理，它包含三种不同的实现：
- RejectAndDontRequeueRecoverer：重试耗尽后，直接reject，丢弃消息（缺省值）
- ImmediateRequeueMessageRecoverer：重试耗尽后，返回nack，消息重新入队
- RepublishMessageRecoverer：重试耗尽后，将失败消息投递到指定的交换机（一个专门转发失败的消息的交换机，转发给error.queue）

## 延迟消息问题

死信交换机

当一个队列中的消息满足以下条件之一时，可以成为**死信**：
- 消费者使用`basic.reject`或`basic.nack`声明消费失败，并且消息的`requeue`参数设置为false（即失败后不重新入消息队列，而是抛弃）
- 消息是一个过期消息，超时无人消费
- 要投递的队列消息堆积满了，最早的消息可能成为死信

如果该队列配置了`dead-letter-exchange`属性，指定了一个交换机，那么队列中的死信就会投递到这个交换机中，即死信交换机（Deat Letter Exchange，简称DLX）

> 每条消息是否过期是在即将投递到消费者之前判定的。

> 延迟队列：
> 不是及时的队列，也就是发送者发出的消息要延迟一段时间，消费者才能接收到

### 消息超时

消息超时的配置方式：
- 给队列配置ttl，进入队列后超过ttl时间的消息变为死信
- 给消息配置ttl，队列接收到消息超过ttl时间后变为死信
- 两者共存时，以时间短的ttl为准

### 延迟队列

利用TTL结合死信交换机，实现消息发出后，消费者延迟收到消息的效果，即延迟队列模式

## 消息堆积问题

当生产者快于消费者，就会导致队列中的消息堆积，直到上限，**导致最早接受的消息成为死信，被抛弃，即消息堆积问题**。

除了**增加消费者处理能力、增加消费者数量、扩大队列容积**的方法外，还可以通过**惰性队列**（Lazy Queues）解决消息堆积的问题

惰性队列的特征：
- 接收到消息后==直接存入磁盘而非内存==
- 消费者**要消费消息时，才会从磁盘中读取并加载到内存（带来一定的延迟）**
- 支持数百万条消息的存储

## MQ可用性与并发问题

RabbitMQ的集群有两种模式：
- 普通集群：分布式集群，将队列分散到集群的各个节点，从而提高整个集群的并发能力
- 镜像集群：主从集群，在普通集群的基础上，增加了主从备份功能，提高集群可用性（主从同步不是强一致的，会有数据丢失的风险）
- 仲裁队列：用于替代镜像集群，采用Raft协议确保主从的数据一致性

### 普通集群

又称标准集群，具备以下特征：
- 会在集群的各个节点间共享部分数据，包括：交换机、队列元信息，不包含队列中的消息
- 访问集群某节点时，如果队列不在该节点，桂聪数据所在节点传递到当前节点并返回
- 队列所在节点宕机，队列中的消息就会丢失
  
RabbitMQ底层依赖于Erlang，Erlang默认支持集群模式，集群中的每个RabbitMQ节点使用Cookie来确定它们是否被允许互相通信

### 镜像集群

本质是**主备模式**，具备以下特征：
- **交换机、队列、队列中的消息会在各个mq的镜像节点之间同步备份**
- 创建队列的RabbitMQ节点被称为**该队列的主节点**，备份到的其他节点称为**该队列的镜像节点**
- 一个队列的主节点可能是另一个队列的镜像节点
- **所有操作都是主节点完成，然后同步给镜像节点（如果镜像节点收到消息，会转发给主节点来处理）**
- 主节点宕机后，镜像节点称为新的主节点

### 仲裁队列

镜像队列具有数据不一致的风险，因此用仲裁队列来替代，仲裁队列具备以下特征：
- 与镜像队列一样，**都是主备模式**，支持主从数据同步
- 使用简单，没有复杂的配置
- **主从同步基于Raft协议，强一致**








