
# 分布式基础

## 说说你对分布式的理解

分布式系统是由多个计算机服务器组成的系统，它是由原有系统进行服务的横向与纵向切分得到的，各个服务器进行协同工作，从而提供完整的服务。

分布式环境下面临的问题：
1. 网络通信：网络本身的不可靠性，因此会涉及到一些网络通信问题
2. 网络分区(脑裂)：当网络发生异常导致分布式系统中部分节点之间的网络延时不断增大，最终导致组成分布式架构的所有节点，只有部分节点能够正常通信
3. 三态：在分布式架构里面多了个状态：超时，所以有三态： 成功、失败、超时
4. 分布式事务：ACID(原子性、一致性、隔离性、持久性)
5. 中心化和去中心化：冷备或者热备


## CAP理论

- C：一致性(Consistency)：在分布式环境中，数据在多个副本之间是否能够保持数据一致的特性。
  >在一致性的要求下分布式系统完成某写操作后的任何读操作，都应获取到最新写入的值
- A：可用性(Availability)：系统一直可以提供正常的服务，不会出现系统操作失败或访问超时等问题
- P：分区容错性(Partition tolerance)：大多数分布式系统都分布在多个子网络。每个子网络就叫做一个区，分布式系统遇到**任何**节点或网络分区故障时，分区之间可能无法通信，**此时整个系统仍然能够对外提供满足一致性与可用性的服务**，即**部分故障不影响整体使用**

> 数据库的ACID：
> A：原子性(Atomicity)：事务是一个不可再分割的工作单元，一条SQL默认是一个单独事务，且是自动提交的
> C：一致性(Consistency)：事务开始前和结束后，数据库的完整性的约束没有破坏，即数据库事务不能破坏关系数据的完整性与业务逻辑的一致性
> I：隔离性(Isolation)：多个事务并行访问时，事务之间是隔离的
> D：持久性(Durability)：事务完成后，该事务对数据库所作的修改会持久的保存在数据库中
> > ACID保证数据库中，对字段操作的一致性
> > CAP保证同一数据在分布式系统中，不同服务器上的拷贝是相同的，是一种逻辑保证 

CAP三者不可兼得，一般会根据业务进行取舍：
- CA：优先保证一致性与可用性，放弃分区容错，即放弃系统的扩展性，违背了分布式的初衷
- CP：优先保证一致性与分区容错性，放弃可用性。
- AP：优先保证可用性与分区容错性，放弃一致性

Zookeeper就是一个CP系统
> Zookeeper不能保证每次服务请求的可用性（极端情况下，zk会丢一一些请求，消费者程序需要重新请求才能得到结果）
> 进行Leader选举时，集群是不可用的

## BASE理论

BASE理论是对CAP的一种解决思路：
- Basically Available（**基本可用**）：分布式系统故障时，**允许损失部分可用性**，即保证核心可用
- Soft State（**软状态**）：在一定时间内，允许出现中间状态，即**临时的不一致**
- Eventually Consistent（**最终一致性**）：在软状态结束后，保证**最终的数据一致**

分布式事务最大的问题就是各个子事务的一致性，因此可以借鉴CAP定理和BASE理论：
- AP模式：各个子事务分别执行和提交，**允许出现结果不一致，然后采用弥补措施进行数据恢复**，实现最终一致
- CP模式：各个子事务执行后互相等待，**同时提交，同时回滚，达成强一致**。但在事务等待过程中，处于弱可用

## 一个分布式项目，至少需要有哪些模块

微服务网关、注册中心、配置中心、RPC、服务保护组件

# 分布式事务

## 什么是分布式事务

分布式系统会把一个应用系统拆分为可独立部署的多个服务，因此需要服务与服务之间远程协作才能完成事务操作。
这种**分布式系统环境下由不同的服务之间通过网络远程协作完成的事务**称之为分布式事务

## 刚性事务与柔性事务

- 柔性事务：
  - 追求数据的最终一致性，允许在业务过程中，出现短暂的数据不一致状态，保证了高可用与分区容错性（AP），例如TCC、SAGA、MQ事务、本地事务表
- 刚性事务：
  - 追求事务的**强一致性**，对性能的影响较大，例如2PC与3PC（CP）

## 两阶段提交（2PC）协议

2PC即两阶段提交协议，是将整个事务流程分为两个阶段：**准备阶段（Prepare phase）**、**提交阶段（commit phase）**

整个事务过程由**事务协调者**和**参与者**组成：
- 事务协调者负责决策整个分布式事务的提交和回滚
- 参与者负责自己本地事务的提交和回滚

在计算机中部分关系数据库如 Oracle、MySQL 支持两阶段提交协议，如下图：
1. 准备阶段：**事务协调者**给每个**参与者**发送 `Prepare` 消息，**每个数据库参与者在本地执行事务**，并写本地的 `Undo/Redo` 日志，并在完成后返回成功消息给协调者，此时事务没有提交。（失败则返回失败消息）
   - 可以理解为，在准备阶段完成**提交之外的工作 **
2. 提交阶段：
   - 如果事务协调者**收到了参与者的执行失败或者超时消息**时，直接给每个参与者发送**回滚（`Rollback`）消息**；**否则，发送提交（`Commit`）消息**；
   - 参与者根据事务协调者的指令执行提交或者回滚操作，并释放事务处理过程中使用的锁资源，完成提交或回滚之后，返回ACK消息给协调者。注意：**必须在最后阶段释放锁资源**。

![](./../images/分布式/2PC-成功.jpg)

存在的问题：
- 同步阻塞：
  - 在第一阶段，要等待所有参与者都返回了OK，才能进入第二阶段。
  - 而**事务参与者会一直锁住占用的资源**，例如A转账给B，但其他事务需要操作A或B，就会造成阻塞。
- 单点问题：
  - 如果事务协调者在第一阶段（准备）发送请求之后挂掉了，事务参与者就会一直阻塞住，不知道下一步的操作
- 二将军问题：
  - 在提交阶段，协调者向所有参与者发送了提交消息，如果有一个参与者未返回这个提交消息的ACK，协调者完全不知道参与者发生了什么状况（可能没收到消息，也可能是收到了，并成功提交，但返回的ACK因为网络故障丢失了），也就无法决定是否需要进行全员的回滚
- 数据不一致：
  - 可能由于事务协调者/参与者宕机，导致数据不一致，例如在第二阶段（提交），部分网络出现问题，导致部分参与者收不到commit/rollback消息，就会导致数据不一致


### 事务协调者故障处理

如果事务协调者故障了，可以通过**选举**等操作，选出一个新的事务协调者

- 如果事务协调者在**准备阶段**挂了，**直接都回滚**就行
- 如果事务协调者在**提交阶段**挂了，如果**事务参与者都没挂**，则新的协调者可以**从参与者的状态推断下一步的操作**
  - 但**如果有参与者挂了**，就**难以得知其在挂掉前是否提交了事务**
    因此，可以**在实现时引入日志，记录参与者的操作**

但尽管加入了日志，在**极端情况**下还是**无法避免数据不一致**的问题：
- 如果**参与者在挂之前事务提交成功**，新协调者确定存活着的参与者都没问题，那肯定得向其他参与者发送提交事务命令才能保证数据一致。
- 如果**参与者在挂之前事务还未提交成功**，参与者恢复了之后数据是**回滚**的，此时协调者必须是向其他参与者发送**回滚**事务命令才能保持事务的一致。

## 三阶段提交（3PC）协议

三阶段提交：`CanCommit` 阶段、`PreCommit` 阶段、`DoCommit` 阶段，简称3PC

三阶段提交协议是2PC的改进版本。与两阶段提交不同的是，三阶段提交有两个改动点：
- 3PC ==将 2PC 的**准备阶段**拆分为**准备阶段**与**预提交阶段**==，
  - 在准备阶段，执行的任务仅仅是**询问参与者的状态**
    - 不会一来就干活并给资源上锁，使得在**某些资源不可用**的情况下所有参与者都阻塞着
  - 在预提交阶段，执行的才是**提交外的所有工作（类似于2PC的准备阶段）**
    - 预提交阶段的引入起到了一个**统一状态**的作用，它像一道栅栏，表明**在预提交阶段前所有参与者其实还未都回应**，**在预处理阶段表明所有参与者都已经回应**
    
- 引入==超时==机制。**在协调者和参与者中都引入超时机制**。
  - 由于3PC引入超时机制，如果**参与者等待`commit/rollback`命令超时**，则**默认进行事务提交**
    - 但这时可能导致数据不一致（因为**实际上可能是要进行事务回滚**）
    > 在2PC中，如果事务协调者在提交请求还未发出去的时候宕机了，就会导致**所有的事务参与者阻塞等待**
- 当三个阶段的任意一个失败或者等待超时，执行`RollBack`。保证了在最后提交阶段之前各参与节点的状态是一致的

![](./../images/分布式/3PC.png)

3PC 的引入是为了解决**提交阶段 2PC 协调者和某参与者都挂了**之后，新选举的协调者无法知道所有事务参与者的情况，就不知道当前应该提交还是回滚的问题：

- 在3PC中，经过**准备阶段**的确认，即使协调者挂了，参与者也知道自己所处预提交阶段，因为已经得到准备阶段所有参与者的确认了。
- 新协调者来的时候发现**有一个参与者处于预提交或者提交阶段**，那么表明**已经得到了所有参与者的确认**了，所以此时执行的就是**提交**命令

缺点：
2PC是尽量保证强一致性的分布式事务方案，是同步阻塞的，效率较低。
而3PC在2PC的基础上增加了一个新的阶段，引入了额外的交互，但**只是为了解决提交阶段协调者与部分参与者挂掉时，新的协调者不知道该干啥的问题**，为了这个概率很低的问题，进一步降低了性能，得不偿失

## TCC事务（补偿事务）

**2PC 和 3PC 都是数据库层面的，而 TCC 是业务层面的分布式事务。**

TCC事务是`Try`、`Confirm`、`Cancel`三种指令的缩写，其逻辑模式类似于2PC，但是实现方式是**在代码层面人为实现**。
- `Try`：尝试执行，**需要完成业务检查，并锁定需要的业务资源**
- `Confirm`：确认执行，协调者调用所有服务的confirm接口，**各个服务执行事务并提交**
  - 如果Try成功，就会执行Confirm，
  - 如果任何一个服务的try接口失败或超时，则调用cancel接口
- `Cancel`：取消执行，**资源预留的撤销，为把Try阶段的预留资源撤销了**

![](./../images/分布式/TCC事务.png)

**TCC是一种服务层面上的2PC，那么他是如何解决 2PC 无法应对宕机问题的缺陷的呢？**

答案是：不断重试。

由于 `try` 操作锁住了全局事务涉及的所有资源，**保证了业务操作的所有前置条件得到满足**，因此无论是 confirm 阶段失败还是 cancel 阶段失败，**都能通过不断重试直至 confirm 或 cancel 成功**

由于需要多次重试`confirm`与`cancel`，就需要==保证这两个操作的幂等性==，即整个事务中不论执行了多少次`confirm`或`cancel`，效果都等同于只执行了一次

TCC的难点在于业务上的定义，**对于每一个操作你都需要定义三个动作分别对应Try - Confirm - Cancel**。
因此，**TCC对业务的侵入较大和业务紧耦合**，需要根据特定的场景和业务逻辑来设计相应的操作。


## SAGA

SAGA属于长事务的解决方案，核心思想就是将长事务拆分为包含多个本地短事务的事务序列，逐个执行每个短事务，达到执行长事务的效果，且**每个短事务都有对应的补偿操作，用于执行失败后的补偿**

SAGA也是需要业务开发者自己实现的分布式事务模式，但**SAGA没有Try动作，其中的本地短事务会被直接提交**，如果**出错则通过补偿操作来补偿**。

> 补偿操作通常都是一定能够成功的，但如果因为服务器宕机等原因，补偿失败的话，就需要人工干预了。
> 因此，还需要对各个本地短事务进行日志记录，从而知道哪些短事务成功了


## 本地消息表

本地消息表其实就是利用了 **各系统本地的事务** 来实现分布式事务。

本地消息表顾名思义就是会有一张存放本地消息的表，一般都是放在数据库中

在执行业务的时候：
1. **将业务执行和消息放入消息表中的操作放在同一个事务中**，这样就能保证消息放入本地表中业务肯定是执行成功的。
2. 调用下一个操作，如果下一个操作调用成功，则将消息表的消息状态直接改成已成功。
3. 调用失败，会有 **后台任务定时去读取本地消息表**，筛选出还未成功的消息**再次调用对应的服务**，服务更新成功了再变更消息的状态。

4. 这时候有可能消息对应的操作不成功，因此**也需要重试**，重试就得保证对应服务的方法是**幂等**的，而且一般重试会有最大次数，超过最大次数可以记录下报警让人工处理。

可以看到本地消息表实现的是**最终一致性**，容忍了数据暂时不一致的情况。

## RocketMQ事务

无论是 2PC & 3PC 还是 TCC、事务状态表，基本都遵守 XA 协议的思想。
即这些方案本质上都是**事务协调者协调各个事务参与者的本地事务的进度**，使所有本地事务共同提交或回滚，最终达成一种全局的 ACID 特性。
协调者需要收集各个本地事务的状态，并根据这些状态发起下一步的指令。

**但是这些全局事务方案由于操作繁琐、时间跨度大，或者在全局事务期间会排他地锁住相关资源，使得整个分布式系统的全局事务的并发度不会太高。**

因此，开发者研发出很多**与XA协议背道而驰的分布式事务解决方案**，例如利用消息中间件实现的最终一致性事务。

利用**RocketMQ**来**异步完成事务的后半部分更新**，实现系统的**最终一致性**。这个方式避免了像XA协议那样的性能问题。

下面的图中，使用RocketMQ完成事务在分布式的另外一个子系统上的操作，保证了动作一致性。

![](./../images/分布式/MQ事务.png)

流程：
1. 发送方向RocketMQ的`Broker`发送半消息（即**事务消息**），发送成功后，**发送方开始执行本地事务**
   - 半消息是暂不能投递的消息：发送方已经将消息成功发送到了 MQ 服务端，但是**服务端未收到生产者对该消息的二次确认，此时该消息被标记成“暂不能投递”状态，即对消费者不可见**，处于该种状态下的消息即半消息。
2. 根据本地事务的结果向 Broker 发送 `Commit` 或者 `RollBack` 命令。 
   - **发送方**会提供一个**反查事务状态接口**，如果一段时间内，半消息没有收到任何操作请求，那么 `Broker` 会**通过反查接口得知发送方事务是否执行成功**，然后执行 `Commit` 或者 `RollBack` 命令。 
3. 如果是 `Commit`，那么订阅方（消费者）就能收到这条消息，然后再做对应的操作，做完了之后再消费这条消息即可。
   - 如果是 `RollBack`， 那么订阅方收不到这条消息，等于事务就没执行过。

要使用MQ事务，只需要定义好**反查事务状态接口**即可，同时，发送方需要在本地保存好事务的执行结果。

MQ事务实现的也是最终一致性，

**事务消息消费失败怎么办？**
**RocketMQ会自动重试消费，如果超过最大重试次数，则认为该消息有问题，将其放入死信队列**，需要人工矫正

Kafka事务：https://mp.weixin.qq.com/s?__biz=MzkxNTE3NjQ3MA==&mid=2247485732&idx=1&sn=3231cc9daf8bd5f8f29173adc145e5f6&scene=21#wechat_redirect

## 最大努力通知

最大努力通知其实只是表明了一种柔性事务的思想：我已经尽力我最大的努力想达成事务的最终一致了。


# 分布式Session

## 分布式Session是啥，有什么作用，为什么用

Session代表Server和Client的一次**会话过程**，**存储特用户会话所需的属性与配置信息**，**当用户在应用程序的Web页之间跳转时，存储在Session对象中的变量不会丢失**。

Session是**身份认证**的重要凭证，在用户登录后由服务器分发给用户，保存在客户端（客户端存个ID就可以了）与服务端，用户在请求时需要附带SessionID，以辨识自己的身份。
当服务器收到请求，需要创建Session对象时，会检查请求中是否包含SessionId，如果有，则直接根据该ID返回Session对象。

而分布式Session则是在分布式服务器环境下的Session，如果用户在 A 服务器登录了，第二次请求跑到服务 B 就会出现登录失效问题。

## 分布式Session的实现方式

分布式 Session 一般会有以下几种解决方案：

- **客户端存储**：
  - 直接将信息存储在cookie中，可保存不敏感信息
  - 但Cookie保存数据的大小与类型受限，且会增大网络开销

- **Nginx ip_hash 策略**：
  - 服务端使用 Nginx 代理，每个请求**按访问 IP 的 hash 分配**，这样**来自同一 IP 固定访问一个后台服务器**，避免了在服务器 A 创建 Session，第二次分发到服务器 B 的现象。
  - 但容易导致服务器的负载不均衡
- **Session 复制**：
  - 任何一个服务器上的 Session 发生改变（增删改），该节点会把这个 Session 的所有内容序列化，然后广播给所有其它节点。
  - 会严重增大网络开销，且占用内存较大
- **共享 Session**：
  - 服务端无状态化，将用户的 Session 等信息使用缓存中间件（如Redis）来统一管理，保障分发到每一个服务器的响应结果都一致。

# 分布式ID

## 分布式ID是什么

分布式 ID 是分布式系统下的 ID，它是在分布式系统中全局唯一的，需要满足以下要求：
- 全局唯一
- 高性能
- 高可用
- 方便易用
- 有序递增

## 分布式ID的常见解决方案

### 基于数据库的分布式ID

基于数据库的分布式ID生成算法有以下实现思路：

- 数据库主键自增
  - 这种方法比较直白，就是通过关系型数据库的**自增主键**来产生唯一的分布式ID
    - 创建一个数据表sequence_id，包含自增主键id与一个占位的`stub`
    - 通过`replace into sequence_id (stub) values ('stub);`来插入数据，如果主键或唯一索引字段出现重复数据错误而插入失败时，先从表中删除含有重复关键字值的冲突行，然后再次尝试把数据插入到表中。
  - 特点：**实现简单，但每次都要访问数据库，性能较差，并发量不大，存在单点问题、安全问题**

- 数据库号段模式
  - 从数据库中一次获取一个**序号段**，需要使用时直接从号段中取
    - 数据表中字段包含：
      1. id：主键
      2. current_max_id：当前最大ID
      3. step：号段长度，与current_max_id一起用于号段的获取
      4. 版本号：用于解决乐观锁的ABA问题
    - 每次获取号段时，通过更新数据表中的内容即可

- NoSQL（Redis）
  - 通过`incr`命令实现对id的原子顺序递增

### 分布式ID生成算法

**UUID**
  - UUID 包含 32 个 16 进制数字（8-4-4-4-12），值得注意的是，不同版本的UUID生成算法是不同的：
    - 版本1：根据时间和节点ID（通常是MAC地址）生成
      - 需要反向解析MAC地址则使用
    - 版本2：根据标识符（通常是用户ID）、时间和节点ID生成
    - 版本3：通过散列（hashing）名字空间（namespace）标识符和名称生成；
    - 版本4：**使用随机性或伪随机性生成**
    - 版本5：和版本3类似，但根据特定的值生成，而且在值不变的情况下生成的 UUID 不变

![版本1的UUID](./../images/分布式/UUID.png)

UUID可以保证生成的分布式ID的唯一性，因为其生成规则包括 **MAC 地址、时间戳、名字空间（Namespace）、随机或伪随机数、时序**等元素，重复的概率极低

> 每秒产生10亿的UUID，100年后有一个UUID重复的概率为50%

但是其实UUID现在用的并不多：
- UUID不适合作为数据库的主键，因为它太长了（**包含32个16进制数字，共128位**）
- UUID是**无顺序的**，InnoDB 引擎下，数据库**主键的无序性会严重影响数据库性能**
- 不安全，**基于MAC地址生成，会导致MAC地址泄露**
- **没有具体业务含义、需要解决重复 ID 问题**（如果机器时间不对，可能导致产生重复ID）

**雪花算法**
  - 由**64位的二进制数字**组成，其中每一部分都有特定的含义：
    - 第0位：符号位（表示正负），始终为0
    - 第1~41位：**时间戳**，单位是毫秒，可以支撑 `2 ^41` 毫秒，即可以使用69年
    - 第42~52位：高 5 位表示数据中心 ID，低 5 位表示工作节点 ID，从而区分不同的集群的节点
    - 第53~63位：计数序列号，序列号为**自增值**，代表**单台机器每毫秒能够产生的最大 ID 数**，也就是说**单台机器每毫秒最多可以生成 4096 个唯一ID**

通过雪花算法生成的分布式id，不仅安全、占用空间小，且其序号在单机上是自增的。且分布式ID中的各个字段不再是无意义的了

雪花算法是基于时间一致性的，如果系统时间有误，可能出现问题


# 分布式锁

## 分布式锁应该具备哪些条件

1. 在分布式系统环境下，一个方法在同一时间只能被一个机器的一个线程执行； 
2. 高可用的获取锁与释放锁； 
3. 高性能的获取锁与释放锁； 
4. 具备**可重入**特性； 
5. 具备**锁失效**机制，防止死锁； 
6. 具备**非阻塞锁与阻塞锁**特性，供用户自己选择，
   - 非阻塞：没有获取到锁将直接返回获取锁失败
   - 阻塞：获取锁失败则阻塞住，直到成功获得或超时才返回

## 分布式锁的几种实现

1. 基于**数据库唯一性约束**的分布式锁； 
   - 核心思想：通过**唯一索引**实现锁
     1. 在数据库中创建一个表，表中包含方法名等字段，对方法名字段加上**唯一性约束**，并在方法名字段上创建唯一索引
     2. 想要执行某个方法，就使用这个方法名向表中插入数据，成功插入则获取锁，执行完成后删除对应的行数据释放锁。 
     
   - 问题：
     1.  锁对**数据库可用性**的依赖很强，数据库是一个单点，一旦数据库挂掉，会导致业务系统不可用
     2.  锁**无法设置失效时间**，如果获得锁的线程挂掉了，就会导致无法释放锁
     3. 锁是==非阻塞==的，因为数据的insert操作，一旦插入失败就会**直接报错**。没有获得锁的线程并不会进入排队队列，要想再次获得锁就要再次触发获得锁操作
     4.  锁是**不可重入**的
 - 在上述方法中，是使用增删记录的方式实现分布式锁，除此之外，还可以**借助数据库本身的锁来实现分布式锁（排他锁）**
   - 在查询语句后面增加`for update`，数据库**会在查询过程中给数据库表增加排他锁**。
     - 当某条记录被加上排他锁之后，其他线程无法再在该行记录上增加排他锁。
   - 基于排他锁的分布式锁可以解决**阻塞锁**（for update语句执行失败则会阻塞，直到成功才返回）与服务器宕机的问题（获得锁的服务器宕机后，数据库会自己释放锁）


2. 基于缓存（Redis等）实现分布式锁； 
   - 核心思想：
     - 通过`SETNX` 加锁，并使用`expire`命令设置超时时间，锁的`value`值为一个随机生成的UUID，**在释放锁的时候通过判断UUID是否一致来进行判断**。
   - 实现简单，性能最好
   - 但`SETNX`和`expire`分两步执行，**不是原子操作**，==可能由于expire前宕机，导致死锁，且不支持阻塞等待、不可重入==
3. 基于Redis Lua脚本实现分布式锁
   - 同上，且逻辑更严密，**避免了宕机导致的死锁问题**，但**也不支持阻塞等待，也不可重入**
4. 利用Redission实现分布式锁
   - 通过lua语句实现复杂的逻辑，实现了可重入锁
   - 但在redis主从架构中会出现问题：写入master，master同步给slave前宕机，slave选举为master，则又可以加一次锁 
5. 基于Zookeeper的**临时有序节点**实现分布式锁；
   - 主要思想：
     - 每个客户端对某个方法加锁时，在zookeeper上的**与该方法对应的指定节点的目录下**，生成一个**唯一的临时有序节点**。 
     - 判断是否获取锁的方式很简单，只需要判断**有序节点中序号最小的一个**。当释放锁的时候，只需将这个瞬时节点删除即可
   - 实现步骤：
     1. 创建一个`目录mylock`； 
     2. 线程A想获取锁就在mylock目录下**创建临时顺序节点**； 
     3. 获取mylock目录下所有的子节点，然后获取比自己小的兄弟节点，**如果不存在，则说明当前线程顺序号最小，获得锁**； 
     4. 线程B获取所有节点，判断自己不是最小节点，设置**监听比自己次小的节点**； 
     5. 线程A处理完，删除自己的节点，线程B监听到变更事件，判断自己是不是最小的节点，如果是则获得锁。  
   - Zookeeper分布式锁的特性：
     - **集群部署，无单机问题**
       - ZK是集群部署的，只要集群中有半数以上的机器存活，就能对外服务
     - **宕机后锁自动释放**（通过临时节点实现）
       - 在创建锁的时候，客户端会在ZK中创建一个临时节点，**一旦客户端获取到锁之后突然挂掉（Session连接断开），那么这个临时节点就会自动删除掉**。其他客户端就可以再次获得锁。
     - **锁是阻塞的**
       - 客户端尝试加锁时，会在ZK中创建临时节点，并绑定监听器，之后陷入阻塞，一旦节点有变化，ZK会通知客户端，客户端可以检查自己的节点是不是序号最小的，从而实现阻塞锁的功能
     - **可重入**
       - 客户端在创建节点的时候，把当前客户端的主机信息和线程信息直接写入到节点中，下次想要获取锁的时候和当前最小的节点中的数据比对一下就可以了。
       - 如果是自己的信息，则直接获得锁


## 假设在redis master 中设置分布式锁，这时候master挂了并且还没来得及把数据同步到slave中，这时候slave升级为了master，这种情况怎么解决？

这种情况下，会导致其他请求也可以获得分布式锁，redisson框架为了解决这个问题，提供了一个专门的类：RedissonRedLock，使用了Redlock算法。

RedissionRedLock的解决思路：
1. 需要搭建几套相互独立的redis环境，假如我们在这里搭建了5套。
2. 每套环境都有一个redisson node节点。
3. 多个redisson node节点组成了`RedissonRedLock`
4. 环境包含：单机、主从、哨兵和集群模式，可以是一种或者多种混合。

![](./../images/分布式/redLock.png)

RedissionRedLock的加锁流程如下：
1. 获取所有的redisson node节点信息，**循环向所有的redisson node节点加锁**，假设节点数为N，例子中N等于5。
2. 如果在N个节点当中，有**N/2 + 1**个节点加锁成功了，那么整个RedissonRedLock加锁是成功的。
3. 如果在N个节点当中，小于**N/2 + 1**个节点加锁成功，那么整个RedissonRedLock加锁是失败的。
4. 如果中途发现各个节点加锁的**总耗时大于等于设置的最大等待时间**，则直接**返回失败**。

引入的问题：
1. 需要额外搭建多套环境，申请更多的资源，需要评估一下成本和性价比
2. N个RedissionRedLock节点，最少要加锁`N / 2 + 1`次，才能加锁成功，增加了操作的时间成本

# 高可用系统设计

## 有哪些提高系统可用性的方法？

1. 注重代码质量，测试严格把关
2. 使用集群，减少单点故障
3. 限流
   - 监控应用流量的 QPS 或并发线程数等指标，当达到指定的阈值时对流量进行控制，以避免被瞬时的流量高峰冲垮，从而保障应用的高可用性
4. 超时和重试机制设置
   - 一旦用户请求超过某个时间的得不到响应，就抛出异常。
   - 在读取第三方服务的时候，尤其适合设置超时和重试机制。
   - 一般我们使用一些 RPC 框架的时候，这些框架都自带的超时重试的配置。如果不进行超时设置可能会导致请求响应速度慢，甚至导致请求堆积进而让系统无法再处理请求。重试的次数一般设为 3 次，再多次的重试没有好处，反而会加重服务器压力  
5. 熔断机制
   - 系统自动收集所依赖服务的资源使用情况和性能指标，当**所依赖的服务恶化或者调用失败次数达到某个阈值**的时候就迅速失败，让当前系统立即切换依赖其他备用服务  

6. 异步调用
   - 通过异步调用，可以实现用户请求完成之后就立即返回结果，具体处理我们可以后续再做
   - 使用异步之后我们可能需要 适当修改业务流程进行配合，比如用户在提交订单之后，不能立即返回用户订单提交成功，需要在消息队列的订单消费者进程真正处理完该订单之后，甚至出库后，再通过电子邮件或短信通知用户订单成功。 
7. 使用缓存

## 限流

### 固定窗口计数算法

固定窗口其实就是时间窗口。固定窗口计数器算法 规定了我们单位时间处理的请求数量。

实现思路：通过变量`counter`，记录当前时间窗口内，接口处理的请求数量，每隔固定时间重置为0.

**这种限流算法无法保证限流速率，因而无法保证突然激增的流量。**

### 滑动窗口计数算法

固定窗口计数的升级版，通过把时间以一定比例分片，细化了限流的统计粒度：假设需要限制为每分钟60个请求，可以将1分钟切分为6个窗口，每个窗口管理10s内的请求。如果当前窗口的请求计数总和超过了限制的数量的话就不再处理其他请求。

### 漏桶算法

把发请求的动作比作成注水到桶中，我们处理请求的过程可以比喻为漏桶漏水。

可以通过消息队列实现，用消息队列控制请求的执行效率

### 令牌桶算法

和漏桶算法类似，通过在桶内存储令牌，请求在被处理之前需要拿到一个令牌，请求处理完毕之后将这个令牌丢弃（删除）。
根据限流大小，按照一定的速率往桶里添加令牌。如果桶装满了，就不能继续往里面继续添加令牌了。

## 负载均衡

服务端负载均衡：应用于**系统外部请求**与**网关层**之间，可以使用软件或硬件实现
客户端负载均衡：应用于系统内部的不同服务器之间

- 传输层负载均衡：在TCP/UDP协议的基础上，基于数据包中的源端口地址、目的端口地址，实现负载均衡算法，将请求转发到后端服务器上
- 应用层负载均衡：在HTTP协议的基础上，读取报文的数据，根据读取到的数据内容（如URL、Cookie）做出负载均衡的决策
  

## 降级&&熔断

### 降级

- 服务降级指的是：当服务器压力剧增时，根据当前业务情况以及流量，对一些服务和页面进行策略性的降级，以释放服务器资源，从而保证核心任务的进行（双十一不能退款、查看旧订单）

降级的方式：
1. 延迟服务：比如买了东西之后，核心的下单业务正常执行，而延迟增加京豆的操作，将其放到缓存中，等到服务平稳后再执行
2. 在粒度范围内关闭服务（片段降级或服务降级）：例如关闭退款、历史订单查询的功能
3. 页面跳转（页面降级）：对于商品详情页面的信息推荐、物流查询等**异步加载请求**，如果这些请求较慢，则可以对它们降级
4. 写降级：比如秒杀抢购，可以只进行Cache的更新，然后异步地写入数据库，保证数据的最终一致性
5. 读降级：以多级缓存的模式为例，如果后端服务压力过大，可以降级为只读缓存

自动降级的分类：
- 超时降级：在配置好超时时间与超时重试次数的基础上，使用异步机制探测回复情况
- 失败次数降级：对一些不稳定的API，当调用失败的次数达到一定阈值时，自动触发降级，同样需要使用异步机制探测结果
- 故障降级：如果远程服务挂掉了，则进行降级，通过一些默认值、缓存数据、兜底措施替代挂掉的服务
- 限流降级：QPS超过阈值后，通过限流来限制访问量，当达到限流阈值时，后续请求会被降级：通过错误页提示重试。

### 熔断

- 熔断指的是：应对**微服务雪崩效应**的一种链路保护机制
  - 在微服务调用链路中，远端的一个服务响应过慢或不可用，会导致连锁性的服务崩溃，导致雪崩效应
  - 熔断机制就是针对这种情况，如果出现了上述不可用的节点，则**不再进行该节点的调用，快速返回错误的响应信息**，该服务恢复正常后，才重新将其加入调用链路。

**降级是为了应对系统自身的故障，而熔断是为了应对系统依赖的外部系统的故障**


## 灾备

**灾备 = 容灾+备份**。

- 备份 ： 将系统所产生的的所有重要数据多备份几份。
- 容灾 ： **在异地建立两个完全相同的系统**。当某个地方的系统突然挂掉，整个应用系统可以切换到另一个，这样系统就可以正常提供服务了。
  - 异地多活 描述的是**将服务部署在异地并且服务同时对外提供服务**。
和传统的灾备设计的最主要区别在于**“多活”**，即**所有站点都是同时在对外提供服务的**。异地多活是为了应对突发状况比如火灾、地震等自然或者人为灾害。


## 性能测试

常用指标：
- 响应时间
  - 响应时间就是用户发出请求到用户收到系统处理结果所需要的时间
  - 2-5-8原则：2-5秒响应：页面体验良好；5-8秒响应：还可以接受；8秒以上：裂开
- 并发数
  - 并发数是系统能同时处理请求的数目即同时提交请求的用户数目。
- 吞吐量
  - 吞吐量指的是系统单位时间内系统处理的请求数量。衡量吞吐量有几个重要的参数：QPS（TPS）、并发数、响应时间。
  - QPS：服务器每秒可以执行的查询次数
  - TPS：服务器每秒处理的事务数（这里的一个事务可以理解为客户发出请求到收到服务器的过程）

> QPS和TPS的区别：
> 对访问一次页面，形成一个TPS，但一次页面请求，可能产生多次对服务器的请求，就形成了多个QPS


