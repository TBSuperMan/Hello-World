# 题目

## 统计某一天，看视频次数排名前五的用户，扩展到重复的视频不算

```sql
select u.userid
from user u join view v
  on u.id = v.id
group by (u.userid)
order by count(u.userid)
limit 0,5;
```

## 给了两张mysql表，用sql写出查找是否二人为好友关系



## 计算某个字段的区分度

```sql
select count(distinct userid)/count(userid)
from table
```

## a表中有的b中没有的

```sql
select a.id
from a left join b
where b.id = null
```

## 查询出每个服务区的排名和某个用户的排名



## 实现行转列吧：
举个例子，student表
name, subject, score
ly, yuwen, 95 ----> ly, yuwen, 95, shuxue, 98
ly, shuxue, 98

## readerid，bookid，tagid，查出指定bookid最热门的10个标签，readerid越多，越热门
```sql
select tagid 
from tb 
where bookid=1 
group by tagid 
order by count(readerid) desc 
limit 10
```
## 给定student表
name age
A 18
B 18
C 21
D 19

查询年龄不重复的年龄
结果应为：
age
21
19

```sql
select age
from student
group by age
having count(age) = 1;
```

分段查询年龄的人数，10-19，20-29
结果应为
10-19 20-19
3 1
```sql


```


## 如果 MySQL 中有 10 亿条数据
怎么查询的？B+ 树上又是如何查询的？
自适应哈希对这种情况适用吗？
统计 10 亿条数据中最活跃的前 5 名，如何做？
10 亿条数据分库分表，怎么做？对那个字段分？
查询 TOP K 的 SQL

# 补充
## Mysql 三主如何同步？会造成无线同步循环吗？（我答按照链式无环的方式同步，面试官有问那是不是中间的一台负载会过高，我同意但是想不出其他方案）

三主同步即三台MySQL服务器循环地构建主从关系：A->B, B->C, C->A




## 数据库加锁的系统调用是什么？



## 行锁是在什么情况下出现？

InnoDB的行锁是通过给索引上的索引项加锁实现的，因此，只有通过索引检索数据时，才使用行锁，否则使用表锁

当用范围条件，而不是相等条件查询，并请求共享或排他锁时，InnoDB会给符合条件的已有数据记录的索引项加锁；对于在范围内但不存在的记录，通过间隙锁加锁


## binlog是实现持久性的必须部件吗



## 为什么会有规定not null default？

1. NULL的比较比较麻烦，NULL是一种独立的数值类型，只能通过IS NOT NULL或IS NULL比较
2. NULL值会影响索引的效率

## MVCC怎么解决幻读问题的？

MVCC是在通过undo log和read view解决可重复读的基础上，进一步通过next-key lock解决幻读的

- MVCC解决快照的幻读问题
- next-key lock解决当前读的幻读问题


## 发生外排序（filesort）的根本原因？

因为在排序时，order by的列，无法利用索引进行排序，就只能进行filesort


## java的乐观锁(CAS)，悲观锁(JUC)；mysql的乐观锁(undo_log)和悲观锁(当前读)；

MySQL的乐观锁体现为进行数据库操作时，认为操作不会冲突，不通过锁或其他方式进行特殊处理，而是在更新之后，判断是否有冲突

悲观锁则是

## mysql乐观锁和悲观锁的隔离级别



## 乐观锁到底有锁吗



## mysql进程放在宿主机和Docker中区别，优缺点


## Docker之间的通信？



## 联表查询join原理，两个表join为例（没了解，Nested-Loop Join）



## 你自己如何实现呢？（两张表=两个对象，各取一个相同字段，等值连接，求并结果集）



## 在数据库中，自增ID与UUID的区别（叶分裂），自增ID申请完了会发生什么事情（分布式雪花算法ID）

如果是自己设置的自增ID，或MySQL选中的UNIQUE列，则会在用完之后报错

如果是MySQL为我们生成的row ID，则会在用完之后从头开始，继续使用

使用UUID的话，由于UUID是无规则的，随机性强，就会导致插入过程中经常发生叶分裂的现象，导致消耗很高

https://blog.csdn.net/loney_island/article/details/120689448

## 聚簇索引一定是主键索引吗

https://blog.csdn.net/easybo0m/article/details/112647351


## SQL注入是什么，如何避免SQL注入？

SQL 注入就是在用户输入的字符串中加入 SQL 语句，如果在设计不良的程序中忽略了检查，那么这些注入进去的 SQL 语句就会被数据库服务器误认为是正常的 SQL 语句而运行，攻击者就可以执行计划外的命令或访问未被授权的数据。

**SQL注入的原理主要有以下 4 点**

* 恶意拼接查询
* 利用注释执行非法命令
* 传入非法参数
* 添加额外条件

**避免SQL注入的一些方法**：

- 限制数据库权限，给用户提供仅仅能够满足其工作的最低权限。
- 对进入数据库的特殊字符（’”\尖括号&*;等）转义处理。
- 提供参数化查询接口，不要直接使用原生SQL。

# 数据库基础

## 数据库的登陆流程，是怎样校验账号密码的



## MySQL执行查询的过程？

1. 建立连接与权限校验
   - 客户端通过TCP连接发送连接请求到mysql连接器，连接器会对该请求进行权限验证及连接资源分配
2. 缓存查询
   - 当判断缓存是否命中时，MySQL不会进行解析查询语句，而是**直接使用SQL语句和客户端发送过来的其他原始信息**。所以，任何字符上的不同，例如空格、注解等都会导致缓存的不命中。
3. 语法分析（SQL语法是否写错了）
   - 把语句给到预处理器，检查数据表和数据列是否存在，解析别名看是否存在歧义。
4. 优化
   - 是否使用索引，生成执行计划。
5. 交给执行器，将数据保存到结果集中，同时会逐步将数据缓存到查询缓存中，最终将结果集返回给客户端。

![](http://blog-img.coolsen.cn/img/image-20210220120155334.png)


https://zhuanlan.zhihu.com/p/270620662


## 关系型数据库和非关系型数据库区别

- 关系型数据库
    关系型数据库指的是使用**关系模型（二维表格模型）** 来组织数据的数据库。
    优点：
    1. 表结构贴近正常开发逻辑，容易理解
    2. 支持通用的SQL语句
    3. 丰富的完整性大大减少了数据冗余和数据不一致的问题。并且全部由表结构组成，文件格式一致
    4. 可以用SQL句子多个表之间做非常繁杂的查询；
    5. 提供对事务的支持，提供事务的恢复、回滚、并发控制、死锁问题的解决
    6. 数据存储在磁盘中，安全可靠


    缺点：
    1. 高并发读写能力差
    2. 海量数据下的读写效率低
    3. 可扩展性不足
    4. 数据模型灵活度低

- 非关系型数据库
    通常指数据以**对象**的形式存储在数据库中，而对象之间的关系通过每个对象自身的属性来决定，常用于存储非结构化的数据。

    优势：
    1. 支持多种存储格式
    2. 速度快、效率高，可以使用内存作为存储载体
    3. 海量数据的维护成本低
    4. 扩展简单、高并发、高稳定性、成本低
    5. 可以很方便的实现数据的分布式处理

    缺点：
    1. 不支持SQL语句
    2. 没有事务处理，无法保证数据的完整性与安全性
    3. 功能不如关系型数据库完善
    4. 复杂表的关联查询不易实现



## 数据库三范式，什么时候会进行反范式设计

假设有如下数据库：
| 仓库名 | 管理员 | 物品名 | 数量 |
|--|--| -- |--|
| 北京仓 | 张三 | a | 1 |
| 北京仓 | 张三 | b | 2 |
| 上海仓 | 李四 | c | 3 |
| 上海仓 | 李四 | d | 4 |

三范式：
1. 数据表的每个属性都是**原子性**的，不可再分
2. 非主属性`数量`**都依赖于候选键**：（仓库名、物品名）决定数量，（管理员、物品名）决定数量
   - 但是可能出现部分依赖 
3. 数据表中的非主属性，**不传递依赖于候选键**

然而，该表仍然存在插入、删除、更新时的异常：
> - 增加仓库，但不增加商品，导致主键`商品名`为空值，不符合主键要求，即插入异常
> - 仓库更换管理员，就可能一次修改数据表中的多条记录
> - 如果仓库里的商品售空，则仓库名与对应的管理员也会被删除

出现异常的原因：主属性`仓库名`对候选键（管理员、物品名）是**部分依赖**的关系

因此，提出了`BCNF`：在3NF的基础上==消除了主属性对候选键的部分依赖或传递依赖关系==

反范式设计：高阶的范式会进一步细分数据表，数据冗余度低，但有时会**为了效率，允许少量数据冗余**，通过空间换时间

例如：
商品评论表：
| 字段 |  |  |  |  |  |
|--|--|--|--|--|--|
| 含义 | 商品评论ID | 商品ID | 评论内容 | 评论时间 | 用户ID |

用户表：
| 字段 |  |  |  |
|--|--|--|--|
| 含义 | 用户ID | 用户昵称 | 注册时间 |

调用命令：查询某个商品ID，比如10001的前1000条评论，需要写成：
```sql
SELECT p.comment_text, p.comment_time, u.user_name 

FROM product_comment AS p 
    LEFT JOIN user AS u 
        ON p.user_id = u.user_id 

WHERE p.product_id = 10001 

ORDER BY p.comment_id DESC 

LIMIT 1000;
```

然而，在显示评论时，通常会显示用户的昵称，而不是ID，因此需要如上关联两张表进行查询，在数据量大时效率较低。
但如果我们允许==部分数据冗余==，在商品评论表中加上用户昵称，就只需要单表查询，即反范式设计


## 为什么要使用三范式？每个范式都是怎么设计的


## SQL语句的执行顺序

![](./../images/MySQL/SQL执行顺序.png)

```sql
// 1. FROM执行笛卡尔积，生成虚拟表VT1
FROM  <left_table>

// 2. 对虚拟表VT1应用ON筛选器，得到虚拟表VT2
ON    <join_condition>

// 3. JOIN添加外部行，即添加保留表中被ON语句过滤掉的行，而非保留表的值设置为NULL
<join_type> JOIN <right_table>

// 4. 应用WHERE过滤器，根据指定条件进行筛选
WHERE   <where_condition>


// 5. 将虚拟表中的记录进行分组
//    如果应用了GROUP BY，那么后面的所有步骤都只能得到的虚拟表VT5的列或者是聚合函数（count、sum、avg等）
//      原因在于：最终的结果集中只为每个组包含一行
GROUP BY  <group_by_list>

// 6. 计算聚合函数，计算从列中取得的值，返回一个单一的值

// 7. 应用HAVING过滤器
//      HAVING的作用和WHERE类似，但HAVING是过滤聚合值，而WHERE无法与聚合函数一起工作
HAVING  <having_condition>

// 8. 选出指定的列，并去重
SELECT DISTINCT   <select list>

// 9. 排列
ORDER BY  <order_by_condition>

// 10. 指定返回行数
LIMIT <limit_params>
```

## MySQL建表的约束条件有哪些？

- 主键约束（Primay Key Coustraint） 唯一性，非空性
- 唯一约束 （Unique Counstraint）唯一性，可以空，但只能有一个
- 检查约束 (Check Counstraint) 对该列数据的范围、格式的限制
- 默认约束 (Default Counstraint) 该数据的默认值
- 外键约束 (Foreign Key Counstraint) 需要建立两表间的关系并引用主表的列

## MySQL的binlog有有几种录入格式?分别有什么区别?

有三种格式：

- `statement`模式：
  - 记录==原始语句==
    - 由于sql的执行是有上下文的,因此在保存的时候需要保存相关的信息,同时还有一些使用了函数之类的语句无法被记录复制.
- `row`模式：
  - 记录表中==每一行的改动==，但是当表结构发生变化的时候,会记录语句而不是逐行记录.
    - 基本是可以全部记下来但是由于很多操作,会导致大量行的改动(比如alter table),因此这种模式的文件保存的信息太多,日志量太大。
- `mixed`。 一种折中的方案,
  - 普通操作使用`statement`记录,**当无法使用statement的时候使用row.**


## MySQL中myisam与innodb的区别?

https://blog.csdn.net/helloxiaozhe/article/details/88601028

* InnoDB支持**事务**，而MyISAM不支持事务
* InnoDB支持**行级锁**，而MyISAM支持表级锁
* InnoDB支持**MVCC**, 而MyISAM不支持
* InnoDB支持**外键**，而MyISAM不支持
* InnoDB不支持全文索引，而MyISAM支持。

## InnoDB 存储引擎的数据组织形式

https://juejin.cn/post/6968264298208428046



# 索引

## 索引是什么？

索引是一种特殊的文件（InnoDB数据表上的索引是表空间的一个组成部分），它们包含着对数据表里所有记录的引用指针。

索引是帮助MySQL高效获取数据的数据结构，是**数据表的目录**，在查找内容之前可以先在目录中查找索引位置，以此**快速定位查询数据**。

## 索引能干什么?有什么好处？

当表中的数据量越来越大时，索引对于性能的影响愈发重要。索引能够轻易将查询性能提高好几个数量级，总的来说就是可以**明显的提高查询效率**。




## 索引有哪些优缺点？

**索引的优点**

* 可以大大加快数据的检索速度，这也是创建索引的最主要的原因。
* 通过使用索引，可以在查询的过程中，使用优化隐藏器，提高系统的性能。

**索引的缺点**

* 时间方面：创建索引和维护索引要耗费时间，具体地，当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，会降低增/改/删的执行效率；
* 空间方面：索引需要占物理空间。

## 哪些列上适合创建索引？创建索引有哪些开销？

**经常需要作为条件查询的列**上适合创建索引，并且该列上也**必须有一定的区分度**。
创建索引需要维护，在插入数据的时候会重新维护各个索引树（数据页的分裂与合并），对性能造成影响

## 索引这么多优点，为什么不对表中的每一个列创建一个索引呢？

1. 当对表中的数据进行增删改的时候，索引也要动态的维护，这样就降低了数据的维护速度。
2. 索引需要占物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要建立聚簇索引，那么需要的空间就会更大。
3. 创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增加。

## 索引的种类有哪些？

1、从存储结构上来划分：`BTree`索引（B-Tree或B+Tree索引），Hash索引，full-index全文索引，R-Tree索引。这里所描述的是索引存储时保存的形式，

2、从应用层次来分：**普通索引，唯一索引，复合索引**

* 普通索引：即一个索引只包含单个列，一个表可以有多个单列索引
* 唯一索引：索引列的值**必须唯一，但允许有空值**
* 复合索引：多列值组成一个索引，专门用于组合搜索，其效率大于索引合并


3、根据中数据的物理顺序与键值的逻辑（索引）顺序关系：聚簇索引，非聚簇索引。
* 聚簇索引：并不是一种单独的索引类型，而是一种数据存储方式。
  * 具体细节取决于不同的实现，InnoDB的聚簇索引其实就是**在同一个结构中保存了B+Tree索引和完整数据行**。

* 二级索引： 不是聚簇索引，就是二级索引，二级索引中只保存了索引列与主键列的值


## InnoDB索引数据结构

InnoDB索引采用B+树作为数据结构，以聚簇索引为例，**树的叶子结点为数据页，非叶子结点为目录页**，其中数据页与目录页中分别存储完整的用户记录与目录项纪录。同一层的页之间以**双向链表**的形式连接。

在目录项纪录中，包含**页号**与**页的用户记录中的主键的最小值**，用于查询时的定位。

搜索流程：
1. 找到存储`目录项纪录`的页
2. 在`目录项记录页`中通过二分法找到对应的`目录项纪录`
3. 同上：找到`目录项纪录`后，就可以知道当前`用户记录所在的页`
4. 在`页`中二分找到`当前记录`即可

> 因为MySQL的存储单位为**页**，而在页中的尾部，保存着**描述当前页中纪录的相对位置的页目录**，页目录中包含**每个记录组中最后一条记录的地址偏移量**，即槽。
> 因此，通过==二分法==找到所需记录所在的槽后，就可以找到所需要的记录

根据实际情况，此时可能会再生成一个更高级的目录项，里面保存的是`更高级的目录项纪录`

除此之外，对于二级索引，目录项纪录包含**页号**、**页中记录的索引列的最小值**以及==主键值==（为了保证B+树的每层节点中，各条目录项纪录除了==页号==这个字段外的**其他字段组合是唯一的**），叶子结点存储的不是完整的用户记录，而是**索引列的值**与**主键列的值**

而对于联合索引，其相当于二级索引的延伸，每条目录项记录都由字段1、字段2、页号、主键值这四个部分组成


## 数据库的回表是什么

回表就是查询的一个步骤。
以表`(id, username, password)`为例，假设有一个用户admin，密码123登录，需要到表中进行查询
`SELECT * FROM user WHERE username = 'admin'`
此时，username是一个唯一且经常作为where条件的字段，所以可以给username建立一个B+ Tree索引。

但MySQL的InnoDB使用聚簇索引，==具体的数据只和主键索引放在一起==，**其他非主键索引只保存了数据的地址（主键id）**，所以要找到整条数据信息要根据得到的id再去找。（产生了两个查找过程，这就是回表）

## 说一说索引的底层实现？

**Hash索引** 

基于哈希表实现，只有精确匹配索引所有列的查询才有效，对于每一行数据，存储引擎都会对所有的索引列计算一个哈希码（hash code），并且Hash索引将所有的哈希码存储在索引中，同时在索引表中保存指向每个数据行的指针。

> 图片来源：https://www.javazhiyin.com/40232.html

![](http://blog-img.coolsen.cn/img/image-20210411215012443.png)

**B-Tree索引**（MySQL使用B+Tree）

B-Tree能加快数据的访问速度，因为存储引擎不再需要进行全表扫描来获取数据，数据分布在各个节点之中。

![](http://blog-img.coolsen.cn/img/image-20210411215023820.png)

**B+Tree索引**

是B-Tree的改进版本，同时也是数据库索引索引所采用的存储结构。数据都在叶子节点上，并且增加了顺序访问指针，每个叶子节点都指向相邻的叶子节点的地址。相比B-Tree来说，进行范围查找时只需要查找两个节点，进行遍历即可。而B-Tree需要获取所有节点，相比之下B+Tree效率更高。

B+tree性质：

* n棵子tree的节点包含n个关键字，不用来保存数据而是保存数据的索引。

* 所有的叶子结点中包含了全部关键字的信息，及指向含这些关键字记录的指针，且叶子结点本身依关键字的大小自小而大顺序链接。

* 所有的非终端结点可以看成是索引部分，结点中仅含其子树中的最大（或最小）关键字。

* B+ 树中，数据对象的插入和删除仅在叶节点上进行。

* B+树有2个头指针，一个是树的根节点，一个是最小关键码的叶节点。

![](http://blog-img.coolsen.cn/img/image-20210411215044332.png)


## 为什么 MySQL 的索引要使用 B+树而不是其它树形结构?比如 B 树？

==频繁的IO才是阻碍数据库性能提高的最大障碍==，而频繁IO可以通过==局部性原理==解决，因此**不一次性读一条记录，而是一个页**

**和B-tree比较**

*  B+树的**磁盘读写代价更低**：
   *  而B树每个节点都保存了数据，导致非叶子结点中的指针数量少（扇出小），要保存大量数据，就必须增加树的高度
   *  B+树的内部节点并没有指向关键字具体信息的指针，因此**其内部节点相对B(B-)树更小**，如果把所有同一内部节点的关键字存放在同一盘块中，那么**盘块所能容纳的关键字数量也越多**，一次性读入内存的需要查找的关键字也就越多，相对`IO读写次数就降低`了。

*  B+树更适合进行区间查询
   *  B+树的数据都存储在叶子结点中，且叶子结点按照索引列的顺序有序存放，扫库时只需要扫一遍叶子结点即可，
   *  但B树的分支结点同样存储着数据，我们要找到具体的数据，需要进行一次**中序遍历**按序来扫

**和Hash比较**

- Hash虽然可以快速定位，但是没有顺序，IO复杂度高；
- 不支持范围查询
- 存在**哈希碰撞**问题，在重复键值较多时效率低
- 因为不是按照索引值顺序存储的，就**不能像B+Tree索引一样利用索引完成[排序]()**；

- Hash索引在**等值查询**时非常快 ；
- 因为Hash索引始终索引的**所有列的全部内容**，所以**不支持部分索引列的匹配查找**；

**二叉树：**
- 树的高度不均匀，不能自平衡，**查找效率跟数据有关（树的高度），并且IO代价高**。

**红黑树**
- 树的高度随着数据量增加而增加，IO代价高。

**跳表**
- 跳表的高度随数据量增加而增加，
- 数据不紧凑，节点的读取为随机IO，不能很好的支持数据预读

> 数据预读：


**不使用平衡二叉树的原因如下**：

最大原因：**深度太大(因为一个节点最多只有2个子节点)**，一次查询需要的**I/O复杂度为O(lgN)**，而b+tree只需要$O(log_mN)$，且其出度m非常大（甚至可以达到成百上千），其深度一般不会超过4 

平衡二叉树逻辑上很近的父子节点，**物理上可能很远，无法充分发挥磁盘顺序读和预读的高效特性**。

==使用B+树的原因==
1. B+树在非叶子结点**只以主键或索引列来组织**，**占据的空间少，每个非叶子结点可以存放跟多的主键，从而能显著减少IO次数**，提高效率
2. B+树的**查询效率更加稳定**，因为数据放在叶子节点
3. B+树能**提高范围查询的效率**，因为叶子节点指向下一个叶子节点


## 讲一讲聚簇索引与非聚簇索引？

在 InnoDB 里，索引B+ Tree的叶子节点存储了整行数据的是主键索引，也被称之为聚簇索引，即将数据存储与索引放到了一块，找到索引也就找到了数据。

而索引B+ Tree的叶子节点存储了主键的值的是非主键索引，也被称之为非聚簇索引、二级索引。

聚簇索引与非聚簇索引的区别：

- 非聚集索引与聚集索引的区别在于非聚集索引的叶子节点不存储表中的数据，而是存储该列对应的主键（行号） 

- 对于InnoDB来说，想要查找数据我们还需要根据主键再去聚集索引中进行查找，这个再根据聚集索引查找数据的过程，我们称为**回表**。第一次索引一般是顺序IO，回表的操作属于随机IO。需要回表的次数越多，即随机IO次数越多，我们就越倾向于使用全表扫描 。

- 通常情况下， 主键索引（聚簇索引）查询只会查一次，而非主键索引（非聚簇索引）需要回表查询多次。当然，如果是覆盖索引的话，查一次即可 

- 注意：MyISAM无论主键索引还是二级索引都是非聚簇索引，而InnoDB的主键索引是聚簇索引，二级索引是非聚簇索引。我们自己建的索引基本都是非聚簇索引。

## 非聚簇索引一定会回表查询吗？

**不一定**
如果**查询语句所要求的字段全部命中索引**，就不必再进行回表查询。
**一个索引包含（覆盖）所有需要查询字段的值，被称之为"覆盖索引"**

举个简单的例子，假设我们在员工表的年龄上建立了索引，那么当进行`select score from student where score > 90`的查询时，在索引的叶子节点上，已经包含了score 信息，不会再次进行回表查询。

## 联合索引是什么？为什么需要注意联合索引中的顺序？

MySQL可以使用多个字段同时建立一个索引，叫做联合索引。
在联合索引中，如果想要命中索引，需要**按照建立索引时的字段顺序挨个使用**，否则无法命中索引。

具体原因为:

MySQL使用索引时需要索引有序，假设现在建立了"name，age，school"的联合索引，那么索引的排序为: 先按照name排序，如果name相同，则按照age排序，如果age的值也相等，则按照school进行排序。

当进行查询时，此时索引仅仅按照name严格有序，因此必须首先使用name字段进行等值查询，之后对于匹配到的列而言，其按照age字段严格有序，此时可以使用age字段用做索引查找，以此类推。
因此在建立联合索引的时候应该注意索引列的顺序，一般情况下，**将查询需求频繁或者字段选择性高的列放在前面**。

此外可以根据特例的查询或者表结构进行单独的调整。

## 讲一讲MySQL的最左前缀原则?

最左前缀原则就是**最左优先**，在创建多列索引时，要根据业务需求，**where子句中使用最频繁的一列放在最左边**。

mysql会**一直向右匹配直到遇到范围查询**(`>、<、between、like`)，就停止匹配。

比如`a = 1 and b = 2 and c > 3 and d = 4 `如果建立`(a,b,c,d)`顺序的索引，**d是用不到索引的**，如果建立`(a,b,d,c)`的索引则都可以用到，a,b,d的顺序可以任意调整。

**=和in可以乱序**，比如a = 1 and b = 2 and c = 3 建立`(a,b,c)`索引可以任意顺序，mysql的查询优化器会帮你优化成索引可以识别的形式。

**最左匹配原则的原理**

MySQL中的索引可以以一定顺序引用多列，这种索引叫作联合索引.最左匹配原则都是针对联合索引来说的

- 我们都知道索引的底层是一颗B+树，那么联合索引当然还是一颗B+树，只不过联合索引的健值数量不是一个，而是多个。构建一颗B+树只能根据一个值来构建，因此数据库依据联合索引最左的字段来构建B+树。 
  例子：假如创建一个`（a,b)`的联合索引，那么它的索引树是这样的可以看到==a的值是有顺序的==，1，1，2，2，3，3，而==b的值是没有顺序的1，2，1，4，1，2==。所以b = 2这种查询条件没有办法利用索引，因为联合索引首先是按a排序的，b是无序的。

同时我们还可以发现**在a值相等的情况下，b值又是按顺序排列**的，但是这种顺序是相对的。
所以最左匹配原则遇上范围查询就会停止，剩下的字段都无法使用索引。例如a = 1 and b = 2 a,b字段都可以使用索引，因为**在a值确定的情况下b是相对有序的**，而`a>1and b=2`，a字段可以匹配上索引，但b值不可以，因为**a的值是一个范围，在这个范围中b是无序的。**

优点：最左前缀原则的利用也可以显著提高查询效率，是常见的MySQL性能优化手段。

## 讲一讲前缀索引

因为可能我们**索引的字段非常长**，这既占内存空间，也不利于维护。

所以我们就想，如果**只把很长字段的前面的公共部分作为一个索引**，就可以提高索引的效率，但同时也降低了索引的选择性，即降低了基数（不重复的索引值）与数据表总记录数的比值，导致B+Tree中出现更多的重复叶子结点

前缀索引的基础，是恰当的字符串前缀长度，即最优长度。



> 但是，我们需要注意，**order by不支持前缀索引**

建立流程： 

1. 先计算完整列的选择性，用基数除以记录总数 :`select count(distinct col_1) / count(1) from table_1 `

2. 再计算不同前缀长度的选择性 :`select count(distinct left(col_1,4)) / count(1) from table_1  `

3. 找到最大的选择性对应的**最优长度**，创建前缀索引:`create index idx_front on table_1 (col_1(4))`

## 了解索引下推吗？

MySQL 5.6引入了索引下推优化。默认开启，使用SET optimizer_switch = ‘index_condition_pushdown=off’;可以将其关闭。 

索引下推：即针对**二级索引**，在索引的基础上进行条件判断，而不是在完整记录上判断。

- 有了索引下推优化，可以在**减少回表次数** 

- 在InnoDB中只针对**二级索引**有效

官方文档中给的例子和解释如下：

在 people_table中有一个二级索引`(zipcode，lastname，address)`，查询是`SELECT * FROM people WHERE zipcode=’95054′ AND lastname LIKE ‘%etrunia%’ AND address LIKE ‘%Main Street%’;` 

* 如果没有使用索引下推技术，则MySQL会通过`zipcode=’95054’`从存储引擎中查询对应的数据，返回到MySQL服务端，然后MySQL服务端基于`lastname LIKE ‘%etrunia%’ and address LIKE ‘%Main Street%’`来判断数据是否符合条件 

* 如果使用了索引下推技术，则MYSQL首先会返回符合`zipcode=’95054’`的索引，然后根据`lastname LIKE ‘%etrunia%’ and address LIKE ‘%Main Street%’`来**判断索引是否符合条件**。如果符合条件，则根据该索引来定位对应的数据，如果不符合，则直接reject掉。

**使用条件：**
 - 只能应用于`range`、`ref`、`eq_ref`、`ref_or_null`访问方法
 - 只能用于`InnoDB`和`MyISAM`存储引擎及其分表
 - 对`InnoDB`来说，只适用于其**二级索引**（聚簇索引下推没意义）
 - **引用了子查询的条件不能下推**
 - **引用了存储函数的条件不能下推**，因为存储引擎无法调用存储函数

### MyISAM

B+Tree叶节点的data域存放的是数据记录的地址。

在索引检索的时候，首先按照B+Tree搜索算法搜索索引，如果指定的Key存在，则取出其 data 域的值，然后以 data 域的值为地址读取相应的数据记录。这被称为**非聚簇索引**。 

**MyISAM中，索引文件和数据文件是分离的**

### InnoDB

- InnoDB 的 B+Tree 索引分为**聚簇索引和二级索引**。一张表一定包含一个聚簇索引构成的 B+ 树以及若干二级索引的构成的 B+ 树。
- 二级索引的存在并不会影响聚簇索引，因为聚簇索引构成的 B+ 树是数据实际存储的形式，而二级索引只用于加速数据的查找，所以一张表上往往有多个二级索引以此来提升数据库的性能。
- 很容易明白为什么不建议使用过长的字段作为主键，因为**所有二级索引都引用主键，过长的主键会令二级索引变得过大**。
- 再例如，用非单调的字段作为主键在InnoDB中不是个好主意，因为InnoDB数据文件本身是一颗B+Tree，非单调的主键会造成在插入新记录时数据文件为了维持B+Tree的特性而频繁的分裂调整，十分低效，而使用自增字段作为主键则是一个很好的选择。


## B+ 树一般有多少层

B+树一般不超过4层
假设：
1. 所有存放用户记录的叶子节点代表的数据页可以存放`100`条用户记录
2. 所有存放目录项记录的内节点代表的数据页可以存放`1000`条目录项记录

那么：
- 如果B+树只有1层，即用户记录层，则可以存储100条用户记录
- 如果B+树有2层，可以存放`1000*100=100000`条用户记录

## 索引适用于哪些情况

- 全值匹配
- 匹配左边的列
- 匹配范围值
- 精确匹配某一列并范围匹配另外一列
- 用于排序
- 用于分组

## 如何创建索引？

创建索引有三种方式。

1. 在执行CREATE TABLE时，通过关键字`KEY`创建索引

```sql
CREATE TABLE user_index2 (
	id INT auto_increment PRIMARY KEY,
	first_name VARCHAR (16),
	last_name VARCHAR (16),
	id_card VARCHAR (18),
	information text,
	KEY name (first_name, last_name),
	FULLTEXT KEY (information),
	UNIQUE KEY (id_card)
);
```

2. 使用`ALTER TABLE xxx ADD INDEX`命令去增加索引。

```sql
ALTER TABLE table_name ADD INDEX index_name (column_list);
```

   - `ALTER TABLE`用来创建**普通索引、唯一索引或聚簇索引**。
    索引名index_name可自己命名，缺省时，MySQL将根据第一个索引列赋一个名称。

3. 使用`CREATE INDEX xxx ON xxx`命令创建。

```sql
CREATE INDEX index_name ON table_name (column_list);
```


## 为什么官方建议使用自增长主键作为索引？

**减少页分裂和移动的频率**

1. 结合B+Tree的特点，自增主键是连续的，**在插入过程中可以减少页分裂**，即使要进行页分裂，也**只会分裂很少一部分**。
2. 能减少数据的移动，**每次插入都是插入到最后**。

插入连续的数据：

> 图片来自：https://www.javazhiyin.com/40232.html

![](http://blog-img.coolsen.cn/img/java10-1562726251.gif)

插入非连续的数据：

![](http://blog-img.coolsen.cn/img/java8-1562726251.gif)


## 建索引时需要遵循什么原则

1. 只为用于搜索、排序、分组的列创建索引
   - `SELECT birthday, country FROM person_name WHERE name = 'Ashburn';`
  其中，对`birthday`与`country`，没必要建立索引，只需对`WHERE`中的`name`建立即可
当然，如果要实现覆盖索引，也可以为这三个数据建立一个二级索引

2. 考虑**列的基数**(对**不重复数据多**的列建立索引)
   - 列的基数即**某一列中不重复数据的个数**，例如[1,1,2,2,3,3]，基数为3
    **行数一定，列的基数越大，列的值越分散**，列基数越小，值的分布越集中，建立索引的效果越差
    
3. 索引列的**类型尽量小**
   - 类型大小指的是类型**对应的数据范围大小**
     类型越小，查询越快，索引占用的储存空间越少（**一个数据页内能放入更多的记录**）

4. 索引**字符串的前缀**
   - 对类型为字符串的列建立索引时，如果字符串很长，会非常占用空间，因此可以考虑只对**字符串的前几个字符**建立前缀索引
  `KEY idx_name_birthday_phonenumber (name(10), birthday, phone_number);//对name的前10个字符建立索引`
    > 但对以下查询，由于二级索引中不包含完整的`name`列信息，无法对前十个字符相同、且后续不同的记录进行排序，即**前缀索引的方式不支持使用索引排序，只能使用文件排序**
    > `SELECT * FROM person_info ORDER BY name LIMIT 10;`

5. 让索引列在比较表达式中单独出现，且尽量不要进行函数运算
   - `WHERE my_col * 2 < 4`不好！
   - `WHERE my_col < 4 / 2`好！
其中，第1句会查询表里所有记录，计算表达式，而第二句会直接使用B+树索引

6. 主键插入顺序
   - 主键插入顺序最好**依次递增**，以避免不必要的**页面分裂**与**记录移位**

7. 尽量扩展索引，而不是新建索引，避免冗余索引
   - 例如对a字段建立了索引，如果想建立a，b字段的二级索引，则应当对a的索引进行扩充 
   - 即避免对某个列的重复索引，应当及时定位并删除表中的重复和冗余索引
8. 非空字段：应该指定列为`NOT NULL`，除非你想存储NULL。
   - 在mysql中，含有空值的列很难进行查询优化，因为它们使得索引、索引的统计信息以及比较运算更加复杂。
   - 应该**用0、一个特殊的值或者一个空串代替空值**；

## 使用索引查询一定能提高查询的性能吗？

通常通过索引查询数据比全表扫描要快。但是我们也必须注意到它的==代价==。

索引需要空间来存储，也需要定期维护， **每当有记录在表中增减或索引列被修改时，索引本身也会被修改**。 
这意味着每条记录的`INSERT，DELETE，UPDATE`将为此**多付出4，5 次的磁盘I/O**。 
因为索引需要额外的存储空间和处理，那些不必要的索引反而会使查询反应时间变慢。

使用索引查询不一定能提高查询性能，索引范围查询(INDEX RANGE SCAN)适用于两种情况:

* 基于一个范围的检索，一般查询返回结果集小于表中记录数的30%。
* 基于非唯一性索引的检索。

## mysql索引对于读写数据有什么影响

MySQL索引在大部分情况下可以提高查询的速度，但如果查询语句没有利用索引或优化器判定使用索引的效率更低，则不会有影响

但是会影响增删改的速度，因为在执行这些写操作的同时，需要进行索引的维护，需要进行额外操作（例如B+树节点的分裂、合并）

## 什么情况下不走索引（索引失效）？

1. 使用`!=` 或者 `<>` 导致索引失效

2. **类型不一致**导致的索引失效

3. **函数**导致的索引失效
`SELECT * FROM `user` WHERE DATE(create_time) = '2020-09-03';`
如果使用函数在索引列，则不走索引。
4. **运算符**导致的索引失效
`SELECT * FROM `user` WHERE age - 1 = 20;`
如果你对索引列进行了（`+，-，*，/，!`）, 那么都将不会走索引。
 5. `OR`引起的索引失效
`SELECT * FROM `user` WHERE `name` = '张三' OR height = '175';`
OR导致索引是在特定情况下的，并不是所有的OR都是使索引失效
**如果OR连接的是同一个字段，那么索引不会失效，反之索引失效**。
6. 模糊搜索导致的索引失效
`SELECT * FROM `user` WHERE `name` LIKE '%冰';`
当`%`放在匹配字段前是不走索引的，**放在后面才会走索引**
7. NOT IN、NOT EXISTS导致索引失效

## 联合索引的组织方式，如果将联合索引（A,B）设计成先以字段 A 为 B+ 树，然后叶子节点索引到字段 B 的 B+ 树上，这样设计如何评价？

联合索引可以覆盖多个索引列，在目录项纪录与用户记录中，按照各个索引列的值递增排序。

想要利用联合索引，需要满足最左前缀法则等规则。

如果采用这种设计，首先从字段A的B+树中获得所需的记录，再到字段B的B+树中查找，**会造成额外的IO开销**，但最终查询到的记录数都是一样的。

## select * from A join B on A.id = B.id;执行过程性能差，原因可能是？执行过程是什么？在A.id还是B.id建立索引?

性能差的原因：`select *`包含的字段较多，解析流程长或未命中索引，只能全表遍历，但其实最紧密的原因还是==被驱动表B没有建立id索引==

> 连接查询流程：
> 1. 选定驱动表（A），对其进行单表查询
> 2. 将得到的**每一条记录**到被驱动表（B）中进行查询
> 3. 将最终的查询结果放入集合，作为最终的结果
---
如果被驱动表没有建立id索引，MySQL就会使用==Block Nested-Loop算法==：
1. MySQL首先会**将驱动表A中的若干条记录**读入线程内存中的一块固定大小的区域`join_buffer`中
2. 然后扫描被驱动表B，**==每一条被驱动表的记录==一次性和`join buffer`中的==多条驱动表记录==进行匹配**

`join_buffer` 中的数据都是**无序存储**的，由于**没有用上被驱动表的索引**，所以**对表 B 中的每一行**，取出来后需要==与 join_buffer 中的所有数据==分别做判断
假设 A 表 `100` 行， B 表 `1000` 行，那么总共需要做的判断次数是：$100 * 1000 = 10 万次$

---

如果能够用上被驱动表B的索引，则使用的是==Index Nested-Loop算法==:
1. 遍历表A的每一行，**对其中的每一行都去表B中根据`id`查询**
2. 而由于建立了表B的`id`列的索引，所以每次只需要扫描==一行==即可（==假设`id`是`unique`的，即无重复的==）

这样，B表就一共扫描了`100`次，所以INL一共只扫描了`200`行记录。

> 具体步骤其实就是一个**嵌套查询**

因此，可以**在被驱动表B.id字段上建立索引**。

==驱动表的选择==：
- left join：左表为驱动表
- right join：右表为驱动表
- join：选择数据量比较小的作为驱动表，大表作为被驱动表
==永远是小表驱动大表==
## SELECT * FROM t2 WHERE t2.m2 = 2 AND t2.n2 < 'd';应该怎么建索引

- 在m2列上建索引
  - 因为对m2是**等值查找**，可能使用到`ref`访问方法，然后回表判断`t2.n2 < d`是否成立
  > 如果`m2`是t2表的主键或唯一二级索引，那么使用`t2.m2 = 常数值`这样的条件从t2表中查找记录的过程就是常数级别的。
  > （因为在单表中使用主键值或者唯一二级索引列的值进行等值查找的方式为`const`，而在连接查询中对被驱动表使用主键值或者唯一二级索引列的值进行等值查找的查询执行方式称之为：`eq_ref`）

- 在n2列上建索引
  - 涉及的条件是`t2.n2 < 'd'`，可能用到`range`访问方法，如果使用`range`，则需要**索引下推**，判断`m2`列上的条件是否成立

- 如果`m2`与`n2`两列都有索引
  - 挑一个代价更低的索引去执行对t2表的查询



> 另外，有时候连接查询的查询列表和过滤条件中可能只涉及被驱动表的部分列，而这些列都是**某个索引的一部分**，这种情况下即使不能使用`eq_ref`、`ref`、`ref_or_null`或者`range`这些访问方法执行对被驱动表的查询，也可以使用索引扫描，也就是`index`的访问方法来查询被驱动表。
> 所以我们建议在真实工作中最好不要使用*作为查询列表，最好把真实用到的列作为查询列表。


## select count(*)、count(id)、count(1)的区别

`select count(*)`与`select count(1)`没有区别，都是将表中所有行进行计数，即计算总行数
- 原因：因为`count()`中的值均不为null，只要不是null，就会选取所有行（`count(null)`查询结果为0）
  - 当count()括号内的值为null时，mysql内部自动返回0，不进行进一步查询

而对于`select count(id)`，由于`count()`中的`id`是属性列，所以会根据`id`的值得出不同的结果：
- 当a=null，count(a)的值=0
- 当a!= null时且不是表的列名，count(a)为该表的行数
- 当a为列名，`count(a)`为**该表中a列的值不等于null的行的总数**

## select id,name from user where age > 10 order by id desc limit 0,10怎么建立索引？SQL语句的运行顺序，这个语句中会扫描多少数据

建立`idx_id_age_name`索引

（是不是`idx_age_id_name`索引更合适呢？）

## order by limit 分页出现重复数据问题

比如limit(0,10)表示列出第一页的10条数据，limit(10,10)表示列出第二页。但是，当limit遇到order by的时候，可能会出现翻到第二页的时候，竟然又出现了第一页的记录。

**SQL语句如下：**
`select *  from user where gender = 1 ORDER BY age desc limit 5;`
`select * from user where gender = 1 ORDER BY age desc limit 5,5;`

**解决方案：**
1. **索引排序字段**
   - 如果在排序字段上添加了索引，自然会按照索引的有序性进行读取并分页，从而规避重复数据的问题 


**原因分析：**
- 在MySQL5.6以上的版本，优化器在遇到`order by ... limit`语句时，会使用**优先队列**进行排序，以达到**在不能使用索引有序性的情况下，如果要排序，且使用了`limit n`，可以在排序过程中保留n条记录**的效果。
进而减少`sort buffer`内存的使用
> 因为堆排序是**不稳定的排序方法**，所以排序结果会变化

> 从SQL执行顺序的角度来看，进行order by排序时，所有的记录都是以堆排序的方式排列得到的，若排序字段不具备索引有序性，则在第二页数据中，MySQL看见哪条记录就拿哪条记录

> 此外，分页本来就是衍生出来的高级需求，MySQL和Oracle都没有明确定义分页的概念，且不同场景对数据分页的需求不明确，自然支持的不好

- 而在MySQL5.x的版本，有**常规排序与优化排序**两种==filesort方式==

**常规排序**
1. 从WHERE条件筛选后的临时表中获取全部记录
2. 将要排序的字段值与主键组成键值对（例如：`(id, age)`），存放到`sort buffer`中
3. 如果`sort buffer`能够容纳所有满足条件的键值对，则采用==快速排序算法==进行排序；
   - 否则，在`sort buffer`每次满后，进行==快排==，并持久化到临时文件中； 
4. 若排序中产生了临时文件，则**利用==归并排序算法==，保证临时文件中的记录是有序的**
5. 循环执行上述过程，直到所有满足条件的记录全部参加排序
6. 扫描排序后的键值对，利用id，回表得到`SELECT`需要的列
7. 将结果集返回给用户

**优化排序**

常规排序除了排序本身之外，还需要**额外进行两次IO**：回表、排序结果存储到临时文件。
优化排序相对于常规排序，减少了回表的IO损耗。
主要区别在于：放入`sort buffer`的键值不再由主键与排序字段组成，而是**查询的字段与排序字段**。
但这种方法是有代价的：同样的`sort buffer`，能够存储的`(查询字段，排序字段)`的数量要少于`(主键，排序字段)`，可能导致原本不需要使用临时文件的排序反而使用了临时文件，造成额外IO。

> 使用优化排序的条件：当排序元组小于`max_length_for_sort_data`

无论select中的内容是什么、进不进行回表，**==只有当order by 的字段全部出现在 where条件中，并且where条件中的字段全部在一个索引中==，才会利用索引的有序性进行排序，而不会在MYSQL中进行内存排序操作**。


## Mysql 为什么不好用模糊查询（为什么用 like 查询效率低？）

1. 模糊查询**很有可能不使用索引**
2. **即使使用索引，也会比普通的查询慢很多**

## 模糊查询如何优化

解决方案：
https://cloud.tencent.com/developer/article/1159624

1. 全文索引
2. 对模糊查询使用前缀索引
3. 用搜索引擎，例如ES的倒排索引
## 为什么不用select *

1. 增加查询器分析成本，数据库会首先查询字典，明确`*`代表什么，且复杂的语句解析成本高
2. 增减字段时，容易与`ResultMap`不一致，导致代码要修改
3. 无用字段增大了网络开销，特别是`text`这些比较大的类型
4. ==降低了使用覆盖索引的可能性==

## 假如现在有三个普通索引a，b，c，我sql查询where a = xx and b = xx and c == xx会怎么样？(索引合并是什么)

==索引合并即在一次查询中使用多个二级索引==

- 交集合并（**搜索条件使用`AND`连接**）
   某个查询使用多个二级索引，**将多个索引的查询结果取交集**
   例如：`SELECT * FROM single_table WHERE key1 = 'a' AND key3 = 'b';`
   可以同时使用`idx_key1`与`idx_key3`，**利用两个结果得到主键的交集，进行下一步的回表**

   > 为什么不直接使用`idx_key1`，然后进行回表，再过滤另一个搜索条件？
   > 虽然读取多个二级索引比读取一个二级索引更消耗性能，但是**读取二级索引的操作是`顺序I/O`，而回表操作是`随机I/O`**。
   > 只读取一个二级索引时，需要回表的记录数会更多，就会造成更多的==回表开销==


    **使用场景（不一定采用索引合并，取决于优化器对统计数据的分析结果）：**

    
    1. 对于二级索引，索引列均为**等值匹配**；
        对于复合索引，每个索引列都必须为**等值匹配**，且**不能出现只匹配部分列的情况**。
    ```sql
    对于idx_key1 和 idx_part1_part2
    可以索引合并：
    SELECT * FROM table1 WHERE key1 = 'a' AND part1 = 'a' AND part2 = 'b';

    不能索引合并：
    不是等值匹配， 
      SELECT * FROM single_table WHERE key1 > 'a' AND key_part1 = 'a' AND key_part2 = 'b';
    复合索引出现部分匹配， 
      SELECT * FROM single_table WHERE key1 = 'a' AND key_part1 = 'a';
    ```

    2. 主键列是范围匹配，也可以利用索引合并
      `SELECT * FROM single_table WHERE id > 100 AND key1 = 'a';` 

    > 原因：从二级索引中取出的是`索引列+主键`，取出的索引列是**按主键排序**的，而排序的主键很容易就可以取到交集；
    > 而且排序的主键也可以很方便的用范围过滤，所以索引合并中，主键可以使用范围查询


- 并集合并（**搜索条件使用`OR`连接**）
   使用场景：
   1. 对于二级索引列，索引列均为等值匹配
       对于复合索引，其中的每个列都必须等值匹配，不能出现只匹配部分列的情况。（**和交集合并相同**）
        `SELECT * FROM single_table WHERE key1 = 'a' OR ( key_part1 = 'a' AND key_part2 = 'b' AND key_part3 = 'c');`
        其中，如果第一个条件改为`key1 > 'a'`或`OR`之后的条件不覆盖联合索引，则不能合并

   2. 主键列是范围匹配（同交集合并）
   3. 使用交集索引合并的搜索条件
        `SELECT * FROM single_table WHERE key_part1 = 'a' AND key_part2 = 'b' AND key_part3 = 'c' OR (key1 = 'a' AND key3 = 'b');`
        优化器可能先按照`key1 = 'a' AND key3 = 'b'`对`idx_key1`与`idx_key3`进行交集合并，再对`key_part1 = 'a' AND key_part2 = 'b' AND key_part3 = 'c'`使用联合索引得到另一个主键集合，然后使用并集索引合并得到最终的主键集合，进行回表


- Sort-Union合并
    由于并集合并的条件太过苛刻，必须保证**各个二级索引列为等值匹配**，使用频率较低
    例如`SELECT * FROM single_table WHERE key1 < 'a' OR key3 > 'z'`就无法使用，是因为从`key1 < 'a'`与`key3 > 'z'`中获得的==主键列表不是排序的==。**使用算法合并两个非排序数组又很麻烦**

    因此，我们可以将从`key1 < 'a'`与`key3 > 'z'`中获得的**主键列表分别进行排序**后，再使用`Union合并`，所以这种方法称为`Sort-Union合并`


## 怎么查看MySQL语句有没有用到索引？

通过explain，如以下例子：

` EXPLAIN SELECT * FROM employees.titles WHERE emp_no='10001' AND title='Senior Engineer' AND from_date='1986-06-26'; `

| id   | select_type | table  | partitions | type  | possible_keys | key     | key_len | ref               | filtered | rows | Extra |
| ---- | ----------- | ------ | ---------- | ----- | ------------- | ------- | ------- | ----------------- | -------- | ---- | ----- |
| 1    | SIMPLE      | titles | null       | const | PRIMARY       | PRIMARY | 59      | const,const,const | 10       | 1    |       |

 

|列名	| 描述|
|--|--|
|id	|在一个大的查询语句中每个SELECT关键字都对应一个唯一的id|
|select_type	|SELECT关键字对应的那个查询的类型|
|table	|表名|
|partitions	|匹配的分区信息|
|type	|针对单表的访问方法|
|possible_keys|	可能用到的索引|
|key	|实际上使用的索引|
|key_len	|实际使用到的索引长度|
|ref	|当使用索引列等值查询时，与索引列进行等值匹配的对象信息|
|rows	|预估的需要读取的记录条，也很重要，MySQL优化器根据统计信息，估算需要扫描的记录行数，直观地显示了SQL的效率高低|
|filtered	|某个表经过搜索条件过滤后剩余记录条数的百分比|
|Extra|	一些额外的信息|

下表中，执行效率依次降低

| type | 含义 |
|--|--|
| NULL | 不访问任何表、索引，直接返回结果 |
| system | 表只有一行记录（等于系统表），是const类型的特例 |
| const | 通过索引一次找到，用于比较**主键索引**或者**唯一索引**。很快。（如将主键置于where列表中，mysql就能将该查询转换为一个常量） |
| eq_ref | 类似ref，区别在于使用的是**唯一索引**，使用**主键的关联查询**，**关联查询出的记录只有一条（常见于主键或唯一索引扫描）**|
| ref | 根据**非唯一性索引**进行扫描，返回匹配某个单独值得所有行，本质上也是一种索引访问 |
| range | **只检索给定返回的行**，使用一个索引来选择行（where之后出现between、<、>、in等操作） |
| index | 只遍历索引树，通常比all快 |
| all | 遍历全表，找到匹配的行 |




# 事务


## 事务是什么？有什么特性

事务即逻辑上的一组操作，这一组操作的整体即事务，具有以下特性

- 原子性
  - 是指事务包含所有操作要么全部成功，要么全部失败回滚。
- 一致性
  - 指事务执行前后，数据库都是符合一致性状态的
  拿转账来说，假设用户 A 和用户 B 两者的钱加起来一共是 5000，那么不管 A 和 B 之间如何转账，转几次账，事务结束后两个用户的钱相加起来应该还得是 5000，这就是事务的一致性。
- 隔离性
  - 是当多个用户并发访问数据库时，比如操作同一张表时，数据表为每个用户开启的事务，不能被其他事务所干扰，**多个并发事务之间要相互隔离**。
- 持久性
  - 持久性是指一个事务一旦被提交，那么对数据库中的数据的改变就是永久的，即便是在数据库系统遇到故障的性况下也不会丢失提交事务的操作

## MySQL事务是怎么实现的

以InnoDB为例：
- 使用`redo log`保证事务的持久性
- 使用`undo log`保证事务的原子性
- 通过锁机制、MVCC保证事务的隔离性
- 在满足上述三个性质之后，一致性就得到了保障

## 并发事务会存在哪些问题

- 脏写：写入的数据失效
  - 最严重的问题，事务A修改了==未提交==事务B修改过的数据，B回滚导致A写入的数据丢失
- 脏读：读取的数据失效
  - 事务A读取了==未提交==事务B 增加或修改的数据，B回滚导致A读取的数据失效
  > 即事务A由于==未提交事务B==的回滚，读取到了==不存在的数据==
- 不可重复读：多次相同读取，得到不同的记录
  - 事务A读取数据，且`事务C`对该数据==修改并提交==后，事务A查询到了不同的值
  > 即事务A受到了`其他事务`的干扰，违反了==事务的隔离性==
- 幻读：多次相同读取，得到了更多的记录
  - 事务A查询到一些记录，`事务D`又插入了一些同类记录，`事务A`==按照同样的条件再查询时，读出了新插入的记录==
  > 但如果`事务D`删除了数据，`事务A`读取的数据变少了，则不属于幻读
  > 幻读：事务按照相同的条件多次读取，后读取时出现了之前没有的记录

## 事务的隔离级别

- 未提交读`READ UNCOMMITED`：
  - 针对脏写
- 已提交读`READ COMMITED`：
  - 针对脏读
- 可重复读`REPEATABLE READ`：
  - 针对不可重复读（MySQL在这一级别也禁止幻读）
- 串行化`SERIALIZABLE`：
  - 针对幻读

在 MySQL 数据库中，支持上面四种隔离级别，默认的为 Repeatable read (可重复读)；
在 Oracle 数据库中，只支持 Serializable (串行化)级别和 Read committed (读已提交)这两种级别，其中默认的为 Read committed 级别。

## MVCC 的原理（undo log、ReadView）

MVCC：多版本并发控制，它是服务于MySQL事务的一种并发控制方式，实现对数据库的并发访问。

> 读已提交与可重复读是基于MVCC实现的，这两个隔离级别分别解决了脏读与不可重复读的问题。

MVCC的实现有几个关键要素：
1. **事务的版本号**，即`事务id`
   - 在事务==第一次真正修改数据前==，都会从**数据库的一个自增长的全局变量**中获得一个唯一的`事务id`，而**通过事务id可以判断事务的执行先后顺序**；
2. 记录的隐藏列`trx_id`与`roll_pointer`
   > 如果没有对表指定主键，也没有`unique`的列，则数据库会自动添加一个隐藏列`row_id`作为主键。
   - 其中，`trx_id`表示**操作该记录的事务**的事务id；`roll_pointer`**指向用于回滚的undo日志**；
   - `row_id`则是单调递增的行ID
3. undo log，回滚日志，记录了**数据被修改前的信息**。
   - 在表记录修改之前，会**先把数据拷贝到undo log里**。如果事务回滚，即可以通过undo log来还原数据。而记录的多个undo log通过**指针**连接起来，就形成了记录的==版本链==
4. Read View，读视图，
   - 它是**事务执行SQL语句时产生**的，主要用于进行**事务的可见性判断**，即**判断当前事务可以看到哪个版本的数据**。
    > 在Read View中，有几个关键字段，保证了可见性的判断
    > 1. `m_idx`：系统中的**活跃的读写事务的id列表**
    > 2. `min_limit_id`：**生成Read View时**，当前系统中**活跃的读写事务**中的最小的事务id，即m_idx中的最小值
    > 1. `max_limit_id`：生成Read View时，系统应当分配给下一个事务的id值，即m_idx的最大值+1
    > 1. `creator_trx_id`：**创建当前Read View的事务ID**

Read View的判定规则：
1. 记录的`trx_id` < ReadView中的**最小事务id**，说明**创建该记录的事务已提交**，可以读取
2. 记录的`trx_id` >= ReadView中的**最大事务id**，说明创建该记录的事务**还未启动**，无法读取
3. 如果在两者之间，则判断**记录的事务id在不在`m_idx`中**
   1.  如果包含，即**ReadView生成时，生成该记录的事务还未提交**，**若等于`creator_trx_id`，则说明是自己创建的，可以看到**，不等于就不能看到
   2.  不包含，这说明事务在ReadView生成前已提交，**可以看到**

基于MVCC的查询过程：
1. 获取记录中的**事务id**，即最后一次改动该事务的id
2. 生成ReadView
3. 将记录中的事务id与ReadView中的事务id进行比较
4. 如果不符合ReadView的可见性规则，则需要**到undo log中依次比较**
5. 最后返回符合规则的数据

MVCC是如何分别实现**已提交读**与**可重复读**的：
- 对已提交读，在同一个事务中，MySQL会在==每次查询前产生一个新的ReadView==，就产生了可见性不一致的问题，即不可重复读问题
- 对可重复读，一个事务中，==只会生成一次ReadView==，之后的查询都会复用第一次生成的ReadView，保证了可见性相同


## MySQL 解决幻读了吗，是如何解决的

MySQL InnoDB默认的是**可重复读**隔离级别，它通过MVCC与间隙锁一定程度上解决了幻读的问题

> 幻读：一个事物的同样的查询条件，第二次查询得到的数据比第一次多。

**快照读**：简单的SELECT操作，由MVCC负责保障并发安全，通过ReadView防止幻读的发生
**当前读**：加锁的SELECT操作，通过间隙锁防止幻读的发生

> 幻读的场景：
> 1.a事务先select，b事务insert确实会加一个gap锁，但是如果b事务commit，这个gap锁就会释放（释放后a事务可以随意操作），
> 
> 2.a事务再select出来的结果在MVCC下还和第一次select一样，
> 
> 3.接着a事务**不加条件地update**，这个update会**作用在所有行上（包括b事务新加的）**，
> 
> 4.a事务再次select就会出现b事务中的新行，并且这个新行已经被update修改了.


## undolog如何保证原子性

事务的原子性是通过`undo log`及**回滚**操作实现的
undo log，回滚日志，记录了数据被修改前的信息。
在表记录的每次修改之前，InnoDB会先把数据记录到`undo log`里，并将undo log连接到记录上。当进行如果事务回滚，即可以通过undo log来还原数据。而记录的多个undo log通过指针连接起来，就形成了记录的==版本链==

## redolog的原理

https://blog.csdn.net/qq_41999455/article/details/106161484

redo log是InnoDB特有的日志类型之一，其记录了某个数据页上被修改的位置与内容，主要用于崩溃后的恢复，保证了事务的==持久性==。


==redo log最大的意义就是减少对数据页刷盘的要求==

### redo log分为两部分：

1. redo log buffer：日志缓存，存在于内存中
2. redo log file：日志文件，用于持久化到磁盘上，不易丢失。
   - redo log file属于日志文件组`redo log group`
   - 每个InnoDB引擎至少有一个重做日志文件组
   - 每个组至少2个redo log file
   - 而redo log file是**循环连接**的，可以重复利用 

### redo log的记录格式：==Physiological Logging（物理逻辑日志）==


采用了逻辑和物理的综合体，就是**先以Page为单位记录日志，每个日志里记录的是Page中具体修改的位置与修改前后的内容**

> 逻辑日志：例如bin log，记录了修改的语句（或其他形式）
> 物理日志：记录了修改的是哪个表空间的哪个数据页，在哪一行数据的哪个字段进行修改，改变后的内容是什么

> - 逻辑日志：因为一个表可能有多个索引，每个索引都是一颗 B+ 树，插入一条记录，同时更新多个索引，自然可能修改多个Page。 
>   如果Redo Log采用逻辑日志的记法，**一条记录牵涉的多个Page写到一半系统宕机了**，**要恢复的时候很难知道到底哪个Page写成功了，哪个失败了**
> - 物理日志：即使1条逻辑日志只对应一个Page，也可能要修改这个Page的多个地方。因为一个Page里面的记录是用链表串联的，所以如果在中间插入一条记录，不仅要插入数据，还要修改记录前后的链表指针。
>   对应到Page就是多个位置要修改，**会产生多条物理日志**。 
>   
> 所以**纯粹的逻辑日志宕机后不好恢复；物理日志又太大，一条逻辑 日志就可能对应多条物理日志**。

因此，redolog使用的是逻辑物理日志

### **redo log 写入的原子性：**

在将磁盘上写入redo log或数据时，如果写到一半机器宕机，就会造成原子性问题（写入成功的字节是0？还是(0,512)间的一个数？）==partial page write问题==

InnoDB为了不让这个问题影响数据的一致性，采用了==双写缓存==
1. 首先将`redo log buffer`**连续地写入**到内存中的`doublewrite buffer`中
2. 再将`double write buffer`**离散地写入**写入各表空间文件

> 实际上，用户空间下的缓冲区的数据是不能直接写入磁盘的，中间必须经过**操作系统内核空间缓冲区**

> redo log只会在事务提交时进行刷盘，而且只有在刷盘完成后才会将结果反馈给client

## WAL（Write Ahead Log）

https://cloud.tencent.com/developer/article/1850833

InnoDB存储引擎由**缓冲池**与一些**后台线程**组成，其中内存池又称缓冲池，也即笔记中所说的内存。

拥有了缓冲池之后，**读取页**的操作为：
1. 首先将从磁盘读到的页存放在缓冲池中
2. 下一次再读相同的页时，首先判断该页是否在缓冲池中。
   - 若在缓冲池中，称该页在缓冲池中被命中，直接读取该页
   - 否则，读取磁盘上的页
> 这里的内存池和MySQl的查询缓存不同

**修改页**的操作为：**首先修改在缓冲池中的页；然后再以一定的频率刷新到磁盘上。**

> 其中，如果缓冲池中的页已经被修改了，但是还没有刷新到磁盘上，那么我们就称缓冲池中的这页是 ==脏页==，即**缓冲池中的版本比磁盘中的版本新**。

因此，**缓冲池的大小直接影响数据库的整体性能**

而后台线程最大的作用就是完成 **`将从磁盘读到的页存放在缓冲池中`** 以及 **`将缓冲池中的数据以一定的频率刷新到磁盘上`** 这两个操作

但是，如果每当一个页发生变化，我们就将其同步到磁盘，这个开销是极大的，且如果修改的热点数据集中在某几个页中，则数据库性能会变得极差；
**另外，一旦在从缓冲池将页的新版本刷新到磁盘时发生了宕机，那么这个数据就不能恢复了**。

因此，为了避免数据丢失，现有的数据库系统采用了`WAL(Write Ahead Log)`策略，即==当事务提交时，先写redo log，再修改页（先修改缓冲池，再异步地刷新到磁盘）==；
当由于发生宕机而导致数据丢失时，通过 redo log 来完成数据的恢复

> （假设每次提交事务都将redo log刷盘）且事务在完成redo log刷盘之后才返回成功，如果服务器宕机了，事务会回滚

## CheckPoint机制

然而，仅仅有`redo log`是不够的，我们仍然面临下面3个问题：
- 缓冲池不是无限大的，也就是说不能没完没了的存储我们的数据等待一起刷新到磁盘
- **redo log 是循环使用而不是无限大的**（也许可以，但是成本太高，同时不便于运维），那么当所有的 redo log file 都写满了怎么办？
- 当数据库运行了几个月甚至几年时，这时如果发生宕机，重新应用 redo log 的时间会非常久，此时恢复的代价将会非常大。

因此，`CheckPoint`技术被用来解决上述问题：
- **缓冲池不够用时**，将脏页刷新到磁盘
  - 此时，InnoDB会采用`LRU`算法释放最少使用的页，如果是脏页，则强制执行CheckPoint，将脏页刷新到磁盘中
- **redo log 不可用时**，将脏页刷新到磁盘
  - 事实上，redo log中的数据并不是时时刻刻都是有用的，那些已经不再需要的部分就称为 **可以被重用的部分**，数据库宕机时**不需要恢复这部分数据**，因此这部分数据可以覆盖重用
- **缩短数据库的恢复时间**
  - 数据库宕机时，数据库**不需要重做所有的日志**，因为 **Checkpoint 之前的页都已经刷新回磁盘**。只需要对Checkpoint之后的redo log进行恢复
  
> 所谓 CheckPoint 技术简单来说其实就是在 `redo log file` 中找到一个位置，**将这个位置前的页都刷新到磁盘中去**，这个位置就称为 CheckPoint（检查点）
> CheckPoint的值用`LSN（Log Sequence Number）`表示，LSN并非物理位置，它是一个8字节（64位）的整数且单调递增，代表着自数据库启动以来，至当前时间点写入至Redo Log中数据的总量（字节数）。

## 一条 SQL update语句是如何执行的（redo log 两阶段提交、redo log 和 bin log 一致性问题）

https://blog.csdn.net/xcy1193068639/article/details/84922580

update语句与普通的SQL查询语句的执行流程，但其中还涉及了bin log与redo log这两个模块

`update T set c=c+1 where ID = 2;`
执行器与InnoDB引擎执行update语句的流程为：
1. 执行器先找到引擎取 `ID=2` 这一行，ID 是主键，引擎直接用B+树搜索找到下一行。
    如果 `ID=2` 这一行所在的数据页本来就在内存中，就直接返回给执行器；
    否则，需要先从磁盘读入内存，然后再返回。
2. 执行器拿到引擎给的行数据，把这个值加上1，比如原来是 N，现在就是 N+1，得到新的一行数据，再**调用引擎接口写入这行新数据**。
3. 引擎将这行新数据**更新到内存中**，并为这条数据加上相应的`undo log`，同时**将这个更新操作记录到 redo log buffer，并进行redo log的刷盘**，==此时 redo log 处于prepare 状态==，然后告知执行器执行完成了，随时可以提交事务。 
4. 执行器**生成这个操作的 binlog，并把 binlog 写入磁盘**。
5. （到这里才提交事务）执行器调用引擎的**提交事务接口**，引擎把刚刚写入的 `redo log` 改成==提交（commit）状态==，更新完成
   
> - 如果在写入binlog时宕机，由于redo log处于prepare状态，且找不到对应的binlog，会进行回滚
> - 如果在设置commit状态时，服务器宕机了，不会进行事务回滚
> 因为尽管redo log处于prepare状态，但仍然能够通过事务id找到对应的binlog，MySQL认为事务是完整的

6. 至此，事务的提交完成了，但还需要根据实际条件，将脏页刷新到磁盘中

注意：其中将redo log的写入拆分为了两个步骤：==prepare==与==commit==，这就是redo log的两阶段提交，两阶段提交的目的就是==保证redo log与bin log的一致性==

如果不采用两阶段提交：
- 先写`redo log`，后写`bin log`：
  - 假设在 **redo log 写完，binlog 还没有写完的时候**，MySQL进程异常重启。由于redo log 已写完，系统即使崩溃，仍然能够把数据恢复回来，所以恢复后这一行 c 的值是1。
    但是由于 binlog 没写完就 crash 了，这时候 **binlog 里面就没有记录这个语句**。
    因此，**之后备份日志的时候，存起来的 binlog 里面就没有这条语句**，如果用这个binlog来恢复临时库，就会**与原库的值不符**。
    
- 先写`bin log`，再写`redo log`：
  - 假设`bin log`写完后系统崩溃，**崩溃恢复后这个事务无效**，**但这个事务的逻辑操作已经被记录到了`bin log`中**，也会导致在备份日志时数据不符

> 实际上，InnoDB只有在事务提交时才会记录bin log，此时记录还在内存中，那么binlog的刷盘时机是什么呢？
> mysql 通过参数控制binlog的刷盘时机，详见下面两条

> 注意：由于二阶段提交机制的存在，时序上，redo log先prepare，再写binlog，然后提交事务，最后把redo log commit。如果将`innodb_flush_log_at_trx_commit`设置为1，则redo log在prepare阶段就要持久化一次（因为是先提交再commit）。
> 每秒一次后台轮训刷盘，再加上崩溃恢复的逻辑，InnoDB就认为redo log在commit时不需要fsync了，只要write到文件系统的page cache即可。

通常我们说的MySQL的“双1配置”，即`sync_binlog`与`innodb_flush_log_at_trx_commit`均设置为1，即==一个事务完整提交前，需要等待两次IO刷盘，一次是redo log（prepare阶段），一次是binlog==

### MySQL的组提交机制

在双1配置下，如果MySQL的TPS是每秒两万，每秒就会写四万次磁盘，但如果磁盘的能力也就是两万左右，要如何实现每秒两万的MySQL TPS？

此时就需要使用==组提交机制==

首先，组提交是建立在LSN（Log Sequence Number，日志逻辑序列号）的基础上的。LSN是单调递增的，用来对应redo log的一个个写入点，每次写入长度为len的redo log，LSN就会加上len。

> LSN也会写入到InnoDB的数据页中，来确保数据页不会被多次执行重复的redo log。

假设三个并行事务（trx1、trx2、trx3）在prepare阶段，它们都写完了redo log buffer，现在正处在将redo log buffer持久化到硬盘中的阶段，假设对应的LSN分别为`50`、`120`、`160`，它们作为组进行后续流程。

假设trx1是第一个到达的，会被选作组的leader。
当trx1开始刷盘时，会发现LSN=160（组里最大的），因此**当trx1返回时，所有LSN小于等于160的redo log就都已经被持久化到磁盘了**，所以trx2和trx3就可以直接返回了

> 因此一次组提交里，组员越多，节约磁盘IO的效果就越好。
> 如果只有单线程压测，那就只能一个事务一个事务的持久化了
> 而在并发更新场景下，第一个事务写完redo log buffer后，接下来的fsync越晚，则带的组员就越多，省下的IO就越多

为了让事务组的组员更多，**即一次`fsync`能够带上更多的组员**，MySQL会将两阶段提交的流程修改为：
1. 调用write，将redo log从 redo log buffer写到磁盘上的redo log file（此时断电则文件消失），但暂时不使用fsync持久化；
2. 调用write，将binlog从 binlog buffer写到磁盘上的binlog文件
3. 调用fsync，对redo log file持久化
4. 调用fsync，对binlog file持久化

> **2和3调换了顺序**

因此，WAL机制主要得益于两方面：
- `redo log`和`bin log`都是**顺序写**，磁盘的顺序写比随机写速度快
- 组提交机制，**大幅度减少磁盘的IO消耗**
- redo log的大小比buffer pool的数据页小得多，因此写入的数据量也会小很多，速度更快

## redo log 与 bin log的区别

1. 适用对象不同：
   - bin log 是 MySQL 的 Server 层实现的，所有引擎都可以使用
   - 而 redo log 是 InnoDB 引擎特有的
2. 写入内容不同
    - bin log 是**逻辑日志**，记录的是这个语句的**原始逻辑**，比如 “给 id = 1 这一行的 age 字段加 1”
    - redo log 是**物理逻辑日志**，记录的是 “在某个数据页上的哪个位置做了什么修改”
3. 写入机制不同
    - bin log 是可以**追加写入**的。
      - “追加写” 是指 bin log 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志
    - redo log 是**循环写**的，空间固定会被用完   

由于bin log采用的是`追加写`的方式，**没有用于判断哪些数据已经刷入硬盘的标志**

> 例如
> `记录1：给 id = 1 这一行的 age 字段加 1`
> `记录2：给 id = 1 这一行的 age 字段加 1`
> 假设在记录 1 刷盘后，记录 2 未刷盘时，数据库崩溃。
> 重启后，**只通过 bin log 数据库是无法判断这两条记录哪条已经写入磁盘，哪条没有写入磁盘**，不管是两条都恢复至内存，还是都不恢复，对 id = 1 这行数据来说，都是不对的。

但redo log不一样，**只要刷入磁盘的数据，都会从 redo log 中被抹掉或用checkpoint标识**，数据库重启后，直接把 redo log 中的数据都恢复至内存就可以

## 有了bin log，为什么还需要redo log

MySQL 架构可以分成俩层，一层是 Server 层，它主要做的是 MySQL 功能层面的事情；另一层就是存储引擎，负责存储与提取相关的具体事宜。

redo log 是 InnoDB 引擎特有的日志，而 Server 层也有自己的日志，包括错误日志（error log）、二进制日志（binlog）、慢查询日志（slow query log）、查询日志（log）

其中，Bin Log记录了对MySQL数据库执行更改的所有操作，即记录了语句的原始逻辑。可以看出，**==Bin log==只用于归档、备份、主从同步，无法为MySQL提供崩溃恢复的能力**

## 事务还没提交的时候，redo log 有没有可能被持久化到磁盘呢？(MySQL 是如何保证数据不丢失的呢？)

https://blog.csdn.net/w372426096/article/details/88103918

有可能，为了控制redo log的写入策略，InnoDB提供了`innodb_flush_log_at_trx_commit`来控制刷盘的时机（详见下一条）
此外，InnoDB有一个后台线程，每隔1s，就会把redo log buffer中的日志，调用write写入到os cache中，然后调用fsync持久化到磁盘（还有两种，详见下一条）

> 注意：事务执行中间过程中的redo log也是写在redo log buffer中的，这些redo log也会被后台线程一起持久化到磁盘。因此，没有提交的事务的redo log也会被持久化到磁盘。




## undo log、redo log与bin log

### 写入过程：
- bin log：
    事务执行过程中，当一条SQL执行完成，将日志写至`bin log cache`
- redo log：当一条 SQL 更新完 Buffer Pool 中的缓存页后，就会记录一条 redo log 日志，并随后写入redo log buffer，并在事务提交时，通过WAL与两阶段提交，将redo log持久化到磁盘中（调用write和fsync）

### 刷盘时机：
- redo log：
  1. MySQl正常关闭时
  2. MySQL后台进程**每隔一段时间**刷盘`redo log buffer`一次
  3. redo log buffer的**写入日志量超过redo log buffer的一半**时
  4. **事务提交**时，根据配置参数决定是否刷盘`innodb_flush_log_at_trx_commit`
    - 0：事务提交时不刷盘，而是每秒执行一次写入os buffer，并fsync
    - 1：事务提交时，将该事务对应的redo log所在的redo log block从内存写入磁盘，并调用fsync，确保数据落入磁盘==这种情况能保证不丢失数据==
    - 2：每次提交仅写入`os buffer`，然后每秒调用`fsync()`写入redo log file
- bin log：
  - 事务**提交**时将`binlog cache` 用 write 写入文件系统的 `page cache`，再用fsync 写入 binlog文件中
   > 一个事务的binlog是不能拆分的，**不论这个事务多大，也要确保一次性写入**。
   > 因此，一旦单个线程的`binlog cache`超过了给定的大小，则需暂存至磁盘
   
   > **每个线程都有一个独立的binlog cache**，但共用一份binlog文件

   > 其中，write和fsync的时机由参数`sync_binlog`控制：
   > - 0：每次提交事务只write，不fsync
   > - 1：每次提交事务都fsync
   > - N：每次提交事务都write，但积累N个事务后才fsync

实际上，用户空间下的缓冲区的数据是不能直接写入磁盘的，中间必须经过**操作系统内核空间缓冲区(os cache)**，在调用`fsync`方法，才会将数据从`os cache`刷新到磁盘上

> 疑问：既然redo log的刷盘参数建议为1，也就是每次事务提交时，还是得将redo log写入磁盘，发生IO，那何必在每次更新时只将数据更新到Buffer Pool，而不是直接将数据更新到磁盘呢？
> 1. 写 redo log 时，我们将 redo log 日志追加到文件末尾，虽然也是一次磁盘 IO，但是这是==顺序写==操作（不需要移动磁头）；
> 而对于直接将数据更新到磁盘，涉及到的操作是**将 buffer pool 中缓存页写入到磁盘上的数据页上**，由于涉及到寻找数据页在磁盘的哪个地方，这个操作发生的是==随机写==操作（需要移动磁头）
> 2. 通常一次更新操作，我们往往==只会涉及到修改几个字节==的数据，而如果因为仅仅修改几个字节的数据，就将整个数据页写入到磁盘（无论是磁盘还是 buffer pool，他们管理数据的单位都是以==页==为单位，每个页默认是 ==16KB==），这个代价未免也太了，而一条 redo log 日志的大小可能就只有==几个字节==，因此每次磁盘 IO 写入的数据量更小，那么耗时也会更短
> 3. WAL可以通过组提交机制**一次将多条redo log buffer写入到磁盘上，节省了IO的次数**

### Buffer数量
- undo log：
- redo log：多个线程共用一个redo log buffer
- bin log：每个线程有单独的bin log buffer

## 数据库回滚与崩溃恢复的算法是什么（Aries算法）

https://zhuanlan.zhihu.com/p/455122515

https://www.cnblogs.com/defectfixer/p/15835714.html#:~:text=ARIES%E7%AE%97%E6%B3%95%20%E5%A6%82%E6%9E%9C%E9%9C%80%E8%A6%81%E6%B7%B1%E5%85%A5%E4%BA%86%E8%A7%A3%E6%95%B0%E6%8D%AE%E5%BA%93%E6%9C%AC%E5%9C%B0%E4%BA%8B%E5%8A%A1%E5%8E%9F%E7%90%86%EF%BC%8C%E4%B8%8D%E5%BE%97%E4%B8%8D%E6%8F%90%E5%88%B0ARIES%E7%AE%97%E6%B3%95%EF%BC%8C%E8%AF%A5%E7%AE%97%E6%B3%95%E5%85%A8%E7%A7%B0%E4%B8%BAAlgorithms%20for%20Recovery,and%20Isolation%20Exploiting%20Semantics%EF%BC%88%E5%9F%BA%E4%BA%8E%E8%AF%AD%E4%B9%89%E7%9A%84%E6%81%A2%E5%A4%8D%E4%B8%8E%E9%9A%94%E7%A6%BB%E7%AE%97%E6%B3%95%EF%BC%89%EF%BC%8C%E4%BC%97%E5%A4%9A%E4%B8%BB%E6%B5%81%E7%9A%84%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E9%83%BD%E5%8F%97%E5%88%B0%E8%AF%A5%E7%AE%97%E6%B3%95%E7%9A%84%E5%BD%B1%E5%93%8D%E3%80%82



## 介绍一下InnoDB的锁机制，锁分为几类

事务利用`MVCC`进行的读取操作称之为一致性读，或者一致性无锁读，有的地方也称之为快照读。
> 所有普通的SELECT语句在READ COMMITTED、REPEATABLE READ隔离级别下都算是一致性读
> 一致性读并不会对表中的任何记录做加锁操作，其他事务可以自由的对表中的记录做改动。

而利用锁实现的并发读写控制，称为锁定读


当一个事务`T1`**修改了一条记录**后，就会产生一个锁结构与该记录相关联；
在事务`T1`提交前，如果事务`T2`想要修改该记录，就会侦测到记录上的已有锁结构的`is_waiting`为false，表示已经加锁了，事务`T2`则获取锁失败，**将锁结构与记录相连**，但`is_waiting`为true。==在事务T1提交后，释放其对应的锁结构==，唤醒对应的事务线程

在锁的结构中，比较重要的两个字段为：
- `trx`信息：代表这个锁结构是哪个事务生成的
- `is_waiting`：代表当前事务是否正在等待


锁的分类：
- 使用方式：乐观锁、悲观锁
- 锁的级别：读锁（共享锁）、写锁（排它锁）、意向锁、间隙锁
- 锁的粒度：表锁、行锁
- 操作/加锁方式：DDL锁、DML锁、自动锁、显式锁


## 悲观锁与乐观锁

MySQL的乐观锁：体现为进行数据库操作时，认为操作不会冲突，不通过锁进行特殊处理，而是在更新之后，通过MVCC，判断是否有冲突

悲观锁：认为不加锁就会出错，通过锁来保证并发读写的安全

## 共享锁与独占锁（行锁）

InnoDB实现了以下两种行锁

- 共享锁（S锁，读锁）：多个事务对于同一数据可以共享一把锁，但只能读不能改
- 独占锁（X锁，写锁）：亦称排他锁，不能与其他锁并存，获取排他锁的事务可以对数据进行读取和修改

> 对于增、删、改语句，InnoDB会自动给涉及数据加排他锁
对普通SELECT语句，InnoDB不会加任何锁

在InnoDB中，**如果不通过索引条件检索数据，InnoDB会对表中的所有记录进行加锁，即类似表锁**

- INSERT
  - 一般来说，插入记录不加锁，而是通过==隐式锁==来保护这条新插入的记录在本事务提交前不被别的事务访问
- DELETE
  - 对一条记录的DELETE实际上是：
     1. 在B+树中定位记录；
     2. 获取该记录的X锁； 
     3. 执行delete mask操作（配合undo log）
- UPDATE
  - update的加锁情况分三种：
    - **修改了记录的键值**：相当于在原纪录上做了`DELETE`操作后，再来一次`INSERT`操作
    - 未修改该记录的键值 且 **被更新的列占用的存储空间未发生变化**：
       1. 在B+树中定位记录；
       2. 获取该记录的X锁； 
       3. 在原纪录的位置进行修改
    > ==定位待修改记录的过程可以看作一个获取X锁的当前读（锁定读）==
    - 未修改该记录的键值 且 **被更新的列占用的存储空间发生变化**：
       1. 在B+树中定位记录；
       2. 获取X锁；
       3. **将该记录彻底删除（将该记录移入垃圾链表）**；
       4. **插入一条新纪录**
    > ==新插入的记录由INSERT操作提供的隐式锁保护==

## 表锁与行锁

上述**X锁与S锁都是针对记录**的，称为**行锁**
而一个事务也可以对**表**进行加锁，称为**表锁**，同样分为S锁与X锁

- 表级锁（以查询为主）：偏向MyISAM，开销小，加锁快，不会出现死锁，冲突率高，并发度低
- 行级锁（适用于大量按索引条件并发更新少量不同数据）：偏向InnoDB，反之
- 页面锁：介于二者之间

> 间隙锁：当我们用范围条件，而不是相等条件进行检索，并请求共享或排他锁时，InnoDB会给符合条件的已有数据加锁
> 对**键值在条件范围但并不存在的记录**，叫做“间隙”，InnoDB也会对这个“间隙”加锁，即间隙锁(Next-Key Lock)

## 意向锁（表级别）

**上表锁之前**，**需要确保当前表中没有记录被上行锁**，但扫描整张表的效率太低了。
因此InnoDB提出了`意向锁`，用于为之后加表级别的S锁与X锁时，**快速判断表内记录是否有加行锁**。

- 对行级锁
  意向锁是一种**不与行级锁冲突的表级锁**
- 对表级锁
  - 对于表级的共享锁与独占锁，**只有表级共享锁与IS锁不冲突**，其他都互斥
- 对意向锁
  - 插入意向锁之间也不会互相阻塞
---
- 意向共享锁（IS锁）：表示事务有意向对表中的某些记录加共享锁（S锁）
  - 当事务准备在**某条记录**上加S锁时，**需要首先检查表上是否有`IX锁`，之后在**表级别**加一个`IS锁`**
- 意向独占锁（IX锁）：表示事务有意向对表中的某些记录加独占锁（X锁）
  - 当事务准备在**某条记录**上加X锁时，需要首先检查表上是否有`IX锁`与`IS`锁，然后在**表级别**加一个`IX锁`
---
**例如：**
1. 事务 A 获取了某一行的排他锁，并未提交
  `SELECT * FROM users WHERE id = 6 FOR UPDATE;`
  此时 users 表存在两把锁：users 表上的意向排他锁与 id 为 6 的数据行上的排他锁。
2. 事务 B 想要获取 users 表的共享锁：
  `LOCK TABLES users READ;`
  此时，事务 B 检测事务 A 持有 **users 表的意向排他锁**，就可以得知**事务A必然持有该表中某些数据行的排他锁**，
  那么事务 B 对 users 表的加锁请求就会被排斥（阻塞），而**无需去检测表中的每一行数据是否存在排他锁**。
3. 如果事务C想要获得user表的**某一行记录的排他锁**
   `SELECT * FROM users WHERE id = 5 FOR UPDATE;`
   则事务C需要申请**user表的意向排他锁**，而其检测到事务A持有user表的意向排他锁，由于意向锁之间不互斥，事务C可以获得意向排他锁，最终也能获得数据5的排他锁

**意向锁的作用：实现了行锁和表锁共存，且满足事务隔离性的要求**

## 间隙锁，Gap Locks

> MySQL在`REPEATABLE READ`隔离级别下，可以使用MVCC与加锁解决幻读问题
> 但如果使用加锁解决时，当事务**第一次执行读取操作，并不存在幻影记录，无法给不存在的记录加上Record Locks**
> 此时，就需要Gap Locks

![](../images/MySQL/MySQL_Lock_Gap_Locks.jpg)

> 其中，number为主键

如图，可以给number为8的记录加了gap锁，则**不允许别的事务在该记录之前的间隙插入新纪录**，即不允许number值为$(3,8)$这个区间的新纪录立即插入

==`Gap Locks`的作用仅仅是为了防止插入幻影记录==

如果我们要给**最后一条记录后的区域**加Gap锁，那么就需要给`Supremum记录`，即页面中的最大记录加`Gap Locks`

## Next-Key Locks

相当于 `Record Locks` 与 `Gap Locks` 的合体
作用：**既锁住某条记录，又阻止其他记录插入该记录之前的间隙**

InnoDB在`可重复读`的隔离等级下，会通过Next-Key Locks来解决幻读的问题

## 隐式锁

当事务需要加锁的时，如果这个锁==不可能发生冲突==，InnoDB会跳过加锁环节，这种机制称为隐式锁。
隐式锁是InnoDB实现的一种**延迟加锁机制**，其特点是**只有在可能发生冲突时才加锁，从而减少了锁的数量，提高了系统整体性能**。
> 另外，隐式锁是针对被修改的B+ Tree记录，因此**都是记录类型的锁，不可能是间隙锁或Next-Key类型**。

**隐式锁主要用在==INSERT==场景中。**
在Insert语句执行过程中，必须检查两种情况：
1. 如果记录之间加有**间隙锁**，为了避免幻读，此时是**不能插入记录**的
2. 如果Insert的记录**和已有记录存在唯一键冲突**，此时也不能插入记录

除此之外，insert语句的锁都是隐式锁，实际上就是不加锁。
**只有在特殊情况下，才会将隐式锁转换为显示锁**，这个转换动作是**由存在行数据冲突的线程去做的**。

例如事务1插入记录且未提交，此时事务2尝试对该记录加锁。
那么事务2必须先判断记录上保存的事务id是否活跃，**如果活跃则帮助事务1建立一个锁对象**，而事务2自身进入等待事务1的状态。

**如何判断隐式锁是否存在**
- 聚簇索引
  - 聚簇索引的记录中有一个`trx_id`隐藏列，记录最后改动该记录的`事务id`，在当前事务插入一条聚簇索引记录后，该记录中的`trx_id`即其事务id
  - **其他事务**尝试对该记录加`S锁或X锁`时，会查看该记录的`trx_id`是否为当前活跃事务
  - 如果`trx_id`等于当前活跃事务的事务id，则**帮助当前事务创建一个`X锁`，自己进入等待状态**
- 二级索引
  - 对二级索引记录来说，本身没有`trx_id`列，但二级索引页面的`Page Header`有一个`PAGE_MAX_TRX_ID`属性，表示对该页面做改动的`最大的事务id`
  - 如果`PAGE_MAX_TRX_ID`小于`当前最小的活跃事务ID`，说明对该页面的修改都已提交
  - 否则就需要在页面中定位到对应的二级索引记录，然后回表找到它对应的聚簇索引记录，重复上一个场景的做法


## 当前读与快照读及使用场景

快照读：读取的是记录数据的**可见版本（有旧的版本）**。不加锁，普通的select语句都是快照读,如：
`select * from core_user where id > 2;`

当前读：读取的是记录数据的**最新版本**，显式加锁的都是当前读：
- 加读锁：`select * from account where id>2 lock in share mode;`
- 加写锁：`SELECT ... FOR UPDATE;`


## InnoDB三大特性是什么

- Buffer Pool 缓冲池
  - 当我们需要访问某个页中的数据时，就会把该页从磁盘加载到Buffer Pool中，如果该页已经在Buffer Pool中的话直接使用就可以了。通过哈希表来定位缓存页
  - 通过类似LRU的算法来实现旧数据页的淘汰
  - 凡是修改过的缓存页（产生脏页）对应的控制块都会作为一个节点加入到一个链表中，因为这个链表节点对应的缓存页都是需要被刷新到磁盘上的，所以也叫`flush链表`
- 自适应Hash索引
  - InnoDB会根据统计发现**某个索引页被频繁查询，并诊断后发现如果为这一页的数据创建Hash索引会带来更大的性能提升，则会自动为这一页的数据创建Hash索引，就会为其建立hash索引，存在于InnoDB架构中的缓存中（内存）**。
    自适应Hash索引是在B+树索引基础之上建立的索引，其查询的时间复杂度约为`O(1)`，而B+树索引为`O(logN)`
- 双写缓冲区
  - InnoDB在将数据页写入磁盘前，会先将页面刷新入双写缓冲区，如果在写入磁盘的过程中宕机，则可以从双写缓冲区找到良好的数据副本
    > MySQL可以根据redo log进行宕机恢复，而mysql在恢复的过程中是检查page的checksum，checksum就是pgae的最后事务号，发生==partial page write== 问题时，page已经损坏，找不到该page中的事务号，就无法恢复，所以需要双写缓冲区

## InnoDB是怎么保证崩溃恢复能力的？

https://zhuanlan.zhihu.com/p/86604093

# MySQL优化

## mysql 的性能极限和瓶颈是什么样的，具体的表现是什么样的分析

MySQL的瓶颈包含IO瓶颈与CPU瓶颈两类：
- IO瓶颈
  - 磁盘读IO瓶颈，热点数据太多，数据库缓存放不下，每次查询时会产生大量的IO，降低查询速度 -> 分库和垂直分表
  - 网络IO瓶颈，请求的数据太多，网络带宽不够 -> 分库
- CPU瓶颈
  - SQL问题，如SQL中包含join，group by，order by，非索
  - 、引字段条件查询等，增加CPU运算的操作 -> SQL优化，建立合适的索引，在业务Service层进行业务计算
  - 单表数据量太大，查询时扫描的行太多，SQL效率低，CPU率先出现瓶颈 -> 水平分表


## SQL优化的具体操作流程

1. 定位执行效率较低的SQL语句
   可以通过以下两种方式查询执行效率较低的SQL语句：
   - 慢查询日志：记录所有执行时间超过`long_query_time`秒的SQL语句
    > 慢查询日志在查询结束后才记录，不能定位问题

   - `show processlist`：查看当前MySQL正在进行的线程，包括线程状态、是否锁表等 
2. 通过explain分析SQL语句的执行计划
3. 通过show profile分析SQL执行时长
   - 默认关闭，需要手动开启 
4. 在找到了耗时比较长的SQL语句之后，通过`show profile cpu, block io for query query_id`分析整个SQL生命周期中，在CPU、块IO等方面的开销
> 如果发现SQL语句大部分时间在对结果排序、结果的复制等方面，就说明可以优化

## 介绍下 SQL 优化的方法（目前包含 join、order by 语句的优化）

https://www.cnblogs.com/zhouyusheng/p/8038224.html
https://blog.csdn.net/afsvsv/article/details/84998119

1. 表设计的优化
    **数据类型应该越小越好**，越小的数据类型通常会更快，占用更少的磁盘、内存，处理时需要的CPU周期也更少。（整型比字符操作消耗低，因此使用整型来存储ip地址，使用DATETIME来存储时间）

    > 几个错误的优化思路：
    > 1. 通常来说把可为NULL的列改为NOT NULL不会对性能提升有多少帮助，**只是如果计划在列上创建索引，就应该将该列设置为NOT NULL**
    > 1. INT使用32位（4个字节）存储空间，那么它的表示范围已经确定，所以INT(1)和INT(20)对于存储和计算是相同的。
    > 1. UNSIGNED表示不允许负值，大致可以使正数的上限提高一倍。比如TINYINT存储范围是-128 ~ 127，而UNSIGNED TINYINT存储的范围却是0 - 255。
    > 1. 通常来讲，没有太大的必要使用DECIMAL数据类型。即使是在需要存储财务数据时，仍然可以使用`BIGINT`。
      比如需要精确到万分之一，那么可以将数据乘以一百万然后使用BIGINT存储。
      这样**可以避免浮点数计算不准确和DECIMAL精确计算代价高**的问题。
    > 1. **TIMESTAMP使用4个字节存储空间，DATETIME使用8个字节存储空间**。
      因而，TIMESTAMP只能表示1970 - 2038年，比DATETIME表示的范围小得多，而且**TIMESTAMP的值因时区不同而不同**。
    > 1. 大多数情况下没有使用枚举类型的必要，其中一个缺点是枚举的字符串列表是固定的，添加和删除字符串（枚举选项）必须使用ALTER TABLE（如果只只是在列表末尾追加元素，不需要重建表）。
    > 1. 大表ALTER TABLE非常耗时，MySQL执行大部分修改表结果操作的方法是**用新的结构创建一个张空表，从旧表中查出所有的数据插入新表，然后再删除旧表**。
      尤其当内存不足而表又很大，而且还有很大索引的情况下，耗时更久。当然有一些奇技淫巧可以解决这个问题，有兴趣可自行查阅。
2. 建立索引时，遵循使用索引的规则，避免一些常见错误
3. 特定类型语句的优化
   - 大批量插入数据
      1. 按照**主键顺序**插入(避免B+树的分裂)
      2. 关闭**唯一性校验**（执行`SET UNIQUE_CHECKS=0`），可提高导入效率
      3. **手动提交事务**（执行`SET AUTOCOMMIT=0`）

   - 优化insert语句
      1. 如果需要同时对一张表插入多行数据，应当尽量使用多个值表的insert语句，而不是多条单个的insert语句
      2. 在事务中进行数据插入（手动提交事务）
      3. 按主键有序地插入数据
   
   - 优化order by语句
        共包含两种排序方式：
        1. filesort：对返回数据进行排序，即不直接通过索引返回排序结果的排序
        2. using index：通过有序索引顺序扫描，直接返回有序数据，不需要额外排序
        > 使用索引的要求：
        > 返回的字段被索引列全覆盖
        > 数据全升序或全降序
        > 排序字段的顺序与索引一致

   - 优化group by语句

        group by也会进行排序操作，与order by类似，也会利用索引进行优化
        如果查询包含group by，但用户想要避免排序结果的消耗，可以使用`order by null`禁止排序

   - 优化嵌套查询

        原则：用多表联结替换嵌套查询（子查询）

   - or优化

        必须保证or关联的各个字段均用到索引，且不能使用到复合索引

        如果实在不能添加索引，可以通过union联结两个查询结果，替代原先的or

   - limit优化

        分页时实际上首先进行了排序，查询的起始页数很大时，可以通过以下思路进行优化：
        - 在索引上完成排序分页，最后与原表进行联结，得到查询结果
        > `select * from tb_item t, (select id from tb_item order by id limit 200000,10) a where t.id = a.id;`

        - 对于针对主键自增的表，可以把limit查询转换成某个位置的查询

    - 使用SQL提示

        SQL提示是优化数据库的一个重要手段，即在SQL语句中加入一些人为的提示，从而实现优化

        1. USE INDEX：`select * from tb_seller use index(idx_seller_name) where name = 'xxxx'`
        > 提供一个索引列表，==建议==MySQL使用其中的索引，而不考虑其他可用索引

        1. IGNORE INDEX
        > 让MySQL忽略指定的索引

        1. FORCE INDEX



## SQL语句很慢怎么办

分类讨论：
1. 大多数情况是正常的，偶会出现很慢的情况
2. 在数据量不变的情况下，这条SQL语句一直以来都运行的很慢

- 偶尔很慢
  - 数据库刷新脏页
    当我们向数据库`增改`数据，内存中更新后新的字段不会立即同步到磁盘中，而是将更新的记录写入`redo log`，并在合适的时机同步到磁盘。而与磁盘中数据页不一致的内存数据页称为脏页，脏页的刷新有以下四个时机：
    1. **redo log满了**：
       - 如果数据库一直繁忙，更新又很频繁，则redo log很快就会写满，此时只能暂停其他操作，先将数据同步到磁盘，导致SQL执行慢
    2. 发生缺页，且内存剩余空间不足以容纳所需数据
       - 如果一次查询较多的数据，恰好**所要查询的数据页不在内存中**，需要把查询的内容从硬盘放入内存。
        但是**内存中已有的其他数据页是脏页**，所以先要把内存中的脏页写入硬盘后，然后把硬盘中所需页取到内存中，去执行这个sql所需要的数据
    3. MySQL认为系统空闲时
    4. MySQL正常关闭
  - 拿不到锁 ：可以通过`show processlist`查看当前状态，判断是否真的是拿不到锁

- 一直很慢
  - 分析SQL语句是否有问题
    1. 查询条件没有使用索引
    2. 字段有索引，但没有用上（不符合索引的使用规范）
    3. 优化器判断不使用或选错了索引（系统根据统计数据计算区分度，判断是否使用索引）
    4. 数据表是否数据量过大而没有做分库分表
    5. 并发量是否过大

## MySQL 主从复制原理

主从复制主要实现的是：**主数据库有写操作，则从数据库自动同步**。
从数据库通过I/O线程去**请求主数据库的binlog日志文件**，并写到中继日志中，SQL线程会读取中继日志，并解析成具体操作同步数据到从数据库。

原理：
1. Master主库在事务提交时，将数据变更作为时间Events记录在`二进制日志文件Binlog`中
2. 主库推送`二进制日志文件Binlog`中的日志事件到从库的`中继日志Relay Log`
3. 从库slave重做中继日志中的数据，将改变反映在自己的数据上

## MySQL如何实现读写分离

读写分离指的是：让Master处理写操作，Slave处理从操作。非常适用于读操作量比较大的场景，可减轻master的压力。

**主数据库处理事务性查询，从数据库处理select查询**

> 主从复制被用来**将事务性查询导致的变更同步到集群中的从数据库**

在DBA领域一般配置**主-主-从**或者**主-从-从**两种部署模型

**所有写操作都先在主库上进行，然后异步更新到从库上**。
所以从主库同步到从库机器有一定的延迟，当系统很繁忙时，延迟问题会更加严重，从库机器数量的增加也会使这个问题更严重。

> 此外，**主库是集群的瓶颈**，当写操作过多时会严重影响主库的稳定性，如果主库挂掉，则整个集群都将不能正常工作。

主从复制+读写分离的缺点：
- 无法改进写入的效率
- 主从复制造成延时
- 加表锁的频率上升
- 表变大，缓存率下降

因此，在主从分离无法解决问题的场景下，应当考虑更进一步的分区、分库分表

https://blog.csdn.net/whp15369657805/article/details/52931802

## 单表数据量过大会有什么问题

如果单表数据量过大，会导致查询速度极慢，容易卡死

原因：
- MySQL的连接数存在瓶颈
- 数据量大，有存储压力
- 读写速度会变慢，特别是没有命中索引，进行全表扫描的时候，速度非常慢

方案：
- 优化SQL表设计
- 优化SQL语句
- 分区
- 分库
- 分表

## 分区了解吗

### 分区是什么

众所周知，MySQL存储结构中，**最基本的存储单位是页（16KB），连续的64个页组成区（extent，1MB），每256个区组成一个组**，而段是完整的区与碎片页组成的逻辑上的概念。

然而，分区中的区与存储结构中的区不同。

分区的意思是：**将同一表中不同行的记录分配到不同的物理文件中，几个分区就有几个.idb文件**
也就是**将一个表或索引分解成多个更小，更可管理的部分**。
每个区都是独立的，可以独立处理，也可以作为一个更大对象的一部分进行处理。

> MySQL数据库的分区是**局部分区索引**，**一个分区中既存了数据，又放了索引**。
> 也就是说，每个区的聚集索引和非聚集索引都放在各自区的（不同的物理文件）。

### 为什么分区

不分区的痛点：
- mysql数据库中的数据是以文件的形式存在磁盘上的，一张表主要对应着三个文件，一个是frm存放表结构的，一个是myd存放表数据的，一个是myi存表索引的。
 如果一张表太大，那么表数据文件与索引文件就将变得很大，查找速度缓慢。
- 如果利用MySQL的分区功能，在物理上将一张表对应的三个文件，划分为几个小块，这样在查找时只需要知道数据在哪个小块即可，也可以将文件放到不同的磁盘。 

分区与分表的区别：分区与分表可以联合使用而不冲突，其中，分区容易操作，而分表需要手动创建子表，服务端读取也稍显麻烦

**分区的目的：**
- 改善大型表以及具有各种访问模式的表的可伸缩性，可管理性和提高数据库效率
- 当表非常大，或表中有大量废弃的历史记录，导致查询频率高的**热数据**处于表的末尾，则可以分区、分表

**分区的优点：**
- 数据量不受单个硬盘的容量限制。与单个磁盘或文件系统分区相比，可以存储更多的数据
- 管理方便。可以通过删除相关分区的形式删除过期数据，也可以为新数据专门增加一个分区，分区的优化、检查、修复简单
- 部分查询语句的速度快。可以将同类数据放在一个分区中，从而使服务器不需要检查无关分区中数据，从而加快查询。
- 涉及到例如`SUM()\COUNT()`这样的聚合查询时，可以**并行处理**
- 通过跨多个磁盘来分散数据查询，来获得更大的查询吞吐量

### 分区的应用场景

1. 当数据量很大(过T)时，肯定不能把数据载入到内存中，这样查询一个或一定范围的item是很耗时，可以通过分区减小每次需要查询的数据量
2. 数据量过大，单个硬盘无法承受
3. 如果数据存在**明显的热点数据**，且除了这部分数据，其他数据很少访问，则可以将热点数据单独放在一个分区，使得查询热点数据时所需的数据减少，有机会缓存在内存中
4. 数据存在**明显的时间序列性**，可以按时间分区（一天一个分区之类的）

### 分区方法有哪些

当前，MySQL支持的分区类型如下：
- RANGE分区
  - 基于一个给定的连续区间，将行数据放入分区；
- LIST分区
  - 基于RANGE分区，区别在于LIST分区是基于列值匹配一个离散值集合中的某个值来进行划分的；
- HASH分区
  - **基于用户定义的表达式的返回值**来选择分区，利用即将插入表中的数据的列值进行计算。
    - 该函数可以包含MySQL中有效的、产生非负整数值的任何表达式
    - HASH分区主要用来确保==数据在预先确定数目的分区中平均分布==。
      > 在RANGE和LIST分区中，必须明确指定一个给定的列值或列值集合应该保存在哪个分区中；
      而HASH分区基于将要被用于哈希计算的列值，主需要提供**一个列值或表达式，以及指定的分区数**，就可以实现自动分布
- KEY分区
  - 类似HASH分区，但**KEY分区只支持计算一列或多列**，且**只能使用MySQL服务器提供的哈希函数**。


### 分区的语句

RANGE分区：
```sql
create table foo_range (
    id int not null auto_increment,
    created DATETIME,
    primary key (id, created)
) 
engine = innodb 
partition by range (TO_DAYS(created))(//RANGE分区
    PARTITION foo_1 VALUES LESS THAN (TO_DAYS('2016-10-18')),
    PARTITION foo_2 VALUES LESS THAN (TO_DAYS('2017-01-01'))
);

//新增一个分区
ALTER TABLE foo_range ADD PARTITION(
  //RANGE分区
    PARTITION foo_3 VALUES LESS THAN (TO_DAYS('2017-10-18'))
);
```
LIST分区：
```sql
create table foo_list(
    empno varchar(20) not null ,
    empname varchar(20),
    deptno int,
    birthdate date not null,
    salary int
) partition by list(deptno)(
    partition p1 values in (10),
    partition p2 values in (20),
    partition p3 values in (30)
);
以部门号为分区依据，每个部门（deptno）一个分区（10、20、30）
```
HASH分区：
```sql
create table foo_hash (
    empno varchar(20) not null ,
    empname varchar(20),
    deptno int,
    birthdate date not null,
    salary int
) 
partition by hash(year(birthdate)) 
partitions 4;
//指定用于分区的列、哈希函数以及分区数
```
KEY分区：
```sql
create table foo_key(
    empno varchar(20) not null ,
    empname varchar(20),
    deptno int,
    birthdate date not null,
    salary int
)
partition by key(birthdate)
partitions 4;
```

## 数据库分库分表了解过吗，具体讲一下什么场景，怎么做

**分库分表的原因:**
- 用户请求量太大：单服务器的TPS、内存、IO都是有限的
  - 解决方案：**分散请求到多个服务器上**；
- 单库数据量太大：单个数据库处理能力有限，或单库所在服务器磁盘空间不足；单库存在IO瓶颈
  - 解决方案：**切分成更小的库**
- 单表数据量太大：CRUD都成问题；索引膨胀；查询超时
  - 解决方案：**切分成多个数据集更小的表**

**分库分表的方法：**
- 如果是因为**表多**而造成数据量大，则进行垂直切分，根据业务切分成不同的库
- 如果是因为**单张表数据太多**而造成数据量大，则进行水平切分，即把表的数据按某种规则切分成多张表，甚至多个库上的多张表

> 分库分表的顺序：先尝试垂直切分，再尝试水平切分
垂直拆分更倾向于业务拆分的过程


### 垂直切分

垂直切分即针对数据库的**属性**进行切分
- 将一个库拆分为多个库，不同的库包含不同的表（垂直分库）
- 将一个表拆分为多个表，不同的表包含不同的属性（垂直分表）

优点：
1. 拆分后业务清晰，拆分规则明确
2. 系统之间进行整合或扩展比较容易
3. 可按照成本、应用等级、应用类型对数据库进行划分到不同的机器，便于管理
4. 便于实现动静分离、冷热分离的数据库表的设计模式
5. 数据维护简单

缺点：
1. 部分业务表无法关联，只能通过接口的方式解决，提高了系统的复杂度
2. 受每种业务不同的限制，存在单库性能瓶颈，对数据扩展和性能提升不友好
3. 事务处理复杂

#### 垂直分库

垂直分库：针对一个系统中的不同业务进行拆分，将一个库拆分为多个服务器上的多个库
- 例如将User、Product、Order分在不同的库中，并存放在多个服务器上

对并发量的影响：
- 提升了系统的并发量，并且可以抽象出单独的业务模块
- 可以将公用的配置表、字典表等抽象到单独的库，甚至可以微服务化

#### 垂直分表

垂直分表：即基于列字段进行**大表拆小表**，一般适用于字段较多，且**存在不活跃字段的表**。
  > 字段多了，会导致单行记录的存储开销大，以至于数据库缓存的数据行减少，查询时会去读磁盘数据产生大量的随机读IO，产生IO瓶颈。
- 可以理解为划分为**列表页**与**详情页**，将不常用的、数据类型较大、长度较长（例如text类型的字段）的拆分到“扩展表”
- 垂直分表的拆分原则是**将热点数据放在一起作为主表**，**非热点数据**放在一起作为**扩展表**。这样更多的热点数据就能被缓存下来，进而减少了随机读IO。
  > 拆分之后，如果要获得完整的数据，可以关联两个表进行查询（但**千万不要用join**，因为join不仅会增加CPU负担并且会讲两个表耦合在一起（必须在一个数据库实例上）），应当在`Service`层做文章
   

**垂直分表时，切分出来的表通常需要有一列交集，一般是主键，用于关联数据**

对并发量的影响：



### 水平拆分

水平切分即针对数据库的**数据**进行切分
- 将一个表拆分为多个表，不同的表的结构相同，但是包含不同的数据（水平分表）
- 将一个库拆分为多个库，对表也进行如上的拆分（水平分库分表）

优点：
1. 单库单表的数据能够维持在较低的量级，有利于性能的提高
2. 切分的表的结构相同，不需要对应用层进行复杂的改造，只需要增加路由规则即可
3. 提高了系统的稳定性与负载能力

缺点：
1. **切分后数据是分散的，很难利用数据库的关系查询，跨库查询性能较差**
2. **拆分规则难以抽象化**
3. **分片的数据一致性问题难以解决**
4. **数据扩容的难度与维护难度大**

#### 水平分表

水平分表：针对数据量巨大的单张表，以**字段**为依据，按照某种规则（hash、range等），**将数据切分到多张表里去，但这些表还是在同一个库中，仍有IO瓶颈**
- 每个表的结构都一样，但数据不同且没有交集
- 所有表的并集是全量数据


对并发量的影响：

**没有改变系统的绝对并发量**：水平分表没有解决IO瓶颈，但由于表的数据量少了，单次SQL的执行效率高，减轻了CPU的负担


#### 水平分库分表

水平分库分表：以字段为依据，按照一定的策略（hash、range等），**将一个库的数据拆分为多个服务器上的多个库，且多个库中的数据相同**
- > 可以有效的缓解单机和单库的性能瓶颈和压力，突破IO、连接数、硬件资源等的瓶颈。
- **每个库的结构都一样，但数据不同且没有交集**
- 所有库的并集是全量数据

水平分库分表的路由规则：
水平切分后同一张表会出现在多个数据库或表中，每个库和表的内容不同，想要知道哪条数据在哪个库里或表里，就需要路由算法进行计算
- **范围路由**
  - 从0-10000一个表，10001到20000一个表...
  - 复杂点主要体现在分段大小的选取上，分段太小会导致切分后子表数量过多增加维护复杂度，分段太大可能会导致单表依然存在性能问题（100w-2000w合适）
  - 优点：
    - 新数据表/库的扩充平滑，原有数据不需要改动
    - 单表大小可控
    - 使用sharding key进行范围查找时，可以很快的进行数据定位
  - 切分范例：
    - 地理区域：按照地理区域进行划分，例如华东华北这样
    - 时间 ：按照时间切分，例如将前6个月的数据切出去到另一个库（随着时间流逝，这些数据被查询概率小，因此也叫冷热数据分离）
- **HASH算法**
  - 选取某个列或几个列的值进行hash运算，然后根据hash的结果分散到不同的数据库表中。
  - 难度体现在初始表数量的选取上：表数量太多维护比较麻烦，表数量太小又可能导致单表性能存在问题。
  - 使用hash路由后，增加数据表的数量是非常麻烦的，所有数据都要重新分布
- **路由配置**
  - 用一张路由表记录路由信息，实现与维护都很简单
  - 缺点：**必须多查询一次**，影响性能；如果路由表太庞大，其本身又会影响性能

对并发量的影响：

## 分库分表有哪些原则

分库分表原则：
- **能不分就不分**
  - 1000 万以内的表，不建议分片，通过合适的索引，读写分离等方式，可以很好的解决性能问题
- **分片数量尽量少**
  - 分片尽量均匀分布在多个 DataHost 上，因为**一个查询 SQL 跨分片越多**，则总体性能越差，虽然要好于所有数据在一个分片的结果，只在必要的时候进行扩容，增加分片数量
- **分片规则需要慎重选择**
  - 分片规则的选择，需要考虑数据的增长模式，数据的访问模式，分片关联性问题，以及分片扩容问题，
  - 最近的分片策略为范围分片，枚举分片，一致性 Hash 分片，这几种分片都有利于扩容
- **sharding column的选择应当慎重**
- **尽量不要在一个事务中的 SQL 跨越多个分片，分布式事务一直是个不好处理的问题**
- 查询条件尽量优化，尽量避免`select *`
  - 大量数据结果集下，会消耗大量带宽和 CPU 资源，查询尽量避免返回大量结果集，并且尽量为频繁使用的查询语句建立索引。

## 分库分表会造成哪些问题

https://www.jianshu.com/p/bf27be3fd448
分库分表的问题：
- ==跨库事务的问题（通过分布式事务解决）==
- **跨节点join的问题**
  - 水平分表后，虽然物理上分散在多个表中，如果需要与其它表进行join查询，需要在业务代码或者数据库中间件中进行多次join查询，然后将结果合并。
- **COUNT(*)的问题**
  - 水平分表后，某些场景下需要将这些表当作一个表来处理，那么count(*)显得没有那么容易 了。
- **跨节点合并排序、分页的问题**
- 多数据管理的问题
- **非sharding key的查询问题**(==水平分库分表，拆分策略为常用的hash法==)
  - 如果用于分库分表分区的列为`id`，但查询条件为`name`，怎么办？
    >  除了sharding key以外，**只有一个列**作为查询条件
    - 映射法
      - 在分库分表的同时维护一个映射表，将`name`映射到`id`
      - 也可以通过分布式缓存来实现映射，但要小心穿透的问题
      ![](./../images/MySQL/单键-映射法.jpg)
    - 基因法
      - 通过`x bit`的基因进行查询，假如需要分8张表$2^3=8$，则取`3bit`的基因。
      - 根据`user_id`查询时可直接取模路由到对应的分库或分表。
      - 根据`user_name`查询时：
        1. 先通过`user_name_code生成函数`将查询条件转换为`64 bit`的`user_name_code`，将其后`x bit`记录下来，作为基因
        2. 通过==分布式id生成算法（雪花算法）==得到`64 bit`的`user_id`
        3. 用之前得到的基因替换`user_id`的最后`x bit`
        4. 对最终的`user_id`取模，路由到对应的分库或分表。
      ![](./../images/MySQL/单键-基因法.jpg)
    
    >  除了sharding key以外，有**多个列**作为查询条件
    - 映射法
      - ![](./../images/MySQL/多键-映射法.jpg)
    - 冗余法
      - 按照order_id或buyer_id查询时路由到db_o_buyer库中，按照seller_id查询时路由到db_o_seller库中。
      - ![](./../images/MySQL/多键-冗余法.jpg)
    > 除了sharding key，还有各种复杂的非sharding key的组合条件查询
    - NoSQL法
      - 将分库分表的数据冗余存储到NoSQL服务中，例如ES、Hbase，遇到复杂请求就去NoSQL服务中取
    > ![](./../images/MySQL/非键-NoSQL.jpg)
    - 冗余法
      - 将分库分表的数据冗余存储到另一个数据库中，如果遇到复杂的请求，就通过消息队列，让那个存储较完整数据的数据库中查询
    > ![](./../images/MySQL/非键-冗余法.jpg) 

    > 除了上述的映射法、基因法、冗余法、NoSQL法之外，还可以通过搜索引擎解决非
- 非sharding key**跨页跨表分页查询**的问题(==水平分库分表，拆分策略为常用的hash法==)
  - 通过NoSQL法解决
- 扩容问题(==水平分库分表，拆分策略为常用的hash法==)
  - 升级从库法
  - 双写迁移法

## MySQL分库分表 冗余法的详细原理

https://zhuanlan.zhihu.com/p/60521049

## MySQL跨库事务的解决方案有哪些

可以通过分布式事务解决跨库事务的问题

- 两阶段提交协议（2PC）
  - 两阶段分别为：准备阶段与提交阶段，两个阶段都是由事务管理器（协调者）发起的，事务管理器能最大限度的保证跨数据库操作的事务的原子性。
  - 具体交互逻辑：
  - ![](./../images/MySQL/两阶段提交.png)
  - 优点：是分布式系统环境下最严格的事务实现防范，保证了数据一致性和操作原子性
  - 缺点：
    1. 难以进行水平伸缩，因为在提交事务的过程中，事务管理器需要和每个参与者都进行准备和提交的协调
    2. 参与者之间的协调需要时间，如果参与者较多，则锁定资源和消费资源之间的时间差就会变长，响应速度变慢，在此期间出现死锁或不确定结果的可能性变大
    3. 两阶段提交协议是阻塞协议，在极端情况下不能快速响应的话，会造成阻塞问题
- 最大努力保证模式
  > 最大努力保证模式是一种非常通用的保证分布式数据一致性的模式，适合**对一致性要求不是十分严格的但是对性能要求比较高**的场景
  - 在更新多个资源时，将多个资源的提交尽量延后到最后一刻进行处理。如果业务流程出现问题，则所有的资源更新都可以回滚，事务仍然保持一致。
  唯一可能出现问题的情况是在提交多个资源时，系统发生问题（例如网络问题），此时进行实时补偿，将已提交的事务进行回滚。（类似TCC模式）
  > 范例：一般情况下，使用消息中间件来完成消费者之间的事务协调，客户端从消息中间件的队列中消费消息，更新数据库，此时会涉及到两个操作，一是从消息中间件消费消息，二是更新数据库，具体的操作步骤如下：
  >   1. 开启消息事务
  >   2. 接收消息
  >   3. 开启数据库事务
  >   4. 更新数据库
  >   5. 提交数据库事务（关键）
  >   6. 提交消息事务（关键）
  > 
  > 上述步骤中，如果5成功了，但是在6出现了系统问题，此时，消息的消费过程并没有被提交到消息队列，消息队列可能会重新发送消息给其他消息处理服务，
  这会导致消息被重复消费，但是可以通过幂等处理来保证消除重复消息带来的影响。
  
- 事务补偿机制
  - 在对性能要求较高的场景中，两阶段提交无法得到足够的性能，而最大努力保证模式也会使多个分布式操作互相嵌套，有可能互相影响。**而事务补偿机制可以得到很高的性能，且能够尽最大可能地保证事务的最终一致性**。
  - 在数据库分库分表后，如果**涉及的多个更新操作在某一个数据库范围内完成**，则可以使用**数据库内的本地事务**保证一致性；
  - 对于**跨库的多个操作**，可通过**补偿**和**重试**，使其**在一定的时间窗口内完成操作**，这样就可以实现事务的最终一致性，突破事务遇到问题就回滚的传统思路。
   >如果采用事务补偿机制，则在遇到问题时，我们需要记录遇到问题的环境、信息、步骤、状态等，后续通过重试机制使其达到最终一致性， 

## 分库分表后的数据迁移方案

1. 修改系统中所有写库代码，让其同时写入旧库与新库，即双写
2. 通过后台数据迁移工具，将旧库中的数据根据最后修改时间等字段，迁移到新库中
   - 注意：不允许老数据覆盖新数据 
3. 比对数据，如果存在数据不一致，则继续复制
4. 数据完全一致后，基于仅仅使用分库分表的最新代码，重新部署一次

## 分布式事务的路由问题



## MySQL水平扩容了解吗

在分库分表后，如果涉及的分片已经达到了承载数据的最大值，就需要对集群进行扩容。扩容是很麻烦的，一般会成倍地扩容。

对MySQL的水平扩容方法如下：

- 升级从库法
  - ![](./../images/MySQL/扩容-升级从库.jpg)
  - 扩容是成倍增加的

- 双写迁移法
  - ![](./../images/MySQL/扩容-双写迁移法.jpg)
  - 双写迁移法即按照新旧分片规则，对新旧数据库进行双写，步骤如下：
    1. 将双写前按照旧分片规则写入的历史数据，根据新分片规则迁移写入新的数据库
    2. 以旧数据为基准，对新生成的数据库校准，如果有数据则进行更新
    3. 将按照旧的分片规则查询改为按照新的分片规则查询
    4. 将双写数据库逻辑从代码中下线，只按照新的分片规则写入数据
    5. 删除按照旧分片规则写入的历史数据
  > 在第(1)步的旧数据迁移中，由于数据量很大，通常会导致不一致。
  > 因此，先清洗旧的数据，洗完后再迁移到新规则的新数据库下，再做==全量对比==，对比后评估在迁移的过程中是否有数据的更新。如果有的话就再清洗、迁移，最后以对比没有差距为准。    

https://www.cnblogs.com/xuwc/p/14122187.html


## 分区和分库分表有什么区别

https://blog.csdn.net/kingcat666/article/details/78324678
https://www.cnblogs.com/langtianya/p/4997768.html

## 数据库是如何实现分页的,假设有100万条数据如何优化分页查询?

分页方式：客户端传递start页码与每页显示的条数，使用LIMIT选取查询的数据范围，进行分页
> 语句：select * from table limit (start-1)*limit,limit;

优化方式：
1. 直接使用数据库提供的SQL语句中的LIMIT关键字
   - 语句：`select * from table_name limit m, n;`：从结果集的m位置取出n条，其他抛弃
   - 适用于数据较少的情况，全表扫描，速度比较慢，且有的数据库结果集返回不稳定（结果顺序不稳定）
2. 建立主键或唯一索引，利用索引加速
   - 语句：`select * from table_name where id_pk > (pageNum * 10) limit n;`
   - 利用索引进行扫描，速度会很快，但值得注意的是，这样查出来的数据并不是按id_pk排序的，可能导致顺序混乱
3. 基于索引再排序
   - 语句：`select * from table_name where id_pk > (pageNum * 10) order by id_pk asc limit n;`
   - 在上一条的基础上加入了排序的语句，最好的情况是`order by`后的列对象是主键或唯一索引列，这样一来就能通过索引取代`order by`操作，且保证结果集是稳定的
   - 但MySQL的排序操作只有ASC，DESC是假的
4. 基于索引使用prepare
   - 语句：`prepare stmt_name from select * from table_name where id_pk > (? * ?) order by id_pk asc limit n;`
     - 第一个问号表示pageNum，第二个问号表示每页的元组数
   - 同样是利用索引进行扫描，prepare语句也比一般的查询语句快一点
5. 利用MySQL支持order操作可以利用索引快速定位部分元组，避免全表扫描
   - 语句：`select * from table_name where pa >= 1000 order by id_pk asc limit 0,20;`
     - 上面的语句表示：读第1000到1019行元组（==pk是主键/唯一键==）
6. 利用**子查询/连接+索引**快速定位元组的位置，然后直接读取元组
- 利用子查询：
```sql
假设id是主键

select * 
from table_name 
where id <= 
    ( 
        select id 
        from table_name 
        order by id desc 
        limit ($page - 1) * $pagesize 
    )
order by id desc 
limit $pagesize;
```
   - 利用连接：
```sql
select * 
from your_table 
as t1 join 
    (
        select id 
        from your_table 
        order by id desc 
        limit ($page - 1) * $pagesize 
    ) as t2 
where t1.id <= t2.id 
order by t1.id 
desc limit $pagesize;
```
 

## 如何使用临时表加快查询

测试表：
```sql
CREATE TABLE `department` (
  `dep_id` varchar(50),
  `dep_name` varchar(50)
) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_unicode_ci

CREATE TABLE `out_warehouse` (
  `dep_id` varchar(50),
  `out_remark` varchar(50),
  `out_total_amount` decimal(18,2),
  `out_date` date
) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_unicode_ci
```

建立临时表的方法如下：
```sql
DROP TEMPORARY TABLE IF EXISTS data1;
CREATE TEMPORARY TABLE data1 (dep_id BIGINT(20), dep_name VARCHAR(200));
 
INSERT INTO data1 SELECT t.dep_id,t.dep_name FROM department t;
 
SELECT * FROM out_warehouse t1 where t1.dep_id IN (SELECT dep_id FROM data1 );
DROP TEMPORARY TABLE IF EXISTS data1;
```

使用案例：
```sql
优化前：table1表数据100W+ table3表数据200W+
SELECT
  b.*,
  A.value1,
  A.value2
FROM table1 b
  LEFT JOIN table2A
    ON b.order_no = A.order_no AND b.channel_no = A.channel_no
WHERE 1 = 1 AND EXISTS 
  (
    SELECT 1
    FROM table3 t
    WHERE b.order_no = t.order_no AND t.ticket_no LIKE '%1792903240%'
  )
ORDER BY CREATE_TIME LIMIT 0, 20;
```
由于`table3`中有200w的数据，并且是循环式和外表扫描查询，且这里的like是不会走索引的，只能全表扫描，导致效率极低
由于是动态语句，并在存储过程中，所以优化就是拆解EXISTS这部分。

主要思路：先从200W+ 的table3中查出来`order_no`然后把`order_no`插入临时表，然后再使用`in`临时表查询，减少关联扫描次数就能极大的优化查询时间

前提： table3中的ticket_no 重复率非常低，200W+的数据 有200W的非重复
   - 为什么强调这个，临时表在处理少量数据时性能很优异
     - 一般只在确定不能用索引的时候才使用临时表
     - 或者在存储过程中某些**固定数据使用次数非常多**的时候使用临时表，其他时候我一般不建议使用
```sql
CREATE TEMPORARY TABLE tmp_order_no (ticket_order_no varchar(100));
 
INSERT INTO tmp_order_no SELECT tp.order_no FROM t_passenger tp WHERE tp.ticket_no LIKE CONCAT('%',2903240,'%');
 
SELECT
  b.*,
  A.value1,
  A.value2
FROM table1 b
  LEFT JOIN table2A
    ON b.order_no = A.order_no AND b.channel_no = A.channel_no
WHERE 1 = 1 AND b.order_no IN (SELECT ticket_order_no FROM tmp_order_no)
ORDER BY CREATE_TIME LIMIT 0, 20;
 
DROP TEMPORARY TABLE IF EXISTS tmp_order_no;
```

## 你启动mysql线程池配置参数相关的有哪些

1. max_connections：允许连接到MySQL数据库的最大连接数
2. back_log：MySQL监听TCP端口时设置的积压请求栈大小。连接数达到`max_connections`时，新来的请求会存在堆栈，以等待某一连接释放资源，等待数超过`back_log`则不授予连接资源，直接报错
3. table_open_cache：所有SQL执行线程可打开表缓存的数量
4. thread_cache_size：MySQL缓存客户服务线程的数量（相当于设置客户服务线程池的大小）
5. innodb_lock_wait_timeout：设置InnoDB事务等待行锁的时间

## MySQL操作优化

### 大批量插入数据

1. 按照主键顺序插入
  > InnoDB默认按照主键的顺序保存，如果没有主键，系统会自动默认创建一个内部列作为主键
2. 关闭唯一性校验（执行`SET UNIQUE_CHECKS=0`），可提高导入效率
3. 手动提交事务（执行`SET AUTOCOMMIT=0`），防止在每个INSERT语句中都自动生成事务

### 优化insert语句

1. 如果需要同时对一张表插入多行数据，应当尽量使用多个值表的insert语句，而不是多条单个的insert语句
2. 在事务中进行数据插入（手动提交事务）
3. 按主键有序地插入数据

### 优化order by语句

共包含两种排序方式：
1. filesort：对返回数据进行排序，即不直接通过索引返回排序结果的排序
2. using index：通过有序索引顺序扫描，直接返回有序数据，不需要额外排序
> 要求：
> 返回的字段被索引列全覆盖
> 要么全升序或全降序
> 排序字段的顺序与索引一致

filesort有两种实现算法：
- 两次扫描算法：在MySQL4.1之前使用。首先根据条件去除排序字段与行指针信息，然后在排序区sort buffer中排序，如果sort buffer不足，则在临时表temporary table中排序存出结果。完成排序后，根据行指针回表读取记录（该操作可能导致大量随机IO）。
- 一次扫描算法：一次性读取出满足条件的所有字段，然后再排序区sort buffer中排序后直接输出结果集。排序时内存开销较大，但是排序效率更高

MySQL通过比较系统变量`max_length_for_sort_data`的大小与Query语句取出的大小，来判断使用哪种算法。
如果`max_length_for_sort_data`更大，则使用一次扫描算法，保证效率

### 优化group by语句

group by也会进行排序操作，与order by类似，也会利用索引进行优化
如果查询包含group by，但用户想要避免排序结果的消耗，可以使用`order by null`禁止排序

### 优化嵌套查询

原则：用多表联结替换嵌套查询（子查询）

### or优化

必须保证or关联的各个字段均用到索引，且不能使用到复合索引

如果实在不能添加索引，可以通过union联结两个查询结果，替代原先的or

### limit优化

分页时实际上首先进行了排序，查询的起始页数很大时，可以通过以下思路进行优化：
- 在索引上完成排序分页，最后与原表进行联结，得到查询结果
 > `select * from tb_item t, (select id from tb_item order by id limit 200000,10) a where t.id = a.id;`

- 对于针对主键自增的表，可以把limit查询转换成某个位置的查询

### 使用SQL提示

SQL提示是优化数据库的一个重要手段，即在SQL语句中加入一些人为的提示，从而实现优化

1. USE INDEX：`select * from tb_seller use index(idx_seller_name) where name = 'xxxx'`
  > 提供一个索引列表，==建议==MySQL使用其中的索引，而不考虑其他可用索引

2. IGNORE INDEX
  > 让MySQL忽略指定的索引

3. FORCE INDEX