# 补充

## 项目中Redis使用，如何解决Redis宕机后系统不可用（脚手架集成；集群、封装Redis工具类并捕获异常，查数据库）



## 为什么用Redis作为缓存，不使用 应用服务器（Tomcat/JVM）作为缓存？


# Redis


## Redis是什么？**简述它的优缺点？**

Redis本质上是一个`Key-Value`类型的内存数据库，很像`memcached`，整个数据库统统加载在**内存**当中进行操作，定期通过**异步操作**把数据库数据flush到**硬盘**上进行保存。

因为是**纯内存**操作，Redis的性能非常出色，每秒可以处理超过 10万次读写操作，是已知性能最快的Key-Value DB。

Redis的出色之处不仅仅是性能，Redis最大的魅力是**支持保存多种数据结构**，此外**单个value的最大限制是1GB，不像 memcached只能保存1MB的数据**，因此Redis可以用来实现很多有用的功能。

比方说用他的List来做FIFO双向链表，实现一个轻量级的高性 能消息队列服务，用他的Set可以做高性能的tag系统等等。

另外Redis也可以对存入的Key-Value设置expire时间，因此也可以被当作一 个功能加强版的memcached来用。 

Redis的主要缺点是**数据库容量受到物理内存的限制**，不能用作海量数据的高性能读写，因此Redis适合的场景主要局限在较小数据量的高性能操作和运算上。

## Redis为什么这么快？

1. 完全基于内存，绝大部分请求是**纯粹的内存操作**，非常快速。数据存在内存中，类似于HashMap，HashMap的优势就是查找和操作的时间复杂度都是O(1)；

2. 数据结构简单，对数据操作也简单，且数据结构是经过**专门优化**的，效率很高；

3. 采用**单线程**，避免了不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗 CPU，不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗；
   且因为Redis的瓶颈不在CPU与内存，而是在IO上，所以采用多线程实际上并没有太多的帮助 

4. 使用**多路I/O复用**模型，NIO模型；

5. 使用底层模型不同，**底层实现方式以及与客户端之间通信的应用协议不一样**，Redis直接自己构建了VM 机制 ，因为使用一般的**系统调用**的话，需要进行用户态与内核态的切换以及数据的拷贝，浪费时间；

## **Redis相比memcached有哪些优势？**

* redis支持**丰富的数据类型**：Redis不仅仅支持简单的`k/v`类型的数据，同时还提供`list，set，zset，hash`等数据结构的存储。memcache支持简单的数据类型，String。
* redis支持**数据的持久化**，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用,而Memecache把数据全部存在内存之中。
* 集群模式：memcached没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据；但是 redis 目前是原生支持 cluster 模式的.
* Memcached是多线程，**非阻塞IO复用**的网络模型；Redis使用单线程的**多路 IO 复用**模型。

## 线程切换的开销主要是产生在哪里

1. 用户态和内核态的切换（内核级线程才需要）
2. 线程切换时，需要进行内核栈与硬件上下文的切换（主要是CPU寄存器内的切换）

> 进程切换需要额外进行地址空间的切换、虚拟内存、打开的相关句柄的切换

## redis的瓶颈在哪里，为什么瓶颈在网络的io

> Reids Server中所有的操作都是通过`Event Loop`**单线程串行调度**的
> `Event Loop`中每个loop中除了处理当前io event外, 还会处理**内存回收、RDB刷出及复制**等逻辑。

Redis单线程处理IO请求性能瓶颈主要包括2个方面：
- Redis是单线程的，**如果有一个请求的耗时比较长，就会到之后后面的请求排队等待，导致并发量变小**，例如以下情况：
  1. 操作`bigkey`：写入一个bigkey在分配内存时需要消耗更多的时间，同样，删除bigkey释放内存同样会产生耗时；
  2. 使用**复杂度过高的命令**：例如`SORT/SUNION/ZUNIONSTORE`，或者O(N)命令，但是N很大，例如lrange key 0 -1一次查询全量数据；
  3. **大量key集中过期**：==Redis的过期机制也是在主线程的事件循环中执行的==，**大量key集中过期会导致处理一个请求时，耗时都在删除过期key，耗时变长**；
  4. **淘汰策略**：==淘汰策略也是在主线程的事件循环执行的==，当内存超过Redis内存上限后，每次写入都需要淘汰一些key，也会造成耗时变长；
  5. AOF刷盘开启`always`机制：**每次写入都需要把这个操作刷到磁盘**，写磁盘的速度远比写内存慢，会拖慢Redis的性能；
  6. **主从全量同步**生成`RDB`：虽然采用fork子进程生成数据快照，但**fork这一瞬间也是会阻塞整个线程的，实例越大，阻塞时间越久**；

- 并发量非常大时，单线程读写客户端IO数据存在性能瓶颈，**虽然采用IO多路复用机制，但是读写客户端数据依旧是同步IO，只能单线程依次读取客户端的数据，无法利用到CPU多核**
> 针对这一问题，redis使用多线程读写客户端数据，进行改进


## 介绍一下BIO、NIO、AIO与IO多路复用

> 同步：用户进程触发IO操作时，需要等待或者轮询的去查看IO操作是否就绪
> 异步：用户进程触发IO操作以后便开始做自己的事情，而当IO操作已经完成的时候会**得到IO完成的通知**

> 阻塞：用户进程调用`read()`时，如果内核缓冲区的数据未准备好，则用户线程陷入阻塞，直到内核通知用户进程数据准备好了


### BIO
BIO：同步阻塞IO，实现模式为一个连接一个线程，即当有客户端连接时，服务器端需为其单独分配一个线程，如果该连接不做任何操作就会造成不必要的线程开销。
  - 如图所示，当调用系统调用`read`时，用户线程会**一直阻塞到内核空间有数据到来为止**，否则就一直阻塞。
  - 
  1. 用户线程调用了`read`系统调用，内核就开始了IO的第一个阶段：**准备数据**。
    很多时候，数据在一开始还没有到达（比如，还没有收到一个完整的Socket数据包），这个时候kernel就要等待足够的数据到来。
  2. 当kernel一直等到数据准备好了，它就会将数据从**kernel内核缓冲区，拷贝到用户缓冲区（用户内存），然后kernel返回结果。
  3. 从开始IO读的read系统调用开始，用户线程就进入阻塞状态。一直到kernel返回结果后，用户线程才解除block的状态，重新运行起来。

![](./../images/redis/BIO.png)


### NIO
NIO：同步非阻塞IO，在一个线程内不断的轮询数据的状态，看看是否有数据准备好发生了改变，从而进行下一步的操作。

但NIO仍然是同步的，即在数据准备好后，将数据从内核缓冲区复制到用户缓冲区时，**用户线程仍然是要阻塞的**

  - NIO的read读操作系统调用，流程如下：
    1. 在内核数据没有准备好的阶段（**内核缓冲区没有数据**），用户线程发起IO请求时，**立即返回**。用户线程需要不断地发起IO系统调用。
    2. 内核数据到达后（**内核缓冲区有数据**），用户线程发起系统调用，**用户线程阻塞**。
        内核开始复制数据。它就会将数据**从内核缓冲区拷贝到用户缓冲区**（用户内存），然后kernel返回结果。
    3. 用户线程解除阻塞的状态，重新运行起来，读取数据，继续执行。

![](./../images/redis/NIO.png)

> 如图，多次调用**不同socket**的read，当内核缓冲区没有数据，则马上返回，直到第N次调用read，才发现内核空间有数据，然后才开始真正读数据

### IO多路复用
  - IO多路复用模型，就是通过一种新的系统调用，**一个进程可以监视多个文件描述符(如socket)，一旦某个描述符就绪（一般是内核缓冲区可读/可写），内核kernel能够通知程序进行相应的IO系统调用**。
  - 在一定程度上来说，IO多路复用算是**同步阻塞**的一种，因为**select会阻塞到有socket数据就绪为止**。
  - IO多路复用的调用过程：
    1. 进行`select/epoll`系统调用，查询可以读的连接。
       - kernel会查询所有select的可查询socket列表，**当任何一个socket中的数据准备好了，select就会返回**。
    > ==当用户进程调用了select，那么整个线程会被block（阻塞掉）==。
    2. 用户线程获得了目标连接后，发起`read`系统调用，**用户线程阻塞**。内核开始复制数据，将数据从内核缓冲区拷贝到用户缓冲区（用户内存），然后kernel返回结果。
    3. 用户线程解除block的状态，用户线程终于真正读取到数据，继续执行。

![](./../images/redis/IO多路复用.png)

> IO多路复用与NIO的区别：
> - 调用方式：
>   - NIO需要在用户程序的循环语句中**不停地检查各个socket是否有数据读入**，
>   - IO多路复用在用户程序层面则**不需要循环语句**，虽然IO多路复用也是轮询，但是**IO多路复用是交给内核进行各个socket的监控的**。
> - 实现原理
>   - 由于NIO多次调用`read`这种系统调用，因此会**造成用户态和内核态的频繁转换**
>   - IO多路复用则是先调用`select`这个系统调用去查询**是否有数据就绪的socket**，然后有数据就绪，才调用`read`这个系统调用来读。
> - 处理IO请求的方式
>   - NIO处理请求的方式是：一个请求一个线程。对每个数据准备好的IO请求，都需要
>   
> 所以从性能上来说，IO多路复用会比NIO好。

### AIO

AIO：异步非阻塞I/O模型。异步非阻塞IO基于事件与回调机制实现，**无需一个线程去轮询所有IO操作的状态改变**，在相应的状态改变后，系统会通知对应的线程来处理。

调用流程：
1. 当用户线程调用了read系统调用，**立刻就可以开始去做其它的事，用户线程不阻塞**。
2. 内核（kernel）就开始了IO的第一个阶段：准备数据。当kernel一直等到数据准备好了，它就会将数据从kernel内核缓冲区，拷贝到用户缓冲区（用户内存）。
3. kernel会给用户线程发送一个信号（signal），或者回调用户线程注册的回调接口，告诉用户线程read操作完成了。
4. 用户线程读取用户缓冲区的数据，完成后续的业务操作。 

![](./../images/redis/AIO.png)

> AIO中，用户都不需要自己调用read()接口，将数据从内核缓冲区复制到用户缓冲区了，只要在最后等通知就行了，流弊！

## BIO、NIO、AIO、IO多路复用各有什么优缺点

BIO：严重依赖线程
- 线程的创建和销毁成本很高；
- 线程本身占用较大内存；
- 线程切换成本很高；
- 容易造成锯齿状的系统负载；

NIO：
- 优点：每次发起的 IO 系统调用，在内核的等待数据过程中可以立即返回。用户线程不会阻塞，则表示**用户线程不用呆呆地等待数据到来**，而是可以去干其他活了
- 缺点：
  - 需要不停地发起read()IO系统调用，这种不断的轮询，将会不断地询问内核，这将占用大量的 CPU 时间，系统资源利用率较低。
  - 任务完成(处理到来数据)的响应延迟增大了，因为每过一段时间才去轮询一次 read 操作，而任务可能在两次轮询之间的任意时间完成。这会导致整体数据吞吐量的降低

IO多路复用：即Java中的NIO（New IO）
- 优点：
  - IO多路复用是交给内核进行各个socket的监控的，不需要由用户来进行轮询
  - 通过`select/poll`这个系统调用实现轮询就绪的socket，不需要频繁进行用户态与内核态的切换
  - 通过`select/poll`实现轮询socket，可以同时处理上万个连接
- 缺点:
  - 本质上，select/epoll系统调用，属于同步阻塞IO。都需要在读写事件就绪后，自己负责进行读写，也就是说这个读写过程是阻塞的。


## IO多路复用模型的底层接口是什么

select，poll，epoll都是IO多路复用的机制，I/O多路复用就通过一种机制，可以**监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作**。
但`select`，`poll`，`epoll`本质上都是**同步I/O**，因为他们都需要在读写事件就绪后**自己负责进行读写**，也就是说这个读写过程是**阻塞**的

### select

`select()`系统调用 实现多路复用的方式是：
1. 将已连接的 `Socket` 都放到一个文件描述符集合
2. 调用 `select` 函数时，将文件描述符集合**从用户空间拷贝到内核里**，**内核通过遍历文件描述符集合**的方式，来检查是否有网络事件发生。 
   - 如果没有，则令用户线程陷入阻塞
3. 当检查到有事件产生后，将此 `Socket` **标记为可读或可写**，唤醒阻塞的用户线程，并**将整个文件描述符集合拷贝回用户空间里**
4. 用户态再通过**遍历文件描述符**的方法，找到可读或可写的 Socket，然后再对其处理。

> 在select的方式中，需要进行**2次文件描述符集合的遍历**，分别是在内核态与用户态各进行一次；
而且还会产生**2次文件描述符集合的拷贝**
>
> 除此之外，select的触发方式是**水平触发**，应用程序**如果没有完成对一个已经就绪的文件描述符进行IO操作**，那么**之后每次select调用还是会将这些文件描述符通知进程**。

select 使用固定长度的`BitsMap`记录socket的文件描述字，导致其支持的文件描述符的个数是有限制的，在 Linux 系统中，由内核中的 `FD_SETSIZE` 限制，即**单个进程的文件描述符上限**， 默认最大值为 1024，只能监听 0~1023 的文件描述符。（64位的机器上为2048）
> 文件描述符仅仅是文件描述符表的下标

注意：单个进程的文件描述符上限是可以修改的，但BitsMap的大小只能通过重新编译linux内核来修改，所以一般认为是不可修改的

> 在内核中，Socket也是以**双向管道文件**的形式存在的，也有对应的文件描述符
> 因此，读写Socket的方式也类似于读写文件的方式
> > 文件描述符：每一个进程都有一个数据结构 `task_struct`，该结构体里有一个指向「文件描述符数组」的成员指针。该数组里列出这个进程打开的所有文件的文件描述符。文件描述符就是该数组的下标，是一个非负整数，而数组的内容是一个指针，指向内核中所有打开的文件的列表，也就是说**内核可以通过文件描述符找到对应打开的文件**
> 并通过文件描述符表到内核的打开文件表中，查询inode表，从而找到文件所在的block

因此，select主要有三大缺点：
1. 每次调用select，都需要**把文件描述字集合从用户态拷贝到内核态**，这个开销在文件描述字集合很多时会很大
2. 每次调用select都需要**在内核遍历传递进来的所有文件描述符**
3. select支持的文件描述符数量太小了，默认是1024
4. select是**水平触发**，没有及时处理的就绪的文件描述符会被多次拷贝、遍历

### poll

在`poll()`系统调用中，不再使用`bitsMap`存储所关注的文件描述符，取而代之的是**以链表形式组织的动态数组**，突破了select的文件描述符个数限制（实际上，还是会有文件描述符个数的上限，不可能是无限的）

但是 poll 和 select 并没有太大的本质区别，都需要**遍历文件描述符集合**来找到可读或可写的 `Socket`，时间复杂度为 O(n)，而且**也需要在用户态与内核态之间拷贝文件描述符集合**，这种方式随着并发数上来，性能的损耗会呈指数级增长。

> 还是需要进行两次遍历与两次复制

### epoll

select低效的原因之一，就是将“维护等待队列”和“阻塞进程”两个步骤合二为一（select），因此epoll将原先的`select/poll`调用划分为三个部分：
1. `epoll_create()`：建立一个`epoll`对象(在epoll文件系统中为这个句柄对象分配资源)，并**返回该对象的句柄**（同时创建了**红黑树**和**就绪链表**）
2. `epoll_ctl()`：操作epoll函数生成的对象，可用于向epoll对象中添加、删除、修改感兴趣的事件，并返回操作成功的标识
3. `epoll_wait()`：**收集在epoll监控中已经准备好的事件**，没有就sleep，超时后返回

针对select的三大缺点，epoll主要通过以下措施进行优化：
- select时，拷贝文件描述字集合的开销大：
  - epoll在调用`epoll_ctl`将事件注册到epoll对象中时，**直接将文件描述字拷贝到内核**，而不是在`epoll_wait`时重复拷贝
    > 在select和poll中，每次`select()`，都会将关注的socket的文件描述符拷贝到内核态，之后再拷贝回用户态
- 每次select时，都需要在内核遍历所有文件描述字
  - epoll只需要在调用`epoll_ctl`时将**挂载到红黑树形式的等待队列中**，并为每个文件描述符设置一个**回调函数**。
  - 当设备就绪，唤醒等待队列上的等待者时，就会调用这个回调函数，**将就绪的文件描述符加入一个就绪链表**
  - `epoll_wait`的工作实际上就是在这个**就绪链表**中查看有没有就绪的文件描述字
- 监控的文件描述字存在上限（1024）
  - epoll与poll没有这个限制

epoll通过两方面的措施，很好的解决了`select/poll`的问题
- epoll 在==内核==里**使用红黑树来跟踪进程所有待检测的文件描述字，把需要监控的 socket 通过 `epoll_ctl()` 函数加入内核中的红黑树里**，通过对红黑树进行操作，==这样就不需要像 `select/poll` 每次操作时都传入整个 socket 集合==，**只需要将一个待检测的 socket传入等待链表中接口**，减少了内核和用户空间大量的数据拷贝和内存分配。
-  epoll 使用**事件驱动**的机制，内核里**维护了一个链表来记录就绪事件**，**当某个 socket 有事件发生时，通过回调函数内核会将socket的文件描述符加入到这个就绪事件列表中**。
   -  当用户调用 `epoll_wait()` 函数时，**只需要在就绪链表中操作，不需要像 select/poll 那样轮询扫描整个 socket 集合**，大大提高了检测的效率。

> epoll直接在`epoll_ctl`中，将监控的socket的文件描述字以红黑树的结构存储在内核中
> 而select/poll则是存储在用户空间，之后复制到内核

![](./../images/redis/epoll.png)

> 使用红黑树存储socket的文件描述符的原因：
> epoll是需要将整个文件描述符集合持久地保存在内核中的，而poll和select则是保存在用户空间，因此，epoll需要频繁地对文件描述符集合进行增删改查，就需要一个高效的数据结构，保证每个操作的效率，因此选择红黑树存储

### 触发方式

epoll除了提供`select/poll`那种IO事件的**水平触发（Level Triggered）**外，还提供了**边缘触发（Edge Triggered）**，这就使得用户空间程序有可能缓存IO状态，减少`epoll_wait/epoll_pwait`的调用，提高应用程序效率。

- 水平触发（LT）：默认工作模式，即当`epoll_wait`检测到某描述符事件就绪并通知应用程序时，应用程序可以不立即处理该事件；==下次调用`epoll_wait`时，会再次通知此事件==
- 边缘触发（ET）： 当`epoll_wait`**检测到某描述符事件就绪并通知应用程序时**，==应用程序必须立即处理该事件==。
  - **如果不处理，下次调用epoll_wait时，不会再次通知此事件**
   （直到做了某些操作导致该描述符变成未就绪状态了，也就是说**边缘触发只在状态由未就绪变为就绪时只通知一次**）。

### 总结

1. select，poll实现需要自己不断轮询所有fd集合，直到设备就绪，期间可能要睡眠和唤醒多次交替。而epoll其实也需要调用`epoll_wait`不断轮询就绪链表，期间也可能多次睡眠和唤醒交替，但是它是设备就绪时，调用回调函数，把就绪fd放入就绪链表中，并唤醒在epoll_wait中进入睡眠的进程。虽然都要睡眠和交替，但是**select和poll在“醒着”的时候要遍历整个fd集合，而epoll在“醒着”的时候只要判断一下就绪链表是否为空就行了**，这节省了大量的CPU时间。这就是回调机制带来的性能提升。

2. select，poll每次调用都要把fd集合从用户态往内核态拷贝一次，并且要把current往设备等待队列中挂一次，而epoll只要一次拷贝，而且把current往等待队列上挂也只挂一次（在epoll_wait的开始，注意这里的等待队列并不是设备等待队列，只是一个epoll内部定义的等待队列）。这也能节省不少的开销。


## Reactor和Proactor

Reactor模式用于同步IO，Proactor模式用于异步IO

### Reactor

Reactor是对IO多路复用的面向对象封装，Reactor模式也叫`Dispatcher`模式，这一名称更符合Reactor的行为模式： **I/O 多路复用监听事件，收到事件后，根据事件类型分配（Dispatch）给某个进程 / 线程**

Reactor模式主要包含**Reactor**与**处理资源池**这两个核心部分：
- Reactor 负责**监听和分发事件**，事件类型包含连接事件、读写事件
- 处理资源池负责**处理事件**，如 `read -> 业务逻辑 -> send`

其中，Reactor可以有一个，也可以有多个；处理资源池可以是单个进程 / 线程，也可以是多个进程 /线程。

因此，常见的Reactor实现方案有以下几种：
- 单 Reactor 单进程 / 线程；（单线程Reactor）
- 单 Reactor 多线程 / 进程；（多线程Reactor）
- 多 Reactor 多进程 / 线程；（主从Reactor）
> 具体使用多线程还是多进程，主要取决于编码的平台，Java一般为多线程（Netty），基于C的则都行：Nginx使用的是多进程，memcached使用多线程

> 下面均以线程为例

**单 Reactor 单进程 / 线程（单线程Reactor模型）**

![](./../images/redis/单Reactor单线程.jpg)

其中，主要包含Reactor、Acceptor、Handler三个对象：
- Reactor 对象的作用是**监听和分发事件**；
- Acceptor 对象的作用是**获取连接**；
- Handler 对象的作用是**处理业务**；

单Reactor单线程的流程为：
- `Reactor` 对象通过 `select`（IO 多路复用接口）监听事件，收到事件后通过 `dispatch` 进行分发，具体分发给 `Acceptor` 对象还是 `Handler` 对象，还要看**收到的事件类型**：
   - 如果是**连接建立**的事件，则交由 `Acceptor` 对象进行处理，Acceptor 对象会通过 `accept` 方法 获取连接，**并创建一个 Handler 对象来处理后续的响应事件**；
   - 如果不是连接建立事件，则交由**当前连接对应的 Handler 对象**来进行响应；Handler 对象通过 `read -> 业务处理 -> send` 的流程来完成完整的业务流程。


单Reactor单线程，不用考虑进程间通信以及数据同步的问题，实现简单   
但是，这种方案存在两个缺点：
1. 因为只有一个进程，无法充分利用 多核 CPU 的性能；
2. **Handler 对象在业务处理时，整个线程是无法处理其他连接的事件的**，如果业务处理耗时比较长，那么就造成响应的延迟；

> 单Reactor单线程模式**不适用计算密集型的场景，只适用于业务处理非常快速的场景**
> 例如Redis

**单Reactor多线程（多线程Reactor模型）**

为了克服单Reactor单线程的缺点，引入了多线程。
![](./../images/redis/单Reactor多线程.jpg)

单Reactor多线程的方案与单线程类似，主要的区别体现在`Handler`的任务上：
- Handler 对象**不再负责业务处理，只负责数据的接收和发送**，`Handler` 对象通过 `read` 读取到数据后，会**将数据发给子线程里的 `Processor` 对象进行业务处理**；
- 子线程里的 `Processor` 对象就进行业务处理，处理完后，将结果发给主线程中的 `Handler` 对象，接着由 `Handler` 通过 `send` 方法将响应结果发送给 `client`；
> 即，Handler的任务流程变为：read -> 让Processor进行业务处理 -> send

由于引入了多线程，不可避免的要面对多线程竞争资源的问题。

> 例如，子线程完成业务处理后，要把结果传递给主线程的 Reactor 进行发送，这里涉及共享数据的竞争。

要避免多线程由于竞争共享资源而导致数据错乱的问题，就**需要在操作共享资源前加上互斥锁**，以保证任意时间里只有一个线程在操作共享资源，待该线程操作完释放互斥锁后，其他线程才有机会操作共享数据。

而对于单Reactor多进程的方案，进程间的通信就更麻烦了，这就导致通常不采用多进程的方案，而是使用多线程，尽管其需要考虑并发安全的问题。

但尽快线程数增加了，单Reactor多线程方案仍有可提升的地方，**一个 Reactor 对象承担所有事件的监听和响应，而且只在主线程中运行，在面对瞬间高并发的场景时，容易成为性能的瓶颈的地方**

**主从Reactor模型**

![](./../images/redis/多Reactor多线程.jpg)

方案如下：

- 主线程中的 `MainReactor` 对象通过 `select` 监控连接建立事件，收到事件后通过 `Acceptor` 对象中的 `accept` 获取连接，将新的连接分配给某个子线程；
- 子线程中的 `SubReactor` 对象将 `MainReactor` 对象分配的连接加入 `select` 继续进行监听，并创建一个 `Handler` 用于处理连接的响应事件。
- 如果有新的事件发生时，`SubReactor` 对象会调用当前连接对应的`Handler`对象来进行响应。
- `Handler` 对象通过 `read -> 业务处理 -> send` 的流程来完成完整的业务流程。

多 Reactor 多线程的方案虽然看起来复杂的，但是实际实现时比单 Reactor 多线程的方案要简单的多，原因如下：
- **主线程和子线程分工明确，主线程只负责接收新连接，子线程负责完成后续的业务处理**。
- 主线程和子线程的交互很简单，主线程只需要把新连接传给子线程，子线程无须返回数据，直接就可以在子线程将处理结果发送给客户端。
- **在子线程中的SubRector中，仍为单Reactor单线程，不必考虑并发时的线程安全问题，设计简洁**


### Proactor
Reactor 是**非阻塞同步网络模式**，而 Proactor 是**异步网络模式**

- **Reactor 是非阻塞同步网络模式，感知的是就绪可读写事件**。
  - 在每次感知到有事件发生（比如可读就绪事件）后，就需要**应用进程主动调用 read 方法来完成数据的读取**，也就是要应用进程主动将 socket 接收缓存中的数据读到应用进程内存中，这个过程是**同步**的，读取完数据后应用进程才能处理数据。
- **Proactor 是异步网络模式， 感知的是已完成的读写事件**。
  - 在发起异步读写请求时，需要传入数据缓冲区的地址（用来存放结果数据）等信息，这样系统内核才可以自动帮我们把数据的读写工作完成，**这里的读写工作全程由操作系统来做**，并不需要像 Reactor 那样还需要应用进程主动发起 read/write 来读写数据，操作系统完成读写工作后，就会通知应用进程直接处理数据。

无论是 Reactor，还是 Proactor，都是一种基于**事件分发**的网络编程模式，区别在于 Reactor 模式是基于**待完成的 I/O 事件**，而 Proactor 模式则是基于**已完成的 I/O 事件**

Proactor模式的示意图如下：

![](./../images/redis/Proactor.jpg)

其主要工作流程为：
- `Initiator` 负责创建 `Proactor` 和 `Handler` 对象，并将 `Proactor` 和 `Handler` 都通过 `Asynchronous Operation Processor`（异步操作处理器） **注册到内核**；
- `异步操作处理器` 负责**处理注册请求，并处理 I/O 操作**；
- `异步操作处理器` 完成 I/O 操作后通知 `Proactor`；
- `Proactor` 根据不同的事件类型**回调**不同的 `Handler` 进行业务处理；
-  `Handler` 完成业务处理；

### 应用范例

单Reactor单线程/进程：redis（进程，业务非常快）
单Reactor多线程/进程：
多Reactor多线程/进程：Nginx、Netty、memcache


## Redis 的数据类型？

Redis 支持五种数据类型（对象类型）： `string`（字符串），`hash`（哈希）， `list`（列表）， `set`（集合） 及 `zset`：（有序集合)。

- `string`：Redis采用的是简单动态字符串，字符串 value 最大可为`512M`。可以用来做一些计数功能的缓存（也是实际工作中最常见的）。 
  - 常规计数：微博数，粉丝数等。
- `list`：list本质上是一个==双向链表==，其key通过链表相连，实现为一个**双向链表**，即可以支持反向查找和遍历，不过带来了部分额外的内存开销。
  - 可以实现一个简单**消息队列**功能，做基于redis的**分页功能**等。
  - 另外可以通过 `lrange` 命令，就是**从某个元素开始读取多少个元素**，可以基于 `list` 实现**分页查询**，基于 redis 实现简单的高性能分页，可以做类似微博那种下拉不断分页的东西（一页一页的往下走），性能高。
- `set`：是一个字符串类型的无序集合。
  - 可以用来进行**全局去重**等。(比如：在微博应用中，可以将一个用户所有的关注人存在一个集合中，将其所有粉丝存在一个集合。Redis可以非常方便的实现如共同关注、共同粉丝、共同喜好等功能。这个过程也就是求交集的过程)
- `sorted set`：是一个**字符串类型的有序集合**，给每一个元素一个**固定的分数score**来保持顺序。
  - 可以用来做**排行榜应用**或者进行**范围查找**等。
- `hash`：键值对集合，是一个字符串类型的 `Key`和`Value` 的映射表
  - 适合用于存储对象，后续操作的时候，你可以直接仅仅修改这个对象中的某个字段的值。 比如我们可以 hash 数据结构来存储用户信息，商品信息等等。

### 高级数据结构

- HyperLogLog
- Geo
- Pub/Sub
- Redis Module
- BloomFilter
- RedisSearch
- Redis-ML

## String的底层数据结构是啥

Redis只会将C字符串应用于字面量，大部分情况下，Redis使用**SDS（Simple Dynamic String， 简单动态字符串）1**作为字符串表示
![](./../images/redis/SDS.jpg)



比起C字符串，`SDS`具有以下优势：
- 实现了**O(1)的字符串长度计算**
- 杜绝了缓冲区溢出的危险
  - SDS的API在修改前会检查空间是否足够，不会出现C中的临近字符串受影响的问题
- **减少修改字符串长度所需的内存分配次数**
  - C字符串底层为**数组**，每次正常或缩短字符串，都要重新分配内存
  - SDS通过**空间预分配**（**扩展时额外分配空间**）优化了字符串增长操作，通过**惰性空间释放**（**缩短时不立刻回收多余的空间**，而是用free记录这些空间）优化字符串缩短操作
- 实现了二进制安全
  - C字符串保存的是**文本数据**，必须符合某种编码，不能出现特殊符号
  - Redis的SDS以处理**二进制**的方式处理其中的字符，而不是将其视作字符（即不用任何预先就有的规则对数据进行限制、过滤、假设）
- 兼容部分C字符串函数

> 当字符串比较短的时候，len 和 alloc 可以使用 byte 和 short 来表示，Redis 为了对内存做极致的优化，不同长度的字符串使用不同的结构体来表示。

## List的底层数据结构

满足以下条件时，使用`zipList`数据结构保存数据，否则使用`linkedlist`:
- **所有字符串元素的长度都小于64字节**
- **保存的元素数量小于512个**

在redis3.2之后，用`quicklist`替代`ziplist`和`linkedList`

### 双向链表`linkedlist`
  - 常规的双向链表，Node节点包含prev和next指针，可以进行双向遍历，包含head和tail指针

因为双向链表占用的内存比压缩列表要多，所以当创建新的列表键时，列表会**优先考虑使用压缩列表**，并且在有需要的时候，才从压缩列表实现转换到双向链表实现。

![](./../images/redis/linkedlist结构.jpg)

双向链表的缺点：
- 在64位系统下，每个指针占`16Byte`，而每个Node都包含prev和next指针，造成了空间的浪费（zip的改进点之一）
- 每个节点都是单独分配的，不要求连续，可能导致**内存的碎片化**

压缩列表转化成双向链表条件：
- 试图往列表新添加一个字符串值，且这个字符串的长度超过`server.list_max_ziplist_value` （默认值为 64 ）。
- ziplist的节点数超过 `server.list_max_ziplist_entries` （默认值为 512 ）。


### 压缩列表`ziplist`

当List中的元素较少，或每个元素都是小整数或长度较短的字符串时，使用ziplist，目的是用时间换空间

ziplist是由一系列**特殊编码的连续内存块**组成的顺序存储结构，类似于数组，==ziplist在内存中是**连续存储**的==，因此，**在ziplist中不需要使用指针，节省了大量空间**
但是不同于数组，为了节省内存，**ziplist的每个元素所占的内存大小可以不同**（数组中叫元素，ziplist叫节点entry，下文都用“节点”），每个节点可以用来存储一个整数或者一个字符串。
![](./../images/redis/ziplist-结构.jpg)
![](./../images/redis/ziplist.png)

- `zlbytes`: ziplist的长度（单位: 字节)，是一个32位无符号整数
- `zltail`: ziplist最后一个节点的偏移量，反向遍历ziplist或者pop尾部节点的时候有用。
- `zllen`: ziplist的节点（entry）个数
- `entry`: 节点
- `zlend`: 值为`0xFF`即`255`，用于标记ziplist的结尾

> 普通数组的遍历是根据**数组里存储的数据类型**找到下一个元素的，例如int类型的数组访问下一个元素时每次只需要移动一个**sizeof(int)**就行（实际上开发者只需让指针p+1就行，在这里引入sizeof(int)只是为了说明区别）。
上文说了，**ziplist的每个节点的长度是可以不一样的**，而我们面对不同长度的节点又不可能直接sizeof(entry)，那么它是怎么访问下一个节点呢？

ziplist将一些**必要的偏移量信息**记录在了**每一个节点**里，**使之能跳到上一个节点或下一个节点**。

![](./../images/redis/entry.png)

每个节点由三部分组成：prevlength、encoding、data

- prevlengh: 记录上一个节点的长度，为了方便反向遍历ziplist
- encoding: 当前节点的编码规则，下文会详细说
- data: 当前节点的值，可以是数字或字符串 

为了节省内存，根据==上一个节点的长度prevlength==，可以将ziplist节点分为两类：
- 用`1字节`描述上一节点长度：entry的**前8位小于254**，则**这8位就表示上一个节点的长度**
- 用`5字节`描述上一节点长度：entry的**前8位等于254**，则意味着上一个节点的长度无法用8位表示，**后面32位才是真实的prevlength**。
  - 用254 不用255(11111111)作为分界是因为**255是zlend的值**，它**用于判断ziplist是否到达尾部**。

根据上述entry布局，可以看出，若要算出data的偏移量，得先计算出prevlength所占内存大小（1字节和5字节）。
接着即可逐步计算encoding所占的字节、data所占字节，即可得到最终的len

#### 操作的时间复杂度

- 往ziplist里插入一个entry 时间复杂度 平均:O(n), 最坏:O(n²)
- 从ziplist里删除一个entry 时间复杂度 平均:O(n), 最坏:O(n²)

**为什么插入节点和删除节点两个接口的最坏时间复杂度会是O(n²)呢？**

这是由于ziplist的==连锁更新==导致的。

连锁更新：假设一个ziplist中，**连续多个entry的长度都是一个接近但是又不到254的值（即用1字节表示prevLength）**，如果添加了一个长度大于`254`的节点（或通过修改实现类似的效果），则下一个节点需要将prevLength改为5字节，如果此时这个节点的长度也因此超过了254字节，则会导致后续的连锁更新。

连锁更新在最坏情况下需要对ziplist执行n次空间重分配操作，而且每次空间重分配的最坏时间复杂度为O(n)
但是出现“连锁更新”的情况并不多见，所以这里基本不会造成性能问题。

#### 整数节点与字符串节点分类

除了prevLength的区别，根据当前节点存储的数据类型及长度，可以将ziplist节点分为9类：
其中整数节点分为6类： 

![](./../images/redis/整数.png)

整数节点的`encoding`的长度为`8位`，其中**高2位用来区分整数节点和字符串节点**（高2位为11时是整数节点），**低6位用来区分整数节点的类型**

> 值得注意的是 最后一种encoding是存储整数`0~12`的节点的encoding，它没有额外的data部分，encoding的高4位表示这个类型，低4位就是它的data。
> 这种类型的节点的encoding大小介于`ZIP_INT_24B`与`ZIP_INT_8B`之间（`1~13`），但是为了表示整数0，取出低四位xxxx之后会将其-1作为实际的data值（`0~12`）。

![](./../images/redis/字符串.png)

- 当data小于63字节时(2^6)，节点存为上图的第一种类型，高2位为00，低6位表示data的长度。
- 当data小于16383字节时(2^14)，节点存为上图的第二种类型，高2位为01，后续14位表示data的长度。
- 当data小于4294967296字节时(2^32)，节点存为上图的第二种类型，高2位为10，下一字节起连续32位表示data的长度。

> 上图可以看出：
> 不同于整数节点encoding永远是8位，字符串节点的encoding可以有8位、16位、40位三种长度
> - 相同encoding类型的**整数节点**，data长度是固定的，
> - 但是相同encoding类型的**字符串节点**，data长度取决于encoding后半部分的值。



### quickList：替代zipList和linkedlis

`quicklist` 实际上是 `zipList` 和 `linkedList` 的混合体，它将 `linkedList` 按段切分，每一段使用 `zipList` 来紧凑存储，多个 `zipList` 之间使用双向指针串接起来。

> zipList是连续存储，LinkedList是离散存储，这样相当于结合两者的优点了

![](./../images/redis/QuickList.png)

quickList相对于linkedlist，提高了空间利用率，相对于ziplist，提高了效率。


## Hash的底层数据结构

哈希对象的编码方式有两种，分别是`ziplist`，`hashtable`

满足以下条件时，使用`zipList`数据结构保存数据，否则使用`hashtable`进行编码，使用==哈希字典==数据结构保存数据:
- 保存的**所有键值对键和值的字符串长度都小于64字节**
- **键值对数量小于512个**

### ziplist保存哈希对象

**同一键值对的两个节点总是紧挨在一起**，键在前，值在后。

先添加到哈希对象的键值对会被放在ziplist的表头方向，后来添加的会被放在ziplist的表尾方向

![](./../images/redis/ziplist-哈希.jpg)

### 哈希表保存哈希对象

Redis的字典使用HashTable为底层实现，HashTable以数组table作为Node数组，table中的每个元素是dictEntry结构，即一个链表

![](./../images/redis/哈希字典.jpg)

注意：==在一个哈希字典中，包含了两张哈希表==，且维护了一个`rehashidx`，作为rehash的标记。
维护两张哈希表，是为了REHASH过程中，将一张哈希表的内容转移到另一张表准备的。

> Redis的HashTable采用链地址法解决哈希冲突，与HashMap类似

哈希算法：
```java
//1、使用字典设置的哈希函数，计算键 key 的哈希值
hash = dict->type->hashFunction(key);
//2、使用哈希表的sizemask属性和第一步得到的哈希值，计算索引值
index = hash & dict->ht[x].sizemask;
```

注：`sizemask = hashTable.size - 1`，因此，哈希表的索引计算与HashMap类似
而redis的哈希值计算采用的是`murmurHash2`算法
如果出现哈希碰撞，则使用头插法，将新的节点插到头部

#### REHASH

当哈希表保存的**键值对数量太多或者太少**时，程序需要对哈希表的大小进行相应的扩展或者收缩，其工作是通过rehash（重新散列）完成的：

1. 为字典的 `ht[1]` 哈希表分配空间，分配的大小如下
   - 扩容：`ht[1]` 的大小为第一个大于等于 `ht[0].used * 2` 的 `2^n`
   - 收缩：`ht[1]` 的大小为第一个大于等于 `ht[0].used` 的 `2^n`
2. 将保存在 `ht[0]` 中的**所有键值对** `rehash` 到 `ht[1]` 上，这个过程会重新计算键的哈希值和索引值， 然后将键值对放置到 `ht[1]` 哈希表的指定位置上
3. 当 `ht[0]` 包含的所有键值对都迁移到了 `ht[1]` 之后 （`ht[0]` 变为空表）， 释放 ht[0] ， 将 ht[1] 设置为 ht[0] ， 并在 ht[1] 新创建一个空白哈希表， 为下一次 rehash 做准备

> rehash后的哈希表大小取决于**之前的哈希表中使用了的节点数**

哈希表的负载因子计算：`load_factor = ht[0].used / ht[0].size`
- `ht[0].used`：哈希表内保存的键值对数量
- `ht[0].size`：哈希表中Entry数组的长度

扩容的触发条件：
1. 服务器目前没有在执行 BGSAVE 命令或者 BGREWRITEAOF 命令，并且**哈希表的负载因子大于等于 1**。
2. 服务器目前正在执行 BGSAVE 命令或者 BGREWRITEAOF 命令，并且**哈希表的负载因子大于等于 5**。

缩容的触发条件：
1. 当哈希表的负载因子小于`0.1`时，程序自动开始对哈希表执行收缩操作

#### 渐进式REHASH

rehash并不是一次直接完成的，而是分多次、渐进式完成的。
原因是 Redis 的字典有可能存储上百万个键值对，如果一次性完成的话，那么 Redis 可能会在一段时间内停止服务，为了保证 Redis 的高性能，这么做肯定是不允许的。因此，每次只将`ht[0]`中的==某一个==`dictEntry`移动到`ht[1]`上

渐进式REHASH步骤：
1. 为`ht[1]`分配空间，让字典同时持有`ht[0]`和`ht[1]`两个哈希表；
2. 维持一个索引计数器变量`rehashidx`，并将它的值设置为`0`，表示rehash开始
3. 在rehash进行期间，**每次**对字典执行`CRUD`操作时，程序除了执行指定的操作以外，还会将`ht[0]`中的数据rehash到`ht[1]`表中，并且将`rehashidx`的值`+1`；
4. 当`ht[0]`中所有数据转移到`ht[1]`中时，将rehashidx 设置成`-1`，表示rehash 结束；

> rehash过程中的增删改查：
> - **增加操作会直接应用到新的哈希表中**
> - 删改查操作在保证一致性的前提下，**一定会先操作旧的哈希表，如果在旧的哈希表中没有操作成功，会继续操作新的哈希表**。

## Set的底层数据结构

Set相当于HashSet，它内部的键值对是无序、唯一的。它的内部实现相当于一个特殊的**字典**，**字典中所有的 value 都是NULL**。

Set的底层数据结构采用了`intset`整数集合和`hashtable`字典两种，当满足以下条件时，采用`intSet`：
- 集合中的**所有元素都为整数**
- 集合中的**元素个数小于等于 512**

优先使用`intset`的原因：
- `intset`是非常紧凑的数据结构，占用的内存已经压缩的非常小了，可以提高内存的利用率。
- 查询方式一般采用**二分查找**法，实际查询复杂度也就在`log(n)`
- `intset`底层是==连续的内存空间==，对CPU高速缓存支持更友好，提高查询效率。

![](./../images/redis/intset.png)

intset底层实现为==有序无重复数组==。
intset这个结构里的整数数组的类型可以是`16位`的，`32位`的，`64位`的。
> 如果数组里所有的整数都是`16`位长度的，如果新加入一个32位的整数，那么整个16的数组将==升级成一个32位的数组==。升级可以提升intset的灵活性，又可以节约内存，但==不可逆==。
> 升级流程：1.根据新元素的类型，扩展当前集合的底层数组的空间大小；2.将所有旧元素转换为新的元素类型，并有序地放到合适的位置


## ZSet的底层数据结构

类似于 Java 中 TreeMap 的结合体，用set保证内部value的唯一性，同时给每个 value 赋予一个 score 值，用来代表排序的权重。

ZSet的底层实现为`ziplist` + `skiplist`，满足以下条件时使用`ziplist`：
- 有序集合保存的**元素数量小于默认值128个**（之前的数据结构都是512个诶）
- 有序集合保存的**所有元素的长度小于默认值64字节**

`zskiplist`编码分为两部分，`dict`+`zskiplist`。
**dict和跳跃表都存储数据**，实际上 **dict 和跳跃表最终使用指针都指向了同一份数据**，即数据是被两部分共享的。
- `dict`结构，主要key是其集合元素，而value就是**对应分值**。
- 而`zkiplist`作为跳跃表，按照分值排序，方便定位成员。

![](./../images/redis/zset.png)

跳表中，使用`跨度span`来表示当前层的两个节点之间的距离，跨度用于计算排位`rank`：在查找某个节点的过程中，将沿途访问过的**所有层的跨度**累加起来，得到的就是目标节点在跳表中的排位

## 为啥ZSet要同时使用Dict与Skiplist

如果只用`Dict`，由于字典存储是无序的，在执行范围型操作时，至少需要`O(NlogN)`的时间复杂度，而SkipList只需要`O(logN)`

如果只用SkipList，在实现根据成员返回分值的操作时，就需要`O(logN)`，而Dict只需要`O(1)`

## 为啥ZSet使用跳表，而不是TreeMap的红黑树？

- 跳表实现简单，不用旋转节点
  - 红黑树的插入、删除节点的操作比较麻烦，还会引起子树的调整
- 跳跃表==范围查询==支持更友好
- 算法实现难度上相比于树实现起来更简单

### HashMap为啥用红黑树，而不是跳表？

HashMap的Entry又不是排序，没法使用跳表

## 为啥ZSet不使用B+树

B+树的原理是叶子节点存储数据，非叶子节点存储索引。

B+树的每个节点可以存储多个关键字，它将节点大小设置为磁盘页的大小，充分利用了**磁盘预读**的功能。每次读取磁盘页时就会读取一整个节点，每个叶子节点还有指向前后节点的指针，为的是最大限度的降低磁盘的IO

B+树需要构建存储数据的叶子结点与存储索引的非叶子结点，**叶子结点与非叶子结点都存储了大量的指针，这增大了内存的负担**

## Redis的对象

Redis中每一个对象都由一个`redisObject`结构表示。其定义如下：

```C++
typedef struct redisObject {
    // 对象类型，Redis支持的对象类型可以有String,List,Hash,Set以及ZSet
    unsigned type:4;
    // 对象编码，即对象底层实现的数据结构
    unsigned encoding:4;
    // LRU，用于过期淘汰策略
    unsigned lru:LRU_BITS;
    // 对象被引用次数，Redis的对象回收使用的是引用计数法回收机制
    int refcount;
    // 指向底层实现数据的指针
    void *ptr;
} robj;
```

Redis在其中数据的基础上构建了一个对象系统，Redis中的对象包含字符串对象、列表对象、哈希对象、集合对象、有序集合对象。
Redis在执行命令前，根据对象的类型判断能否执行给定命令。

当对象不再使用时，Redis通过**引用计数法**判断是否应该进行垃圾回收

除了对象回收机制之外，Redis还有对象共享机制：在Redis初始化服务器时，会创建`0-9999`这一万个字符串对象，需要用到这些对象时，就会使用这些共享对象

类似于Java中的`Object`，redis中的对象都是基于`redisObject`的。
而通过编码方式，Redis可以设定对象的具体的底层数据结构。

### String对象encoding方式

字符串对象的编码方式可以有三种，分别是 `int`，`raw`，`embstr`。

- `int`：字符串对象**只保存了整数类型**，使用`Long`整数类型保存数据。
- **长度小于等于39字节的字符串**：编码类型设置为`embstr`，也使用`SDS`数据结构保存数据。
- **长度大于39字节的字符串**：编码类型为`raw`，使用`SDS`数据结构保存数据。

`embstr`与`raw`编码的区别：
- `raw`：Redis会**调用两次内存分配函数**来分配创建`redisObject`结构以及`SDS`结构。

- `embstr`：专门用于保存**短字符串**的优化编码方式，会为`redisObject`结构以及`SDS`结构分配**一块连续的内存空间**。
    - 这样的好处在于**将内存分配/释放次数从两次降为一次**，并且字符串对象**全部保存在一块连续内存中**，可以更好利用缓存带来的优势。

## Redis的常用场景?

**高性能适合当缓存**
缓存时Redis最常见的应用场景，因为Redis读写性能优异。而且，Redis内部是支持事务的，在使用时候能有效保证数据的一致性。

**丰富的数据格式性能更高，应用场景丰富**
Redis相比其他缓存，有一个非常大的优势，就是支持多种数据类型。
具体每种数据类型的用法见上一问

**单线程可以作为分布式锁**

Redis和memcached的区别，除了数据结构和持久化这两个特性，最大的区别就是：
- Redis是单线程，通过多路IO复用来提高效率
- memcached是多线程，通过CPU线程切换来提高效率

所以基于Redis的这个单线程的特性，就可以将其应用于**分布式锁**的场景

> java的synchronized只能用于一台服务器，如果多台服务器进行并发操作，就需要使用控制多台服务器的分布式锁

```java
//产生锁
while lock!=1
    //过期时间是为了避免死锁
    now = int(time.time())
    lock_timeout = now + LOCK_TIMEOUT + 1
    lock = redis_client.setnx(lock_key, lock_timeout)

//真正要处理的业务
doing() 

//释放锁
now = int(time.time())
if now < lock_timeout:
    redis_client.delete(lock_key)
```

**自动过期能有效提升开发效率**

Redis针对数据都可以设置过期时间，让我们不需要关注过期数据的清理。
可用于短信验证码、具有时间性的商品展示等

**分布式和持久化有效应对海量数据和高并发**

Redis为了适应分布式场景，增加了以下特性：
- Redis服务器主从热备，确保系统稳定性
- Redis分片应对海量数据和高并发

除此之外，Redis还拥有多种持久化的方案，可用于可靠性高的数据库

## redis 过期键的删除策略？

1. 定期删除：为键设置过期时间，**每隔一段时间程序就对数据库进行一次检查，删除里面的过期键**。至于要删除多少过期键，以及要检查多少个数据库，则由算法决定。
  - 定时删除与惰性删除的折中，同时保证过期键对CPU与内存的影响都不大

2. 定时删除：在设置键的过期时间的同时，创建一个定时器 timer。让定时器在键的过期时间到期时，**立即执行对键的删除操作**
 
   - 相当于一旦到达过期时间，就删除，而不需要等到定期删除中的间隔。
   - 对内存友好，但对CPU不友好，过期键比较多的话，会将CPU浪费在过期键的删除上

3. 惰性删除：**放任键过期不管，但是每次从键空间中获取键时，都检查取得的键是否过期**，如果过期的话，就删除该键；如果没有过期，就返回该键。
   - 对CPU友好，但对内存不友好 

> 定期删除与惰性删除都是要等到EventLoop中才删除的

## redis 内存淘汰机制？

redis v4.0前提供 6种数据淘汰策略：

**全局的键空间选择性移出**

- `allkeys-lru`：当**内存不足以容纳新写入数据**时，在键空间中，移除**最近最少使用的key**（这个是最常用的）
- `allkeys-random`：从**数据集**中**任意选择**数据淘汰
- `no-eviction`：**禁止驱逐数据**，也就是说当内存不足以容纳新写入数据时，新写入操作会报错。这个应该没人使用吧！ 

**设置过期时间的键空间选择性移除**

- `volatile-lru`：利用`LRU算法`**移除设置过过期时间的key** (LRU:最近最少使用 Least Recently Used )
- `volatile-ttl`：从**已设置过期时间的数据集**中**挑选将要过期**的数据淘汰
- `volatile-random`：从**已设置过期时间的数据集**中**任意选择数据**淘汰

**redis v4.0后增加以下两种**：

- `allkeys-lfu`：当**内存不足以容纳新写入数据**时，在键空间中，调用lfu，移除最不经常使用的key。
- `volatile-lfu`：从**已设置过期时间的数据集**(server.db[i].expires)中挑选**最不经常使用**的数据淘汰(LFU(Least Frequently Used)算法，也就是最频繁被访问的数据将来最有可能被访问到)

## Redis内存划分

Redis内存主要包含：对象内存+缓冲内存+自身内存+内存碎片

1. 对象内存
   - 对象内存是Redis内存中占用最大一块，存储着所有的用户的数据
     - Redis所有的数据都采用的是key-value型数据类型，每次创建键值对的时候，都要创建key对象和value对象。 
2. 缓冲内存
   - 主要包括：客户端缓冲、复制积压缓冲区、AOF缓冲区 
   1. 客户端缓冲主要包括：
      - **普通的客户端的连接**
      - **从客户端的连接**（主要是复制的时候，异地跨机房，或者主节点下有多个从节点）
      - 订阅客户端（发布订阅功能，生产大于消费就会造成积压）
   2. 复制积压缓冲：2.8版本之后提供的**可重用的固定大小缓冲区**用于实现**部分复制**功能，默认1MB，主要是在主从同步时用到。
   3. AOF缓冲区：**持久化**用的，**会先写入到缓冲区**，然后根据响应的策略向磁盘进行同步，消耗的内存取决于写入的命令量和重写时间，通常很小。
3. 自身内存
   - 主要指`AOF/RDB`重写时，**Redis创建的子进程内存的消耗**
   - Linux具有写时复制技术（copy-on-write），父子进程会共享相同的物理内存页。但**当父进程写请求时会对需要修改的页复制出一份副本来完成写操作**。 
4. 内存碎片
   - 目前可选的分配器有`jemalloc（默认）`、`glibc`、`tcmalloc`
   - 出现高内存碎片问题的情况：
     - **大量的更新操作**，比如append、setrange；
     - 大量的过期键删除，释放的空间无法得到有效利用
    解决办法：**数据对齐**，**安全重启（高可用/主从切换）**。 

## Redis内存分配器

Redis在编译时便会指定内存分配器；内存分配器可以是 libc、tcmalloc、jemalloc（默认）

jemalloc作为Redis的默认内存分配器，在减小内存碎片方面做的相对比较好。
jemalloc在64位系统中，将内存空间划分为`小`、`大`、`巨大`三个范围；每个范围内又划分了许多小的内存块单位；当Redis存储数据时，会选择大小最合适的内存块进行存储。

## redis 持久化机制

为了能够重用`Redis`数据，或者防止系统故障，我们需要将`Redis`中的数据写入到磁盘空间中，即持久化。`Redis`提供了两种不同的持久化方法可以将数据存储在磁盘中，一种叫快照`RDB`，另一种叫只追加文件`AOF`

### RDB快照

在一定的间隔时间中，检测`key`的变化情况，然后通过快照的方式持久化，即当符合一定条件时，Redis自动将内存中==所有数据==生成一份副本，并存储在硬盘上。在服务停止时，也会自动执行。

> 恢复时是将快照文件直接读到内存里。

执行时机：
1. 根据配置规则进行自动快照
   - 用户可设置：每当时间间隔M内改动的键超过N，则进行快照
2. 用户执行`SAVE`或`BGSAVE`命令
   - SAVE：在执行过程中阻塞所有来自客户端的请求
   - BGSAVE：在后台异步快照，可以通过LASTSAVE获取最近一次执行快照的时间
3. 执行`FLUSHALL`命令
   - 执行FLUSHALL命令时，Redis清除数据库中的所有数据，如果**自动快照条件不为空**，Redis就会执行一次快照操作
4. 执行**主从复制（replication）1**时
   - 当设置了主从模式时，Redis会在复制初始化时进行自动快照

快照过程：
1. Redis使用`fork`函数复制一个子线程
2. 当前进程继续接收并处理客户端发来的命令，子进程**将内存中的数据写入硬盘的临时文件**
3. 子进程写完后，**用该文件替换旧的.rdb文件**

> 执行fork时，类Unix系统会使用写时复制(`copy-on-write`)策略，即**父子进程共享同一内存数据，父进程修改其中数据时，会在原有数据的副本上修改，以保证子进程的数据不受影响**。
> 即
> - 当主进程执行**读**操作，**直接访问共享内存**
> - 当主进程执行**写**操作，**拷贝一份数据，在拷贝上执行写操作**

整个过程中，**主进程是不进行任何磁盘IO操作的**，由复制的子进程进行内存数据持久化，这就确保了极高的性能。

如果需要进行大规模数据的恢复，且对于数据恢复的完整性不是非常敏感，那`RDB`方式要比`AOF`方式更加的高效。`RDB`的缺点是**最后一次持久化后的数据可能丢失**。

> 由快照过程可知，旧的.rdb文件是不会在过程中被修改的，因此可以通过定时备份`.rdb文件`的方式来备份Redis数据库。
> 但Redis的异常退出仍然会造成一定的损失，如果无法接受，可以考虑使用AOF方式进行持久化

**优势**

- 适合大规模的数据恢复
- 对数据完整性和一致性要求不高

**劣势**

在一定间隔时间做一次备份，所以如果`redis`意外`down`掉的话，就会丢失最后一次快照后的所有修改。

### AOF(Append Only File，只追加文件)

当使用Redis存储非临时数据时，一般需要打开AOF持久化，来降低进程终止造成的损失。

AOF以日志的形式来**记录每个`redis`执行过的写命令**，**只允许追加文件但不可以改写文件**

`redis`启动之初会读取该文件，从而重新构建数据，换言之，`redis`重启时，**会根据日志文件的内容将写指令从前到后执行一次以完成数据的恢复工作**。

**AOF保存的是appendonly.aof文件**

执行时机：
- 每修改同步：`appendfsync always` 同步持久化，**每次发生数据变更**会被立即记录到磁盘,性能较差但数据完整性比较好
- 每秒同步：`appendfsync everysec` 异步操作，**每秒记录**，如果一秒内宕机，有数据丢失
- 不同步：`appendfsync no`   从不同步

#### AOF重写

AOF采用文件追加方式，文件会越来越大。
为避免出现此种情况，Redis新增了重写机制：
- 当AOF文件的大小超过所设定的阈值时，Redis就会启动AOF文件的**内容压缩，只保留可以恢复数据的最小指令集**。可以使用命令`bgrewriteaof`来进行重写

AOF重写也是由`fork`出的子进程来完成的。
新进程的运行方式与RDB中类似，也是通过**写时复制**进行重写，将内存中的记录对应的写语句语句重写到临时文件，并在最后用临时文件替换旧的.aof文件。

> 重写aof文件的操作，**并没有读取旧的aof文件**，**而是将整个内存中的数据库内容用命令的方式重写了一个新的aof文件**，这点和RDB快照有点类似。


**优势**
1. 丢失的数据少：
   - AOF 持久化的方法提供了多种的同步频率，即使使用默认的同步频率每秒同步一次，Redis 最多也就丢失 1 秒的数据而已。
2. 记录Redis原始命令，容易修复
3. AOF文件的可读性较强，这也为使用者提供了更灵活的处理方式。
   - 例如，如果我们不小心错用了 FLUSHALL 命令，在重写还没进行时，我们可以手工将最后的 FLUSHALL 命令去掉，然后再使用 AOF 来恢复数据。

**劣势**

1. 文件体积较大：
   - 相同数据集的数据而言`aof`文件要远大于`rdb`文件，恢复速度慢于`rdb`
2. 对Redis的效率影响大
   - `aof`提供了多种同步频率，一般情况下，每秒同步一次也会有较好的性能，但在 Redis 的负载较高时，RDB 比 AOF 具好更好的性能保证。
3. 健壮性差：
   - RDB 使用**快照**的形式来持久化**整个Redis数据库**，而AOF只是将每次执行的命令追加到 AOF 文件中，因此从理论上说，RDB 比 AOF 方式更健壮。 

### Redis 4.0 对于持久化机制的优化

redis4.0相对与3.X版本其中一个比较大的变化是4.0添加了新的混合持久化方式：`RDB-AOF混合持久化`

混合持久化同样也是通过bgrewriteaof完成的，不同的是：
当开启混合持久化时：
1. fork出的子进程先将共享的**内存副本**==全量==的以`RDB`方式写入`aof`文件
2. **将重写缓冲区的增量命令**以`AOF`方式写入到文件
3. 写入完成后通知主进程更新统计信息，并将新的含有`RDB`格式和`AOF`格式的`AOF`文件替换旧的的AOF文件。

简单的说：新的AOF文件，**前半段是RDB格式的全量数据，后半段是AOF格式的增量数据**，如下图：

![](https://images2018.cnblogs.com/blog/1075473/201807/1075473-20180726181756270-1907770368.png)

**优点**

混合持久化结合了RDB持久化 和 AOF 持久化的优点, 由于绝大部分都是RDB格式，加载速度快，同时结合AOF，增量的数据以AOF方式保存了，数据更少的丢失。

**缺点**

兼容性差，一旦开启了混合持久化，在4.0之前版本都不识别该aof文件，同时由于前部分是RDB格式，阅读性较差。

## 缓存雪崩、缓存击穿和缓存穿透问题解决方案

### 缓存穿透

缓存穿透说简单点就是**大量请求的key根本不存在于数据库中**，导致请求直接到了数据库上(举例：故意的去请求缓存中不存在的数据，导致请求都打到了数据库上，导致数据库异常。)。
甚至数据根本不存在于数据库中，就导致每次请求都要打到数据库上。

解决方案：

1. 做好**参数校验**
   - 对一些不合法的参数请求，直接抛出异常信息返回给客户端。
     - 比如查询的数据库id不能小于0、记录n秒内访问接口的次数等等 
2. **缓存无效key**
   - 如果缓存和数据库都查不到某个key的数据，就将key写入到redis缓存中，并设置过期时间（不能太长），如果遇到无效key，就直接返回错误消息
3. **布隆过滤器**
   - 把**所有可能存在的请求的值**都存放在布隆过滤器中，当用户请求过来，先判断用户发来的请求的值是否存在于布隆过滤器中。
     - 如果不存在，直接返回请求参数错误信息给客户端，存在的话才会走下面的流程 
4. 设立一个黑名单机制，如果用户发出了这种大量的请求，就让用户去填一个验证码，或者限定多久内禁止下单

### 缓存击穿

举例：redis中存储的是热点数据，当高并发请求访问redis中热点数据的时候，如果**redis中的数据过期了**，会造成缓存击穿的现象，**请求都打到了数据库上**。

缓存击穿针对的是==一个key==，即由一个key的过期造成的数据库崩溃

解决方案：

> **1. 延长热点key的过期时间或者设置永不过期**
>   - 实际上，这个方法并不可行，如果增长过期时间，甚至永不过期，会导致redis需要的内存量急剧提升，不符合实际场景的要求。
>   - 如果是只缓存热点数据，也不能解决这个问题
>       - 真实场景中的热点数据是时刻发生变化的，且无法预估热点数据的数据量，只能找到一个预估值，如果热点数据量超过了预估值，就有可能导致系统负载很大

**2. 定时任务主动刷新缓存设计**
  - 通过定时任务主动调度查询，防止缓存失效。（在缓存失效前，更新缓存）
  - 查询逻辑：先将**所有可能查询到的数据**存入redis，对redis中的数据库定时更新，保证redis**永远都会有数据存在**，==来请求只查redis==

- 优点：

  1. 用户的请求压力**永远不会直接打到数据库上**
  2. 查询redis的内存数据，查询效率很高 

- 缺点：
   1. 对redis内存消耗非常大,因为要提前将数据加载到缓存
   2. 系统设计难度大
      - **定时任务必须非常可靠**，一旦定时任务失效，则那么redis中的数据失效，导致服务不可用   
   3. 数据**不能保证实时刷新**
      - 如果刷新缓存的间隔设置很长，那么数据实时性就不够好
      - 如果刷新缓存的间隔很短，频繁的全量刷数据库到缓存对系统和数据库都是压力，也会让数据库和应用服务器的负载变得不够平稳
   4. 维护难度大
      - 由于是只查询缓存,所以会对业务代码进行较大程度的改动,后期业务变化,可能会非常难以维护 

- 适用情况：
    1. 已有一套现成的高可靠分布式定时任务系统
    2. 查询的数据变化不大
    3. 用户的请求量非常大的情况下 

**3. 使用分布式锁**

- 操作逻辑：
   1. 缓存未命中时，尝试通过`setnx`获取分布式锁（**带超时时间的锁，防止获取锁的进程挂掉**）
   2. 如果没有拿到锁，则阻塞当前线程一小段时间，之后再次**自旋**尝试获取分布式锁
   3. 拿到锁之后**检查数据是否已经被其他线程放到redis缓存**中
      - 如果redis缓存已有，直接返回redis中的数据，释放分布式锁
      - 如果没有，则查数据库，将查询结果放入缓存

> 注意，这里有一个问题，就是分布式锁的超时时间的设置：
> 如果超时时间太短，会**导致锁在线程的业务结束前释放**，并被其他线程获取，导致其他线程等待的时间过长
> 如果超时时间太长，就会导致系统经常被长时间阻塞
> 那么，要怎么设定超时时间，才能两全其美呢？
> 
> 可以通过多线程的方案来解决：
> 在加锁了之后，由于锁会有过期时间，然而又不能保证，锁一定不会在执行结束过后过期，
> 那么，我们就可以采用多线程的方案，**让锁每隔一定时间，就重新设置它的超时时间**

- 优点：

   1. 数据的实时性较高，不需要其他外部系统依赖。
      - 利用了redis自己的特性，实现分布式锁，保证了==同样的数据库查询同时只会查询1次==，对数据库的压力较小
   2. 不会侵入业务代码，spring的aop就能很好的实现 

- 缺点：

   1. 阻塞等待分布式锁是个自旋阻塞操作，对应用服务器来说非常浪费cpu的分片时间。
      - 如果这时候大量请求打过来，**应用服务器反而会先扛不住**，因为这里会有大量的线程在自旋占用CPU。
      - 如果用户的查询是由**多个系统的结果**构成，每个系统的查询依赖上一个系统查询的结果，各个查询是串行的，那么**自旋的睡眠时间可能会成为拖慢请求的罪魁祸首**，多个系统都这么设计都在自旋睡眠，明显效率很低 

- 适用情况：

   1. 要求保证数据库的压力特别小，同样的请求只能查询一次数据库，而且服务器较多，足以将多个请求分散到不同服务器，不至于造成太多线程自旋。
> 但总的来说，这种自旋的设计还是挺笨的，不推荐
> 分布式锁的解决方案要求==同样的请求只能查询一次数据库==，但数据库在大多数情况下并不会有这么大的压力，同样的请求多来几次也问题不大

**4. 基于ReentrantLock锁查询缓存**（即ReentrantLock等锁）
  - 基于ReentrantLock锁的解决方案不强求相同的查询只能查一次数据库
  - 操作逻辑
    1. **缓存未命中时，尝试获取JVM锁**，其他线程阻塞。
    2. 拿到锁之后，检查redis是否有数据，以免其他线程已经刷过缓存
       - 如果redis已经有数据，直接返回，并释放锁，返回数据库结束
       - 如果redis**没有数据，则查询数据库**，并保存到redis缓存中 
    3. 返回数据,释放锁
 - 设有s台服务器，用户请求数为n。那么同一时间参数相同的请求最多只会有`s`次查询打到数据库上。
 - 相当于原来对于数据库来说一个`O(n)`的操作时间下降到了`O(s)`，与用户量无关。
  - 优点：
    1. 数据的实时性较高，且JVM锁的消耗比redis分布式锁低很多
    2. 对于数据库的消耗较小，数据库查询数与请求数量无关， 
    3. 实现难度低
    4. 不会侵入业务代码,spring的aop就能很好的实现
  - 缺点：
    1. 对数据库查询虽然减小到了一个只与服务器数量相关的函数,但依然有冗余
  - 适用情况：
    - 能容忍较少次数的数据库重复查询
> 这种设计就用这种就已经能很好的解决缓存穿透的问题了,而且设计简单复杂度低
复杂度低意味着系统的稳定  

> 能不能用synchronized加锁呢？
> 不能，**synchronized在尝试加锁失败后，就会陷入阻塞**，它不像ReentrantLock那样，可以tryLock，或带有超时时间的Lock。
> 且**synchronized是互斥锁，对所有读请求都会加锁**，会严重影响服务器的运行效率


**5. jvm缓存+redis缓存的二级缓存**
  - 二级缓存的关键在于：
    - 服务器只会在**JVM缓存与Redis缓存均失效**的情况下才会查询数据库。
      - jvm的缓存时间是个随机值，比如 10秒~30秒，而多个服务器的随机生存时间的JVM缓存使得**很大程度上避免的同时失效去查库的情况**，从而**降低了数据库上重复查询出现的可能性**。
    > 可以通过SpringCache实现 
  - 优点：
    1. 数据的实时性较高 (设置合适的jvm缓存过期时间和redis缓存过期时间)
    2. 几乎没有冗余的数据库查询
    3. 绝大多数查询是使用的`jvm缓存`，效率极高
    4. 对cpu的占用很低
    5. 不会侵入业务代码，spring的aop就能很好的实现 
  - 缺点：
    1. 如果查询的参数**离散度**较高，其实会**很浪费业务服务器的内存空间**(但是可以通过减少jvm缓存的时间来优化一点)
    2. 设计稍微有点复杂，需要有经验的工程师来实现 

### 缓存雪崩

缓存雪崩是**缓存击穿的大面积版**，即**大量缓存在同一时间失效**，导致大量的请求都会落到数据库上，造成数据库短时间内承受大量请求而崩掉。
发生的几率比针对一个`key`的缓存穿透的几率高很多，因为涉及到大量`key`。

解决方案：

**针对缓存雪崩本身：**
- 延长热点key的过期时间或者设置永不过期，这一点和缓存击穿中的方案一样；
- 在可接受的时间范围内**随机设置key的过期时间**，分散key的过期时间，以防止大量的key在同一时刻过期；
  - 这一点与缓存击穿解决方案中的**二级缓存**方案类似
- 用**加锁或者队列**的方式保证**不会有大量的线程对数据库同时进行读写**，从而避免失效时大量的并发请求落到底层存储系统上
  - 这个方法治标不治本，会导致大量用户被阻塞
- 对于一定要在固定时间让key失效的场景(例如每日12点准时更新所有最新排名)，可以在固定的失效时间时，**在接口服务端设置随机延时**，**将请求的时间打散**，让一部分查询先将数据缓存起来；
  - 这个思路也是防止大量请求打到数据库

**针对服务器的可用性：**
- 事前：尽量保证整个redis 集群的**高可用性**，发现机器宕机尽快补上。选择合适的内存淘汰策略。
- 事中：本地ehcache缓存 + hystrix限流&降级，避免MySQL崩掉
- 事后：利用redis 持久化机制保存的数据尽快恢复缓存 

## 缓存击穿一般有哪些原因

1. redis内存淘汰策略使用不恰当，在热点数据较多时，导致其中的一些热点数据被从缓存中淘汰


## 布隆过滤器的原理

作用：缓存非法Key，避免缓存穿透；网页黑名单；url去重；内容推荐等

布隆过滤器实际上就是一个**bit数组**，bit数组的值是0或1，在添加元素x的时候，对值x通过**一系列随机hash函数**的计算，每个hash函数得到一个hash值，从而在`bit数组`中得到k个位置，将这k个位置设置为1


在应用阶段，根据待搜索对象的hash值判断其在不在布隆过滤器里面，如果bit数组中K个位置的值均为1，则说明待搜索对象存在于集合中。

但如果有两个对象的hash值相同（确实会出现这种情况），可能会导致出现误判，相当于就是宁愿杀错一k，不愿放过一个。
> 当然，在进行搜索时，如果是别的值将x对应的位置设置为1，可能会造成误判，即认为x在集合中，但其实不在
> 使用布隆过滤器，可能会出现FalsePositive错误，但绝不会出现FalseNegative错误，即可能会抓错，但绝不会漏抓


相较于其他集合，布隆过滤器只需要少量的存储空间，且添加和检查元素是否在集合内的复杂度为O(K)

https://blog.csdn.net/lvsaixia/article/details/51503231

## 缓存预热是啥

缓存预热如字面意思：当系统上线时，缓存内还没有数据，如果直接提供给用户使用，每个请求都会穿过缓存去访问底层数据库。如果并发大的话，很有可能在上线当天就会宕机，因此我们需要**在上线前先将数据库内的热点数据缓存至Redis内**，再提供出去使用，这种操作就成为"缓存预热"。

缓存预热的实现方式有很多，比较通用的方式是写个批任务，在启动项目时或定时去触发将底层数据库内的热点数据加载到缓存内。

> 例如每天都要更新的热搜榜

## 缓存降级是啥

缓存降级是指当访问量剧增、**服务出现问题**（如响应时间慢或不响应）或**非核心服务影响到核心流程的性能**时，即使有损部分其他服务，仍然需要保证主服务可用。
可以将其他次要服务的数据进行缓存降级，从而提升主服务的稳定性。

其实思路与服务降级类似

## 如何识别热点数据？热度如何计算？如何更新热度？

热点数据的发现可以通过以下途径实现：
1. 人为预测，根据运营人员的经验，对热点数据进行预测并标记
2. 系统推算，根据数据访问量推算得到，例如使用`volatile-lfu`等内存淘汰策略，只淘汰使用频率低的数据

热度的计算：
-  

https://www.cnblogs.com/java-stack/p/11952248.html    

## 热点数据的更新时机

热点数据即业务中访问较多的数据，热点数据是会随着业务的改变而改变的。
而热点数据的更新即缓存的更新。




## 缓存更新的策略有哪些

缓存更新方案是通过对更新缓存和更新数据库这两个操作的设计，来**实现数据的最终一致性**，避免出现业务问题。

1. **先更新数据库，再更新缓存**
   - 对并发的更新操作：
       1. 更新数据A=100
       2. 更新数据A=200
       3. 更新缓存A=200
       4. 更新缓存A=100 
    - 导致数据库中的新信息被旧的缓存覆盖 
2. **先删除缓存，再更新数据库**（可通过**延时双删**解决）
    -  试想，对并发的更新与查询操作，
        1. A更新，删除缓存
        2. B查询，没有命中缓存，把老数据读出来后放到缓存中
        3. A更新数据库中的数据（被缓存遮盖了）
    - 于是，在缓存中的数据还是老的数据，导致缓存中的数据是脏的，而且还一直这样脏下去了
3. ==先更新数据库，再删缓存（Cache aside）==
   - 业界常用的缓存更新策略
   - 当然，如果删除缓存失败了，还是会出现一致性问题
   - 或**A读缓存未命中，A查询数据库，B更新数据库，B使缓存失效，A写缓存**

**其他解决方案：**
- 延时双删：（先删除缓存，再更新数据库）
  - 为了避免更新数据库的时候，其他线程从缓存中读取不到数据，就在更新完数据库之后，**将更新数据库的线程sleep一段时间，然后再次删除缓存**

![](./../images/redis/延时双删.jpg)

- 利用消息队列删除缓存（先更新数据库，再删除缓存）
  - 更新数据库成功后，将删除缓存的消息发送到消息队列中，借助消息队列的重试机制实现缓存的最终一致性
  - 或利用消息队列监听binlog，从而实现**无代码侵入的缓存删除**

![](./../images/redis/消息队列-缓存.jpg)

- 设置过期时间
  - 给缓存设置过期时间，适用于一致性要求不高，且缓存变化不频繁的场景

> 为什么删除缓存要优于更新缓存：
> 如果数据库1小时内更新了1000次，那么缓存也要更新1000次，但是这个缓存可能在1小时内只被读取了1次，那么这1000次的更新有必要吗？

**进一步介绍三种缓存策略：**

- `Cache aside`：（先更新数据库，再删除缓存）
  - 查询：**先查缓存，未命中就查数据库，然后加载至缓存内**；
  - 更新：**先更新数据库，然后删除缓存**；
  - 安全的原因：
    - 这种设计模式实际上还是会存在并发读写的问题：**A读缓存未命中，A查询数据库，B更新数据库，B使缓存失效，A写缓存**。
    - 但是，在实际情况下，上述case出现的概率极低：数据库的写操作远比读操作耗时，而且还需要加表锁（或间隙锁），而**读操作必需在写操作前进入数据库操作，而又要晚于写操作更新缓存**，所有的这些条件都具备的概率基本并不大。


- `Read/Write through`：(先更新缓存，再更新数据库，但是数据库的更新是由缓存方实现的)
  - **查询**：当**缓存未命中**时，**缓存方自己去数据库中查询数据，并自己将其更新到缓存中**，并返回；
    > Cache Aside 模式是**由程序调用方负责把数据加载入缓存**，
  - 当**有数据更新**时，如果**没有命中缓存，直接更新数据库**，然后返回。
    - 如果命中了缓存，则**更新缓存**，然后**由缓存方实现数据库的更新**；
  > 更新时的缓存更新、数据库更新由**事务**支持，保证了数据一致性
  > 
  > 注意：在`Read/Write through`中，**程序只和缓存交互**，不会去读写数据库。
  读写数据库的任务交给Cache，或理解为存储自己维护自己的Cache，即缓存与数据库封装为一个整体。

- `Write behind caching`：俗称`write back`，在**更新数据**时发生。
  - **只更新缓存，缓存异步地定时批量更新数据库**；
  - 优点：数据读写快，因为都是操作缓存，IO在之后异步执行（还可以合并对一个数据的多次操作）
  - 缺点：数据不是强一致性的，而且可能会丢失（高可用性与性能的trade-off）；实现逻辑比较复杂，需要追踪有哪数据是被更新了的，需要刷到持久层上

> `Write behind caching`和`read/write through`最大的区别在于：
> read/write through在更新缓存后，**立即写入数据库**
> write behind caching则是在一段时间后才**异步地写入数据库**

注意：上述设计并没有考虑缓存与数据库的整体事务问题，例如：没有考虑更新缓存成功但更新数据库失败的问题

## 需要实时性要求很高的，但不是热点信息怎么保证时效性

通过人工标记的方式，将这些实时性要求高的热点数据设定标记，使得其过期时间延长，

## Redis事务的实现机制

Redis对事务的支持目前还比较简单。
**redis只能保证一个client发起的事务中的命令可以连续的执行，而中间不会插入其他client的命令**。
由于redis是单线程来处理所有client的请求的所以做到这点是很容易的。

一般情况下redis在接受到一个client发来的命令后，会立即处理并返回处理结果，但是当一个client在一个连接中发出`multi`命令后，这个连接会进入一个事务上下文，**该连接后续的命令并不是立即执行，而是先放到一个队列中**。

当从此连接收到`exec`命令后，redis会顺序的执行队列中的所有命令。
并将所有命令的运行结果打包到一起返回给client.然后此连接就结束事务上下文。

> 在事务中，每个命令的执行结果是在**最后一起返回**的，所以无法在事务中实现 **将前一条命令的结果作为下一条命令的参数** 这样的操作。
> 但可以通过WATCH命令，实现对一个键或多个键的监控，保证一旦其中有一个键被修改或删除，之后事务就不会执行（过期不受影响）

redis事务不支持回滚，如果不小心写了错误的命令，需要手动修正


## redis事务中的错误处理

Redis在实现事务时，是将命令放到队列中，然后在收到`EXEC`命令时一起执行的，其中可能会出现错误：
- 在执行`EXEC`之前，命令入队错误
  - 比如说，命令可能会产生语法错误（参数数量错误，参数名错误，等等），或者其他更严重的错误，比如内存不足（如果服务器使用 maxmemory 设置了最大内存限制的话）。
  - 以前的做法是检查命令入队所得的返回值：如果命令入队时返回 QUEUED ，那么入队成功；否则，就是入队失败。
    - 如果有命令在入队时失败，那么大部分客户端都会停止并取消这个事务。
  - 
  - 在Redis2.6.5之前， Redis **只执行事务中那些入队成功的命令，而忽略那些入队失败的命令**
  - 但在Redis2.6.5之后，服务器会**对命令入队失败的情况进行记录**，并在客户端调用 EXEC 命令时，**拒绝执行并自动放弃这个事务**。
-  `EXEC 调用之后命令执行失败`
     -  举个例子，事务中的命令可能处理了错误类型的键，比如将列表命令用在了字符串键上面，诸如此类。
     - 对于`EXEC调用之后失败`的事务，==Redis不会选择回滚，而是继续执行剩下的命令==

## 为什么 Redis 不支持回滚（roll back）

对于`EXEC调用之后失败`的事务，Redis不会选择回滚，而是继续执行剩下的命令

- Redis 命令**只会因为错误的语法而失败**（并且这些问题不能在入队时发现），或是**命令用在了错误类型的键上面**：
  - 也就是说，从实用性的角度来说，**失败的命令是由编程错误造成的，而这些错误应该在开发的过程中被发现，而不应该出现在生产环境中**。
- 因为不需要对回滚进行支持，所以 Redis 的内部可以保持**简单且快速**。

> 有种观点认为 Redis 处理事务的做法会产生 bug ， 然而需要注意的是， 在通常情况下， 回滚并不能解决编程错误带来的问题。 
> 举个例子， 如果你本来想通过 [INCR key](http://redisdoc.com/string/incr.html#incr) 命令将键的值加上 `1` ， 却不小心加上了 `2` ， 又或者对错误类型的键执行了 [INCR key](http://redisdoc.com/string/incr.html#incr) ， 回滚是没有办法处理这些情况的。

鉴于没有任何机制能避免程序员自己造成的错误， 并且这类错误通常不会在生产环境中出现， 所以 Redis 选择了更简单、更快速的无回滚方式来处理事务。

## redis消息的实现机制



## ACID

**Redis的事务总是具有ACID中的一致性和隔离性，其他特性是不支持的**。
当服务器运行在AOF持久化模式下，并且appendfsync选项的值为always时，事务也具有持久性。

**隔离性**
Redis 是**单进程程序**，并且它保证在执行事务时，不会对事务进行中断，事务可以运行直到执行完所有事务队列中的命令为止。因此，Redis 的事务是总是带有隔离性的。

**原子性**
Redis中，**单条命令是原子性执行的**，但**事务不保证原子性**，且没有回滚。
事务中任意命令执行失败，其余的命令仍会被执行。



## 假如 Redis 里面有 1 亿个key，其中有 10w 个key 是以某个固定的已知的前缀开头的，如果将它们全部找出来？

使用 `keys` 指令可以扫出指定模式的 key 列表。

追问： 如果这个 redis 正在给线上的业务提供服务， 那使用 keys 指令会有什么问题？

这个时候你要回答 redis 关键的一个特性：**redis 的单线程的**。
**keys 指令会导致线程阻塞一段时间**， **线上服务会停顿， 直到指令执行完毕**，服务才能恢复。
这个时候可以使用 `scan` 指令， `scan` 指令可以**无阻塞的**提取出指定模式的 `key` 列表， 但是**会有一定的重复概率**， 在客户端做一次去重就可以了， 但是整体所花费的时间会比直接用 keys 指令长。

## **Redis6.0为什么要引入多线程呢？**

> Redis将所有数据放在内存中，内存的响应时长大约为100纳秒，对于小数据包，Redis服务器可以处理80,000到100,000 QPS，这也是Redis处理的极限了，对于80%的公司来说，单线程的Redis已经足够使用了。
> 
> 但随着越来越复杂的业务场景，有些公司动不动就上亿的交易量，因此**需要更大的QPS**。
常见的解决方案是**在分布式架构中对数据进行分区并采用多个服务器**，但该方案有非常大的缺点，例如要管理的Redis服务器太多，维护代价大；某些适用于单个Redis服务器的命令不适用于数据分区；数据分区无法解决热点读/写问题；数据偏斜，重新分配和放大/缩小变得更加复杂等等。

从Redis自身角度来说，因为**读写网络的read/write系统调用占用了Redis执行期间大部分CPU时间**，==瓶颈主要在于网络的 IO 消耗==，而不是cpu与内存, 优化主要有两个方向:

- 提高网络 IO 性能，典型的实现比如使用DPDK从内核层对网络处理模块进行优化（比较麻烦）
- 直接使用**多线程**充分利用多核CPU，典型的实现比如 Memcached。

协议栈优化的这种方式跟 Redis 关系不大，而多线程是一种最有效最便捷的操作方式。
所以总结起来，redis支持多线程主要就是两个原因：

- **可以充分利用服务器 CPU 资源**，目前主线程只能利用一个核 
- 多线程任务可以**分摊 Redis 同步 IO 读写负荷**

![img](https://obs-emcsapp-public.obs.cn-north-4.myhwclouds.com/wechatSpider/modb_20200720_110226.png)

**Redis6.0采用多线程后，性能的提升效果如何？**

Redis 作者 antirez 在 RedisConf 2019分享时曾提到：Redis 6 引入的多线程 IO 特性对性能提升至少是一倍以上。国内也有大牛曾使用unstable版本在阿里云esc进行过测试，GET/SET 命令在4线程 IO时性能相比单线程是几乎是翻倍了。

### 实现机制

**流程简述如下**：

1. 服务器启动时启动一定数量线程，服务启动的时候可以指定线程数，**每个线程对应一个队列**（list *io_threads_list[128]），最多128个线程
2. 主线程通过**IO多路复用**，负责接收建立连接请求，并将获取 socket 放入**全局等待读处理队列** ，同**时将该队列中的元素分发给每个线程对应的`io_threads_list`中**。
3. **每个线程（包括主线程和子线程）接收请求参数并做解析，完事后在client中设置一个标记，表示参数解析完成**，可以操作数据库了
4. 主线程遍历**全局等待读处理队列**，发现设置了上述标记位的，就操作数据库
5. 完成数据库操作后，进行client的响应：将对client的响应任务放入**全局等待写处理队列**，并设置相应的标志位
6. 主线程将**全局写队列**以`轮训Round Robin`的方式**将响应请求的任务分发到每个线程对应的队列**
7. 所有线程将遍历自己的队列，将结果发送给client

![](./../images/redis/多线程.png)

由流程图可见，**Redis多线程主要处理网络IO部分的任务**，main线程负责监听网络事件，并分发给**多条线程来处理网络事件**，处理完后将主动权交给main线程，由main线程执行操作，执行完成后依然交给work thread，写回socket


**Redis的多线程没有使用锁，是否会存在线程并发安全问题？**

**Redis 的多 IO 线程只是用来处理网络请求的，对于命令的执行，Redis 仍然使用单线程来处理**，所以不存在并发安全问题

除此之外，在多线程部分，是**由main线程轮训全局读处理队列与全局写处理队列**，并将其中带有特定标记的任务放到**IO线程私有的队列**中，IO线程之间不存在就竞争，且**主线程会阻塞地等待IO线程完成**

## Redis多线程模型与memcached多线程模型的区别

![](./../images/redis/memcached多线程.png)

Memcached 服务器采用 `master-woker` 模式进行工作，后再辅以 辅助线程。

服务端采用 `socket` 与客户端通讯，主线程、工作线程采用`pipe`管道进行通讯。

主线程采用 libevent 监听 listen、accept 的读事件，事件响应后，将连接信息的数据结构封装起来，根据算法选择合适的工作线程，将 连接任务携带连接信息 分发出去，相应的线程利用连接描述符 建立与 客户端的socket连接 并进行后续的存取数据操作。

主线程和工作线程 处理事件流都采用状态机进行事件转移。

那么显而易见，他们的 线程模型 对比起来： 
- 相同点：都采用了 master-worker 这一经典思路 
- 不同点：
  - Memcached **执行主逻辑也是在 worker 线程**里，模型更加简单，不过这也归功于 Memcached 简易数据操作的特性产生的天然隔离；
  - Redis 把处理逻辑交还给 `master` 线程，虽然一定程度上增加了模型复杂度，但是如果把处理逻辑放在 worker 线程，也很难保证隔离性

## **Redis集群方案应该怎么做？都有哪些方案？**

* codis。目前用的最多的集群方案，基本和twemproxy一致的效果，但它支持在 节点数量改变情况下，旧节点数据可恢复到新hash节点。
* redis cluster，特点在于他的**分布式算法不是一致性hash**，而是hash槽的概念，以及自身支持节点设置从节点
* 在业务代码层实现，起几个毫无关联的redis实例，在代码层，对key 进行hash计算，然后去对应的redis实例操作数据。 这种方式对hash层代码要求比较高，考虑部分包括，节点失效后的替代算法方案，数据震荡后的自动脚本恢复，实例的监控，等等。

## 介绍一下Redis集群的主从架构

Redis集群通常采用一主多从的架构，其中，==主节点负责写请求，并将数据复制到slave节点，而slave节点负责读请求，实现了读写分离==，且易于水平扩容。

如果slave节点过多，导致master节点的复制、同步压力过大，可以采用主-从-从的架构

## Redis主从复制

Redis支持主从同步。数据可以从主服务器向任意数量的从服务器上同步，同步使用的是`发布/订阅机制`。

- **全量同步**：一般发生在**Slave初始化阶段**，这是Slave需要将Master上的所有数据都复制一份

	1. 从服务器连接主服务器，发送`SYNC`命令；
	2. 主服务器收到`SYNC`命令后，通过`BGSAVE`命令生成`RDB`快照文件，并使用缓冲区记录这之后的所有写命令
	3. 主服务器`BGSAVE`执行完成后，向从服务器发送快照文件，并**在发送期间，将写命令记录至缓冲区**
	4. 从服务器收到快照后，**丢弃旧数据，载入收到的快照**
	5. 主服务器快照发送完毕后，**向从服务器发送缓冲区的命令**
	6. 从服务器完成对快照的载入，开始接收命令请求，并**执行来自主服务器缓冲区的写命令**；
	
在旧版本中，从服务器第一次连接主服务器，以及从服务器掉线后的重连，均采用的是**全量同步**，导致效率较低。
且`SYNC`是非常消耗资源的：
1. 主服务器需要执行`BGSAVE`命令来生成`RDB`文件，这个生成操作会耗费主服务器大量的CPU、内存和磁盘I/O资源；
2. 主服务器需要将自己生成的`RDB`文件发送给从服务器，这个发送操作会耗费主从服务器大量的网络资源（带宽和流量），并对主服务器响应命令请求的时间产生影响；
3. 接收到`RDB`文件的从服务器需要载入主服务器发来的RDB文件，并且**在载入期间，从服务器会因为阻塞而没办法处理命令请求**。

## 新版本的主从同步是怎么实现的？

新版本的增量同步涉及到几个概念：
- 主从服务器的运行ID`run id`，用于判断主从数据库是不是同一个数据库
  - 、服务器的运行ID不一致，则进行**全量同步**
- 主服务器的**复制偏移量**（`replication offset`）和从服务器的**复制偏移量**；
  - 主服务器每次向从服务器传播N个字节的数据时，就将自己的复制偏移量的值加上N
  - 从服务器每次收到主服务器传播来的N个字节的数据时，就将自己的复制偏移量的值加上N
- 主服务器的**复制积压缓冲区**（`replication backlog`）；
  - `repl_backlog`是由主服务器维护的一个固定长度的队列，当存储满时，最先加入的会被覆盖

在新版本的主从复制中，用`PSYNC`取代了旧版的`SYNC`

**当一个从数据库启动**：
1. 向主数据库发送`PSYNC`命令、
   - 如果从服务器是**第一次连接主服务器**（或执行过 `SLAVEOF NO ONE`），则向主服务器发送`PSYNC ? -1`命令，主动请求主服务器进行**完整重同步**
     - （因为这时不可能执行部分重同步） 
   - 如果从服务器**已经复制过某个主服务器**，则发送`PSYNC <run_id> <offset>`命令，其中`run_id`是上一次复制的主服务器的数据ID，offset是从服务器当前的数据偏移量，从而标识自己已经拥有的数据，避免全量同步。
2. 主服务器收到`PSYNC`命令后，会有以下三种回复：
   - 主服务器返回`+FULLRESYNC <runid> <offset>`回复，表示执行**全量同步**操作
     - 其中`runid`是这个**主服务器的运行ID**，从服务器会将这个ID保存起来，在下一次发送PSYNC命令时使用；
     - `offset`则是主服务器当前的复制偏移量，从服务器会将这个值作为自己的初始化偏移量；
   - 主服务器返回`+CONTINUE`回复，表示执行**部分重同步**操作，即**增量同步**，从服务器只要等着主服务器将**从服务器缺少的那部分数据**发送过来就可以了；
   - 主服务器返回`-ERR`回复，那么表示主服务器的版本低于`Redis 2.8`，它识别不了PSYNC命令，从服务器将向主服务器发送SYNC命令，并与主服务器执行完整同步操作。 

**全量同步**：
1. 快照完成后，将**RDB快照文件**与**缓存的命令**发送给从数据库，从数据库收到后载入快照，并执行缓存的命令
2. 初始化结束后，主数据库**每次收到命令，就同步给从数据库**

**增量同步**：
Slave初始化后开始**正常工作**时，**主服务器发生的写操作同步到从服务器的过程**
1. 增量复制的过程：主服务器每执行一个写命令就会**向从服务器发送相同的写命令**，从服务器接收并执行收到的写命令。
   - master查询自己的`repl_backlog`，将offset后的命令数据，发送给`slave`

> **master如何判断slave是否第一次来同步？**
> slave进行数据同步时，必须向master声明自己的`run id`和`offset`，用于master判断哪些数据需要同步
> - run id：简称replid，是数据集的标记，**id一直则说明是同一数据集**。
>   **每个master都有唯一的run**，slave会继承master的run id
> - offset：偏移量，随着记录在repl_backlog中的数据增多而增大。slave完成同步时也会记录当前同步的offset。如果slave的offset小于master，则说明slave的数据落后。
> 
> **master通过runid和offset判断是否第一次来同步**
> > repl_backlog本质上是一个循环数组，记录历史命令
> 但repl_backlog是有上限的，**如果slave断开时间过久，导致尚未备份的数据被覆盖，则只能进行全量同步**

Redis2.8后，主从数据库的连接断开重连，不需要重新进行**全量同步**，可以进行**有条件的增量数据传输**（用`PSYNC 主数据库的运行ID 断开前最新的命令偏移量`作为增量复制命令）
> 其实就是和上面的描述一样，从数据库断开后采用增量同步

在使用SYNC命令进行同步的过程中，从数据库并不会阻塞，而是可以**使用同步前的旧数据，对客户端发来的命令进行响应**
(可以通过配置`slave-serve-stale-data`参数为`no`来使从数据库在同步完成前，对任何`INFO`和`SLAVEOF`外的任务回复错误)

>Redis采用的复制策略是**乐观复制**，即容忍一定时间内主从数据库的内容不一致，但最终还是会同步的。因为Redis在主从数据库之间复制数据的过程本身就是异步的。


## 主从同步的优化方案

数据同步优化：
- 在master中启用**无磁盘复制**，避免全量同步时的磁盘IO，直接通过网络传输
- Redis单节点的内存占用不要太大，减少RDB导致的磁盘IO
- 提高`repl_backlog`的大小，发现slave宕机时尽快恢复，**尽可能避免全量同步**
- **限制一个master上的slave节点的数量**，可以采用**主-从-从**链式结构，减少master压力

### 无磁盘复制

复制是基于`RDB`方式的持久化实现的，有几个缺点：
- 因为复制初始化需要在硬盘中创建`RDB`快照文件（不管主数据库是否禁用RDB），会受到硬盘性能的影响
- 当缓存系统使用一主多从的集群架构时，每次和从数据库同步，Redis都会执行一次快照，依赖于硬盘的性能

在网络情况良好的情况下，可以通过无磁盘复制代替需要进行IO的复制

无磁盘复制即**直接将主数据库内存中的RDB数据通过网络传输到从数据库中**，不经过磁盘
> 网卡->内存->CPU->内存->磁盘

### 从数据库持久化

可以在从数据库中启用持久化，**在主数据库禁用**。
但当主数据库崩溃时，需要**手工通过从数据库数据恢复主数据库数据**：（也可以使用哨兵来自动化实现）
1. 在从数据库中使用`SLAVE NO ONE`，将从数据库提升为主数据库
2. 启动之前崩溃的主数据库，使用`SLAVEOF`命令，将其设置为新的从数据库，即可将数据同步回来

## redis怎么判断从数据库的同步进度



## Redis主从集群的哨兵机制

哨兵（`Sentinel`）的作用是监控Redis系统的运行情况，可以在无人干预的情况下，让Redis集群抵御一定程度的故障，维护Redis集群的可用性。哨兵有以下功能：
- 集群监控：监控主从数据库是否正常运行
- 故障转移：如果master故障，则将一个slave升级为master
- 消息通知：Sentinel充当Redis客户端的服务发现来源，当集群发生故障转移，将最新消息推送给client
- 配置中心：如果故障转移发生了，通知 client 客户端新的 master 地址

### 哨兵的原理

哨兵的集群控制基于三个定时任务：
1. **每个哨兵**每10秒会向**主、从节点**发送info命令，以获取最新的拓扑结构图。
   - 哨兵**只需要配置对主节点**的监控即可，通过向主节点发送info，获取从节点的信息，并当有新的从节点加入时可以马上感知。
2. **每个哨兵**每隔2秒会向redis数据节点的指定频道上（sentinel：hello）发送**该哨兵节点对于主节点的判断以及当前哨兵节点的信息**，同时**每个哨兵节点也会订阅该频道**，来了解其他哨兵节点的信息以及对主节点的判断。
    > 虽然sentinel集群中每个sentinel都互相连接彼此来检查对方的可用性以及互相发送消息，但是不需要在任何一个sentinel节点都配置其他的snetinel节点。
    因为sentinel利用了master的发布/订阅机制去自动发现其它监控了统一master的sentinel节点。  
3. 每个哨兵每隔1秒会向**主节点、从节点、其他哨兵**发送ping命令，做心跳检测

- **主观下线**：**某个Sentinel节点**发现某实例**未在规定时间响应**，则认为该实例主观下线
- **客观下线**：**超过指定数量**的Sentinel认为该实例主观下线，则该实例客观下线。
  - 如果下线的是**主数据库**，则**首先选举领头的哨兵节点，对主从系统发起故障恢复**
  - 如果下线的是**首领哨兵**，则**重新选举**


> 相较于专注于分布式一致性的Paxos算法，Raft更多的是从多副本状态机的角度提出，**用于管理多副本状态机的日志复制**。
> **Raft算法使用了更强的假设来减少了需要考虑的状态，使之变的易于理解和实现**。

**首领哨兵的选取：（Raft算法）**
1. 每个==发现首领哨兵客观下线的哨兵节点==都向其他每个哨兵发送命令，**要求对方选自己称为领头节点**
2. 如果目标哨兵节点没有选举其他人，则同意节点A成为领头，否则可以拒接
3. 如果A发现有 **半数且超过quorum的哨兵节点** 同意自己领头，则A成功成为领头
4. 如果有多个哨兵同时竞选，则会出现没有任何节点当选的可能，需要每个参选节点**等待一个随机时间，重新发起参选请求**，进行下一轮选举。

**领头哨兵进行master数据库的选举：**
1. 将与`master`断开时间超过阈值的`slave`排除出master数据库的候选列表；
2. 在剩余`slave`中选择**优先级最高的从数据库**。优先级可以通过`slave-priority`选项来设置，如果是0则永不选举
3. 如果多个数据库的优先级相同，则判断`offset`的值，**越大则数据越新，优先级越高**
   > offset即主从复制中使用过的`已复制数据的偏移量 `
4. 如果以上条件都一样，选择**runID较小**的当选

### 哨兵如何实现故障转移：

当选中了新master后（例如slave1），则需要进行故障转移。

> 故障转移即**当活动的服务或应用意外终止时，快速启用冗余或备用的服务器、系统、硬件或者网络接替它们工作**

故障转移的步骤如下：

- Sentinel给选举出的`slave1`节点发送`slaveof no one`命令，让其成为`master`
- Sentinel向其他slave发送`slaveof ip port`命令，让它们称为master的从节点，从新的master上同步数据
- Sentinel将故障节点标记为slave，恢复后自动成为新master的slave

## Redis主从集群下，数据是怎样存储的，集群中各个机器的数据相同吗

Redis主从集群是**主从结构的集群**（或主从从），在选举了新的主数据库之后，需要让从数据库重新成为其slave

而从数据库在成为主数据库的slave时，会向其发送`PSYNC`命令，请求进行基于`RDB快照`的全量同步或基于`AOF`的增量同步，并在完成同步之后执行缓冲区中的写命令，从而实现**主从集群的最终一致性**；

Redis并不能保证集群的强一致性。在某些特殊情况下，主数据库与从数据库的数据可能不一致，但能够保证**最终的弱一致性**：
- 一个客户端发一个写请求给master，master在同步到slave之前就给client一个回执。这时主从数据库的数据就不同，但最终是一致的，因为最终会同步给slave。
- 在客户端收到了master的一个写请求回执之后，此时master准备把数据同步到slave，**同步之前突然挂了，那么这个数据真的就是会丢失了**
- 发生网络分区时也可能丢失数据（后面再说）

## Redis Cluster

上文中，我们讲解了Redis主从集群的知识，主从集群的主要思想是通过==读写分离==提高性能，但如果主从集群都无法满足需求，就需要考虑使用`Cluster集群`

`Redis Cluster`的特点：
- **多主多从，去中心化**：从节点作为备用，复制主节点，**从节点不做读写操作，不提供服务**（Redis Cluster并不是基于主从分离的思路构建的集群）
- **不支持处理多个key**：因为数据分散在多个节点，在数据量大高并发的情况下会影响性能；
- **支持动态扩容节点**：Redis Cluster最大的优点之一；
- **节点之间相互通信，相互选举，不再依赖sentinel**：准确来说是主节点之间相互“监督”，保证及时故障转移

Redis Cluster与其他集群模式的区别：
- 相较于`sentinel`模式，多个master节点保证主要业务（比如master节点主要负责写）稳定性，**不需要搭建多个sentinel实例监控一个master节点**；
- 相比较**一主多从**的模式，不需要手动切换，具有自我故障检测，故障转移的特点；
- 相比较其他两个模式而言，对数据进行**分片（sharding）**，不同节点存储的数据是不一样的；
- 从某种程度上来说，Sentinel模式主要针对高可用（HA），而Cluster模式是不仅针对大数据量，高并发，同时也支持HA。

### 数据分片原理

通常，在实现需要数据分片时，需要使用**一致性哈希算法**，来确定**数据到服务器的映射关系**。

Redis Cluster同样采用了==类一致性哈希算法==，通过哈希槽`Hash Slot`实现数据分片。

一致性哈希是对$2^{32}$取模，而Redis Cluster则是**对$2^{14}$（也就是`16384`）取模**。
Redis Cluster将自己分成了16384个Slot（槽位）。通过`CRC16`算法计算出来的哈希值**会跟16384取模**，取模之后得到的值就是对应的槽位，然后**每个Redis节点都会负责处理一部分的槽位**，就像下表这样。

|节点	|处理槽位|
|:--:|:--:|
|A|	0 - 5000|
|B|	5001 - 10000|
|C|	10001 - 16383|

每个Redis实例会自己维护一份`slot - Redis`节点的映射关系，假设你在节点A上设置了某个key，但是这个key通过`CRC16`计算出来的槽位是由节点B维护的，那么就会**提示你需要去节点B上进行操作**

### 集群水平扩容原理

Redis Cluster通过`reshard(重新分片)`实现水平扩容

`reshard`可以**将任意数量的已经指派给某个节点的slot改为指派给另一个节点（目标节点），并且相关槽所属的键值对也会从源节点被移动到目标节点上**，在Redis内部是由`redis-trib`负责执行的。
> 可以理解为Redis其实已经封装好了所有的命令，而redis-trib则负责向获取slot的节点和被转移slot的节点发送命令来最终实现reshard。

**重新分片操作可以在线进行，在重新分片过程中，集群不需要下线，并且源节点和目标节点都可以继续处理命令请求**。

### MOVED错误与ASK错误

**MOVED错误**

在集群中，客户端只会连接到集群中的某个节点，每个节点只负责一部分的槽，如果**客户端请求的是其他槽**怎么办？Redis会报`MOVED`错误：

`MOVED 10086 127.0.0.1:7002`：表示槽10086正由127.0.0.1，端口号为7002的节点负责，该命令可以指引客户端转向（redirect）正确的节点，并再次发送之前想要执行的命令，得到正确的结果。

**ASK错误**

在进行**重新分片**期间，源节点向目标节点迁移过程中，可能会出现这样一种情况：

当客户端有操作键值对的有关的命令，同时该键值对正好属于**被迁移槽**，并且**被迁移槽的部分键值对还驻留在source节点**中，**另外部分键已经保存在target节点中**；则会进行下列动作：

- 如果能在`source`节点找到对应的key，那么**直接执行client的命令**；
- 如果找不到该key，那很有可能就在`target`中，此时source节点会向client发送一个**ASK错误**，**引导client转向正在导入槽的target节点，并再次发送之前想要执行的命令**。

**二者的区别**

- `MOVED`错误表示：槽的负责权已经从一个节点转移到另外的节点。
- `ASK`错误则是表示：两个节点在**迁移槽过程中**对key处理的负责权。

### 高可用性的保证

Redis Cluster中保证集群高可用的思路和实现和Redis Sentinel如出一辙，但并不是另起一个Sentinel集群，而是**Cluster内部对自己进行监控**。

Cluster同样采用了主从架构，但与`Redis Sentinel`的主从任务分配不同，**Cluster中的从节点是其所属的主节点的备份**，不参与读写任务，只在主节点宕机时发挥作用，替代成为新的主节点。

当一个节点成为从节点以后，其开始复制某个主节点这一信息会通过消息发送给集群中的其他节点，最终集群中所有节点都会知道某个从节点正在复制某个主节点。

### 故障检测的原理

集群中每个节点都会**定期地向集群中的其他节点发送PING消息**，以此检测对方是否在线；
如果接收PING消息的节点没有在规定的时间内，向发送PING消息的节点返回PONG消息，那么发送PING消息的节点就会将目标节点标记为**疑似下线PFAIL**

如果在集群中，==超过半数以上负责处理槽的主节点都将某个节点X标记为PFAIL==，则某个主节点就会将这个主节点X就会被标记为**已下线（FAIL）**，并且**广播到这条消息**，这样其他所有的节点都会立即将主节点X标记为FAIL。

- PING消息：
  - 集群中的每个节点默认每隔一秒钟就会从已知节点列表中随机选出五个节点，然后对这五个节点中**最长时间没有发送过PING消息的节点发送PING消息**，以此来检测被选中的节点是否在线。
  - 除此之外，如果节点A最后一次收到节点B发送的PONG消息的时间，**距离当前时间已经超过了节点A的cluster-node-timeout选项设置时长的一半**，那么节点A也会向节点B发送PING消息，这可以**防止节点A因为长时间没有随机选中节点B作为PING消息的发送对象而导致对节点B的信息更新滞后**。

- PONG消息
  - PONG消息是对PING消息的响应，一个节点也可以通过向集群广播自己的PONG消息来让集群中的其他节点立即刷新关于这个节点的认识（例如一次故障转移成功后）



**故障转移具体流程**

当一个从节点发现自己正在复制的主节点下线时，从节点将开始对下线主节点进行故障转移：

1. 在该下线主节点的**所有从节点中，选择一个做主节点**
2. 被选中的从节点会执行`SLAVEOF no one`命令，成为新的主节点；
3. 新的主节点会**撤销对已下线主节点的槽指派，并将这些槽全部派给自己**。
4. 新的主节点向集群广播一条`PONG`消息，让其他节点知道“我已经变成主节点了，并且我会接管已下线节点负责的处理的槽”；
5. 新主节点开始接收和自己负责处理的槽有关的命令请求，故障转移完成。

**主节点选举流程**
1. ==集群配置纪元==是一个自增计数器，它的初始值为`0`，**当集群里的某个节点开始一次故障转移时，集群配置纪元的值会被增加1**
    
2. 当从节点发现**自己正在复制的主节点进入已下线状态**时，从节点会向集群广播消息：要求所有收到这条消息、并且具有投票权的**主节点**向该从节点投票。

3. 如果一个主节点具有投票权，并且这个主节点尚未投票给其它从节点，那么**主节点将要求投票的从节点返回一条`ACK`消息**，**表示支持该从节点成为新的主节点**。
> 在每个配置纪元，**集群里的每个负责处理槽的主节点**都有一次投票的机会，每个主节点会将票投给**第一个要求他选自己的从节点**

4. **每个主节点在每个配置纪元只有一次投票机会，所以有N个主节点的话，那么具有大于N/2+1张支持票的从节点只有一个**。

5. 如果在一个配置纪元里没有从节点能收集到足够多的支持票，那么**集群进入一个新的配置纪元**，并再次进行选举，直到选出新的主节点为止。

总结：这跟sentinel模式下的选举类似，两个都是基于**Raft算法**的首领选举方法来实现




## 一致性哈希算法原理，主要解决啥问题

https://zhuanlan.zhihu.com/p/129049724

在分布式存储中，不同机器上存储不同对象的数据，一致性哈希算法用于构建从数据到服务器的映射关系。

> 如果采用普通的，*与集群节点数相关*的哈希算法：例如3台机器，哈希算法为`m = hash(i) mod 3`，当机器数变为4时，哈希算法也会产生变化。
> 这就导致数据所在的位置被频繁改变，造成不必要的网络压力

一致性hash算法正是为了解决此类问题的方法，它可以保证**当机器增加或者减少时，节点之间的数据迁移只限于两个节点之间**，不会造成全局的网络问题。

一致性Hash算法也是使用取模的方法，不过，上述的取模方法是对服务器的数量进行取模，而一致性的Hash算法是对==2的32次方==取模。
  即，一致性Hash算法将整个Hash空间组织成一个虚拟的圆环，Hash函数的值空间为`0 ~ 2^32 - 1`(一个32位无符号整型)，整个哈希环如下：

> redis cluster是通过CRC16算法计算数据Key的hash值，并对**2的14次方**取模

![](./../images/redis/一致性hash环.png)

整个圆环以顺时针方向组织，圆环正上方的点代表0，0点右侧的第一个点代表1，以此类推。
第二步，我们将各个服务器使用Hash进行一个哈希，具体可以选择**服务器的IP或主机名**作为关键字进行哈希，这样每台服务器就确定在了哈希环的一个位置上，比如我们有三台机器，使用IP地址哈希后在环空间的位置如下图所示：

![](./../images/redis/一致性hash环-1.png)

现在，我们使用以下算法定位数据访问到相应的服务器：
> 将数据`Key`使用**相同的函数Hash**计算出哈希值，并确定此数据在环上的位置，从此位置沿环顺时针查找，遇到的服务器就是其应该定位到的服务器。

例如，现在有ObjectA，ObjectB，ObjectC三个数据对象，经过哈希计算后，在环空间上的位置如下：

![](./../images/redis/一致性hash环-2.png)


根据一致性算法，Object -> NodeA，ObjectB -> NodeB, ObjectC -> NodeC

### 一致性hash算法的容错性和扩展性

在上图中，假设NodeC对应的服务器宕机了，由图可见，A、B不会受到影响，**只有Object C对象被重新定位到Node A**。

因此，在一致性Hash算法中，如果一台服务器不可用，**受影响的数据仅仅是此服务器到其环空间前一台服务器之间的数据**，其他数据和服务器不受影响

如果进行服务器的扩容，也只是会影响环中的一个节点而已。

### 数据倾斜问题

在一致性Hash算法服务节点太少的情况下，容易因为节点分布不均匀面造成数据倾斜（被缓存的对象大部分缓存在某一台服务器上）问题

![](./../images/redis/一致性hash环-3.png)

如图，大量节点集中在服务器A上。为了解决数据倾斜问题，一致性Hash算法引入了==虚拟节点==机制，即**对==每个服务器节点==计算多个哈希，每个计算结果位置都放置一个此服务节点，称为虚拟节点**。

![](./../images/redis/一致性hash环-虚拟节点.png)

加入虚拟节点后，数据定位算法不变，只需要增加一步：**虚拟节点到实际点的映射**。
所以加入虚拟节点之后，即使在服务节点很少的情况下，也能做到数据的均匀分布。

## Redis分区的方案

- 客户端分区：就是**在客户端就已经决定数据会被存储到哪个redis节点或者从哪个redis节点读取**。大多数客户端已经实现了客户端分区。
- 代理分区：意味着客户端将请求发送给代理，然后**代理决定去哪个节点读写数据**。
  - 代理根据**分区规则**决定请求哪些Redis实例，然后根据Redis的响应结果返回给客户端。redis和memcached的一种代理实现就是`Twemproxy`
- 查询路由：意思是**客户端随机地请求任意一个redis实例，然后由Redis将请求转发给正确的Redis节点**。
  - `Redis Cluster`实现了一种混合形式的查询路由，但并不是直接将请求从一个redis节点转发到另一个redis节点，而是**在客户端的帮助下直接redirected到正确的redis节点**。


## redis能不能作为内存数据库



## 讲一讲 位图(Bitmap)

位图不是特殊的数据结构，它的内容其实就是普通的字符串，也就是 **byte 数组**。

我们可以使用普通的 `get/set` 直接获取和设置整个位图的内容，也可以使用位图操作 `getbit/setbit` 等将 byte 数组看成「位数组」来处理。

当我们要统计月活的时候，因为需要去重，需要使用 set 来记录所有活跃用户的 id，这非常浪费内存。
这时就可以考虑使用**位图**来标记用户的活跃状态：**每个用户会都在这个位图的一个确定位置上**，0 表示不活跃，1 表示活跃。然后到月底遍历一次位图就可以得到月度活跃用户数。不过这个方法也是有条件的，那就是 userid 是整数连续的，并且活跃占比较高，否则可能得不偿失。

## 讲一讲HyperLogLog 

`HyperLogLog`，下面简称为`HLL`，它是 `LogLog` 算法的升级版，作用是能够提供不精确的去重计数。存在以下的特点：

- 代码实现较难。
- 能够使用极少的内存来统计巨量的数据，在 `Redis` 中实现的 `HyperLogLog`，只需要`12K`内存就能统计`2^64`个数据。
- 计数存在一定的误差，误差率整体较低。标准误差为 0.81% 。
- 误差可以被设置`辅助计算因子`进行降低。

### 为什么用HyperLogLog

如果要实现这么一个功能：

> 统计 APP或网页 的一个页面，每天有多少用户点击进入的次数。同一个用户的反复点击进入记为 1 次。

用 `HashMap` 这种数据结构就可以，假设 APP 中日活用户达到`百万`或`千万以上级别`的话，我们采用 `HashMap` 的做法，就会导致程序中占用大量的内存。

估算下 `HashMap` 的在应对上述问题时候的内存占用。假设定义`HashMap` 中 `Key` 为 `string` 类型，`value` 为 `bool`。`key` 对应用户的`Id`,`value`是`是否点击进入`。明显地，当百万不同用户访问的时候。此`HashMap` 的内存占用空间为：`100万 * (string + bool)`。

### HyperLogLog原理

如图，给定一系列的随机整数，我们记录下低位连续零位的最大长度 k，通过这个 k 值可以估算出随机数的数量。

<img src="C:\Users\mashuaisen\OneDrive\typora-user-images\Redis-HyperLogLog\7f698cf742b08017cfac781250378b56" alt="img" style="zoom:67%;" />

HyperLogLog与伯努利试验有关，具体可参考[HyperLogLog 算法的原理讲解](https://juejin.im/post/5c7900bf518825407c7eafd0)

## `Redis`单线程如何处理那么多的并发客户端连接？

因为Redis 的线程模型：基于非阻塞的IO多路复用机制。

Redis 是基于 reactor 模式开发了网络事件处理器，这个处理器叫做文件事件处理器（file event handler）。由于这个文件事件处理器是单线程的，所以 Redis 才叫做单线程的模型。采用 IO 多路复用机制同时监听多个 Socket，根据 socket 上的事件来选择对应的事件处理器来处理这个事件。模型如下图：


![](./../images/redis/文件事件处理器.png)


从上图可知，文件事件处理器的结构包含了四个部分：

- 多个 Socket
- IO 多路复用程序
- 文件事件分派器
- 事件处理器

多个 socket 会产生不同的事件，不同的事件对应着不同的操作，IO 多路复用程序监听着这些 Socket，当这些 Socket 产生了事件，IO 多路复用程序会将这些事件放到一个队列中，通过这个队列，以有序、同步、每次一个事件的方式向文件时间分派器中传送。当事件处理器处理完一个事件后，IO 多路复用程序才会继续向文件分派器传送下一个事件。


## 介绍一下Redis事件机制

Redis服务器是一个事件驱动程序，服务器需要处理两类事件：
- 文件事件（file event）：Redis服务器通过Socket与客户端（或其他Redis服务器）进行联结，**文件事件就是服务器对套接字操作的抽象**。服务器与客户端的通信会产生相应的文件事件，而服务器则通过监听并处理这些事件完成网络通信。
- 时间事件（time event）：Redis服务器中的一些操作需要在给定的时间执行，**时间事件就是服务器对这类定时操作的抽象**

### 文件事件

Redis的基于`Reactor模式`开发了自己的网络事件处理器，这个处理器被称为文件事件处理器（file event handler）：

- 文件事件处理器使用**IO多路复用**程序来监听多个套接字，并根据套接字目前执行的任务为套接字关联不同的事件处理器
- 当被监听的套接字准备好执行连接应答（accept）、读取（read）、写入（write）、关闭（close）等操作，在文件事件产生时，这些文件事件处理器就会调用套接字之前关联好的事件处理器来处理事件

### 时间事件

Redis的时间事件分为：
- 定时事件
- 周期性事件
